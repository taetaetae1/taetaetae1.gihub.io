{"meta":{"title":"꿈꾸는 태태태의 공간","subtitle":"taetaetae","description":"개발,사진,일상 모든것에 대한 기록","author":"taetaetae","url":"https://taetaetae.github.io"},"pages":[{"title":"all-archives","date":"2020-04-23T04:41:41.341Z","updated":"2020-04-23T04:41:41.341Z","comments":false,"path":"all-archives/index.html","permalink":"https://taetaetae.github.io/all-archives/index.html","excerpt":"","text":""},{"title":"all-categories","date":"2020-04-23T04:41:41.347Z","updated":"2020-04-23T04:41:41.347Z","comments":false,"path":"all-categories/index.html","permalink":"https://taetaetae.github.io/all-categories/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2020-04-23T04:41:41.353Z","updated":"2020-04-23T04:41:41.353Z","comments":false,"path":"all-tags/index.html","permalink":"https://taetaetae.github.io/all-tags/index.html","excerpt":"","text":""}],"posts":[{"title":"그런 개발자로 괜찮은가 - '자기개발' 편","slug":"a-good-developer-in-terms-of-self-development","date":"2020-06-28T11:51:03.000Z","updated":"2020-06-28T12:31:45.496Z","comments":true,"path":"2020/06/28/a-good-developer-in-terms-of-self-development/","link":"","permalink":"https://taetaetae.github.io/2020/06/28/a-good-developer-in-terms-of-self-development/","excerpt":"학창 시절엔 ‘선생님’께서 정해놓으신 커리큘럼에 따라가기만 하면 큰 문제 없이 지식을 학습할 수 있었다. 거기에 주기적으로 치르는 시험을 통해 ‘점수’라는 평가 기준으로 얼마나 잘 성장했나를 검사하기도 한다.","text":"학창 시절엔 ‘선생님’께서 정해놓으신 커리큘럼에 따라가기만 하면 큰 문제 없이 지식을 학습할 수 있었다. 거기에 주기적으로 치르는 시험을 통해 ‘점수’라는 평가 기준으로 얼마나 잘 성장했나를 검사하기도 한다. 졸업 후 어렵게 어렵게 취업에 성공을 하여 ‘신입 개발자’라는 배지를 달고 회사에 첫 출근. 그렇게 n 년이 지난 지금과 라떼 시절(?)을 비교해 보며 과연 ‘학습’에 대한 열정 그래프가 아직도 우상향 중인가? 하는 질문엔 일단 단전부터 올라오는 깊은 한숨과 함게 이상하게도 앞이 캄캄해진다. 우리는 모두 라떼 시절을 가지고 있다. 출처 : https://www.dogdrip.net/212294087우리는 모두 라떼 시절을 가지고 있다. 출처 : https://www.dogdrip.net/212294087 배워야 할게 너무 많다. 아니 그보다 배운 것을 이제 활용해야지 싶으면 또 새로운 기술이 등장한다. 그렇게 매너리즘에 빠지고. 거기다 회사일이 바쁘다는 핑계로 자기계발을 멈추다 보면 남들보다 뒤처진다는 생각에 괜히 자괴감이 들어 우울해 지곤 한다. (코로나 블루 때문만은 아니겠지…) 그 가운데 회사에는 정말 좋은 선배님들도 많고 멘토-멘티 관계를 잘 활용하면 충분히, 잘, 올바른 길로 성장할 수 있을 것이라 생각한다. 하지만 그렇게 누군가에게 ‘의존’만 하다 그 대상이 없어진다든지 심지어 그런 대상조차 없을 경우에는 어떻게 해야 할까? 점점 기술은 발전하고 배워야 할 것들은 홍수처럼 넘쳐흐르고 있는 가운데 ‘회사원’에서 나아가 ‘개발자’로써 성장을 하기 위해서는 어떠한 방법이 있을까? 이번 포스팅에서는 개발자로 살아가면서 성장하기 위한즉, 자기계발의 ‘방법’에 대해 이야기해보려 한다. 이것이 정답이다 하는 은 탄환을 소개하려는 것은 아니다. 특히 개발자로서의 생을 마감(?) 할 때까지는 계속 배워야 하는 숙명과도 같은 직업이기에 첫 단추를 잘 끼워서 갑작스러운 기술의 변화에 일희일비 하지 않고 스펀지처럼 무엇이든 흡수하는. 말랑말랑한 정신을 갖기 위함이라고나 할까. # 블로그 개발자가 글도 써야 하나?라는 질문에는 필자가 예전에 정리해둔 개발하기 바쁜데 글까지 쓰라고? (글쓰는 개발자가 되자.)라는 글을 참고해봐도 좋을 것 같다. 해당 포스팅에서 수차례 강조하였지만 그만큼 개발자에게는 특히나 글쓰기가 중요하고 필요하다. 글을 꼭 ‘잘’써야 한다는 부담을 가질 필요는 없다. (필자도 그렇게 잘 쓰는 편은 아니다…) 다만 무언가를 기록하고 정리하고 자신만의 기준에 맞추어 재 정리하는 습관을 기르다 보면 이러한 생각들이 개발을 할 때에도 도움이 상당히 되었기 때문이다. 개발을 하다보면 꼼꼼하게 체크해야할 예외가 너무 많다. 출처 : https://gfycat.com/ko/menacingeducatedatlasmoth개발을 하다보면 꼼꼼하게 체크해야할 예외가 너무 많다. 출처 : https://gfycat.com/ko/menacingeducatedatlasmoth 복잡한 구조가 필요로 하는 개발을 해야 한다고 가정해보자. 연동하는 시스템도 많고 정말 다양한 요구 사항을 하나의 시스템에서 구현을 해야 할 경우 보통 개발을 하기에 앞서 ‘설계’라는 단계를 거치기 마련이다. 그때 글쓰기를 했을 때의 습관(스킬?)을 적용해 보면 요구 사항들 중에 중요한 feature 기준으로 정리를 하게 되고, 각 이해관계자들에게 정리한 부분을 공유하며 예외 상황을 보다 빠르게 확인할 수도 있다. 심지어 코드 레벨에서도 지난밤에 야식으로 먹은 라면 면발처럼 꼬여있는 부분들을 보다 개발하기 편하고 유지 보수가 용이하게 구조를 변경하는 ‘정리’의 습관 또한 글쓰기를 통해서 수련을 할 수 있다. 이러한 ‘꼼꼼함’을 기르는 데에는 글쓰기만 한 게 없다고 생각한다. 우리는 다양한 개발 언어로 코딩을 하곤 한다. 왜 읽기좋은 코드가 좋은 코드라는 책이 있듯이 결국 코딩 또한 커뮤니케이션이 일종이라 생각한다. 내가 생각하는 로직을 개발 언어로 코딩을 해야 하는 상황이면, 결국 내가 생각하는 로직이 명료하고 정리가 잘 된 상태에서야 코드 또한 소위 ‘읽기 좋은 코드’가 되지 않을까 싶다. 블로그를 시작할 때 어디서부터 시작해야 하나 막막하다면, 오늘의 배운 내용 (개발자들 사이에서 유행처럼 번지고 있는 TIL에 대해서 정리해 보는 것부터 추천한다. 경력이 1년 차여도 10년 차여도 개발을 하다 보면 새로운 것을 발견하기 마련이다. 그렇게 조금씩 적절한 블로그 플랫폼에 정리를 해 나가다 보면 어느새 자신만의 개발 히스토리가 만들어지고, 나아가 글쓰기가 전해주는 긍정적인 효과를 만끽하리라 자부한다. # 토이프로젝트 우리는 개발자이다. 맘만 먹으면 생각하고 있는 동작을 얼마든지 만들 수 있는 능력을 가진 대단하면서도 신기한 사람들이다. 그러나, 회사에서 주어진 스펙을 개발하다 보면 이미 만들어진 개발 환경(일명 레거시)에 스펙을 위한 기능만을 개발하다 보니 정작 아무것도 없는 상황에서 시작하려면 머리가 하얘지기 마련이다. 더불어, 회사에서 추구하는 개발 방법론과 주어진 스킬 트리에 맞춤형(?) 개발자가 되다 보니 새로운 기술을 습득하는데 상당한 기술적 장벽이 생기면서 자칫 ‘기술적 고립’이 될 수도 있는 처지에 빠질 수도 있다. 이렇게 위험한 상황을 조금이나마 벗어나기 위해서는 단언컨대 ‘토이 프로젝트’를 시작하는 것이 가장 좋다고 생각한다. 어떠한 기능을 어떠한 기술로 만들 것인지. 이런 것도 해볼까 저런 것도 해볼까. 마치 어렸을 적 레고 블록이라는 장난감으로 여러 가지를 만들었던 것처럼 밑바닥부터 새롭게, 그리고 자유롭게 만들어 나가면서 정말 회사에서 배우지 못한 다양한 기술들을 뼈저리게(?) 배울 수 있는 기회이기 때문이다. 요즘 핫한 개발 방법론이라든지, 새로운 언어, 새로운 기술 set을 접목시켜 봄으로써 회사라는 명찰을 떼고 나 자신이 개발자로써 어떠한 기술을 사용할 수 있는지 확인 또한 가능하다. 여기서 중요한 점은, 그냥 읽어보고 따라 하기만 하는 것보다 그걸 활용해서 실제 결과물을 만들어 보는 것까지 가 중요한 포인트 같다. 이론은 누구나 대충 어림잡아 알고 있다. 하지만 실제로 해보는 건 하지 않은 것과 엄청난 차이가 있다는 걸 명심하자. 꼭 회사 밖에서 찾을 필요는 없다. 팀 내에서 반복적이고 귀찮은 작업들을 자동화 시켜 본다든지, 아니면 바깥에서 들은 기술들을 우리 팀에 접목시켜본다든지. 내가 주체가 되어 무언가를 진행한다는 그 자체가 중요하고 거기서 기술적인 인사이트를 찾고 나만의 것으로 만드는 과정이 핵심 포인트라 생각한다. 필자는 회사 내부에서도 언제부터인가 시키지도 않는 스펙 개발 이외의 것을 시도하려고 부단히 노력 중이다. 일단 무언가를 만들어 본다가 가장 중요하다. ( 참고 : 어려운 것을 쉽게 배우는 방법 : 슈퍼파워를 장착하기 위한 3단계 학습법 ) 이제는 약 3천여 명이 구독하고 있는 필자의 첫 토이 프로젝트인 기술블로그 구독서비스의 개발 후기를 읽어보는 것도 좋을 것 같다. 벌써 만든 지 2년이 다 되어 가는데 기술적인 인사이트뿐만 아니라 ‘서비스’라는 것에 대한 또 다른 시각을 넓혀주는 좋은 계기가 되었기 때문이다. (어서 또 다른 것을 만들어 봐야 하는데 … ) # 배우기 위한 노력 스터디를 하는 방법은 다양하다. 오프라인으로 마음 맞는 사람들끼리 모여서 정해진 규칙에 따라 스터디를 하거나, 나 홀로 인터넷 강의를 들으면서 정해진 커리큘럼대로 배우거나, 그게 아니면 적당한 책을 구입해서 읽거나. 이 외에도 수많은 방법으로 스터디를 하곤 하는데 여기서 이야기하고자 하는 건 스터디의 방법이 아니라 무언가를 배우기 위한 ‘노력’을 꾸준히 해야 한다는 걸 말하고 싶다. 일반적으로 회사에 출근을 하면 업무에 치여서 팀 메신저에 ‘왜 벌써 6시에요?’라는 소리가 나올 정도로 시간 가는 줄 모르고 일한다. 그러다 퇴근을 하면 피곤에 절어 쉬거나 개인적인 여가활동을 하다 보면 어느덧 자야 하는 시간. 그럼 도대체 언제 개발 공부를 해야 할까? 사람마다의 추구하는 행복의 가치관이 다 다르고 훌륭한 개발자로서의 모습 또한 다 다른 만큼 이 부분에 있어서는 정답은 없다. 하지만 개발자로써 학습능력을 꾸준하게 기르기 위해서는 업무시간 외적으로 어느 정도는 개발 공부를 하기 위해 시간을 할애를 해야 한다고 생각한다. 예컨대, 필자는 하루에 최소 한 시간은 회사 업무 외적으로 개발 공부를 하는데 시간을 쏟으려 한다. (하지만 잘 지켜지지 않는 게 함정…) 그렇게 하기 위해서는 꾸준한 운동으로 건강한 몸이 뒷받침되어야 가능하기에 야근을 하든 안 하든 퇴근 후 운동과 개발 공부는 어떤 일이 있어도 (잠을 줄여서라도) 꼭 하려고 하고 있다. 어쩔 땐 그러한 ‘공부를 해야 한다’라는 생각들이 필자를 오히려 구속(?) 시키는 느낌도 받지만 그럴 때면 (위에서 말했던 것처럼) 무언가를 만들어 보며 좀 더 ‘재미있게’ 마인드를 유지하려고 노력 중이다. (이러한 글을 쓰는 것도 어떻게 보면 마인드 컨트롤 방법 중 하나라 볼 수 있다.) 상황에 따라 맞는 기술이 있다. 출처 : https://gfycat.com/ko/tightwholebrontosaurus상황에 따라 맞는 기술이 있다. 출처 : https://gfycat.com/ko/tightwholebrontosaurus 만약, JAVA 라는 언어에 대해 완벽히 안다고 가정해보자. 그럼 10년 20년 그 지식으로 평생 먹고 살수 있을까? 기술은 시대의 변화에 맞춰 바뀌기 마련이며 그러한 변화를 싫어하다보면 우물안의 개구리가 되는건 시간문제라 생각한다. 개발자는 기술의 변화에 민감하게 반응해야 하고 그러한 부분들을 너그럽게 수용할줄 알아야 하며 그렇게 하기 위해서는 자신에게 맞는 방법을 찾아서 흘러 넘치는 변화의 밑물에 노를 저을줄도 알아야 한다 한다. # 나를 정리하는 시간 앞서 이야기 했던 부분들은 결국은 ‘악셀을 밟고 앞으로 전진!’같은 성격의 이야기라면 이번엔 악셀에서 잠시 발을 떼고 정차 후 점검을 하는 관점으로 이야기를 해 보고자 한다. 장거리 운전을 할때는 워셔액은 충분한지, 타이어 공기압은 괜찮은지 점검하는 것처럼 말이다. 필자가 학부시절 연구실을 다닐때 한 선배에게 들은 말이 아직도 생각난다. 본인은 (대학생시절) 한달에 한번씩 이력서를 업데이트 한다고… 그러다 보면 변화가 있었는지 한눈에 알 수 있고, 부족한 부분이 무엇인지 점검이 가능하다는 이야기. 얼마전에 이력서 작성법에 대해 아주 좋은 정리글을 보았다. (개발자 이력서 작성하기 (feat. 이력서 공개)) 이처럼 이력서는 이직할때만 사용하는게 아니라 상시 나를 돌아볼수도 있는 수단이라 생각이 들기 때문에 자신만의 방식으로 이력서를 업데이트 하는것도 좋은 방법이라 생각이 든다. (물론 필자도 링크드인만 몇줄 적은게 다 이지만… 이참에 정리한번 해봐야겠다.) 자신만의 기술 스택을 정리하는 것 또한 방법이 될 수 있다. 쿠버네티스가 요즘 뜨고 있다고? 머신러닝이 대세라고? 하며 요즘 뜨는 기술들만 따라 하는 자세보다 자신이 생각하는 기본기를 탄탄히 다지면서 각자의 호흡, 각자의 속도를 유지하며 꾸준하게 기술 스택을 만들어 나가는 게 올바른 방법이라 생각이 든다. 우리는 1~2년 짧게 개발만 하는 것이 아니라 오랫동안 마라톤처럼 달려야 하기 때문에 탄탄한 기본기를 다지는 자신만의 방법이 필요한 때이다. e.g. black9p님의 노션을 활용한 개인 기술스택 정리 # 마치며 적어도 ‘개발자’ 라면, 그리고 좀 더 괜찮은 ‘개발자’로써 오랫동안 개발을 하고 싶다면 ‘자기계발’은 뗄래야 뗄 수 없는 필연적인 키워드이기 때문에 속도는 늦더라도 꾸준히 지속하는 게 중요하다고 생각한다. ‘자기계발’이라는 키워드로 이야기를 하다 보면 점점 ‘꼰대’가 되는 것만 같아 아쉽지만, 다~ 잘 되고자 하는 (또 꼰대 말투) 부분들이니 너무 노여워하지는 않았으면 한다. :)","categories":[{"name":"essay","slug":"essay","permalink":"https://taetaetae.github.io/categories/essay/"}],"tags":[{"name":"self-development","slug":"self-development","permalink":"https://taetaetae.github.io/tags/self-development/"}]},{"title":"그런 개발자로 괜찮은가 - '문화' 편","slug":"a-good-developer-in-terms-of-culture","date":"2020-06-21T08:22:09.000Z","updated":"2020-06-21T12:35:51.884Z","comments":true,"path":"2020/06/21/a-good-developer-in-terms-of-culture/","link":"","permalink":"https://taetaetae.github.io/2020/06/21/a-good-developer-in-terms-of-culture/","excerpt":"한동안 글을 쓰지 않았다. 글을 쓰지 않은 것일까 쓰지 못한 것일까. 이런저런 이유로 번아웃 늪에 빠져버려 아무것도 하기 싫어서라는 핑계가 어울릴 수도 있겠다만.","text":"한동안 글을 쓰지 않았다. 글을 쓰지 않은 것일까 쓰지 못한 것일까. 이런저런 이유로 번아웃 늪에 빠져버려 아무것도 하기 싫어서라는 핑계가 어울릴 수도 있겠다만. 요즘 들어 더욱더 무기력함이 극도로 뿜뿜대는 가운데 문득, 개발자로써 얼마나 잘 지내왔는가 뒤를 돌아보고 싶었다. 앞만 보고 달리는 것보다 내 생각과 내 호흡을 점검하는 것 또한 중요하다고 생각했기에 당분간은 더 나은 개발자가 되기 위한 여러 가지 주제로 글을 써보려 한다.이름하여 그런 개발자로 괜찮은가 XX 편 어디까지나 필자의 생각에 대해 적는 것일 뿐 내용이 잘못되었을 수도 있다. 즉, 정답이 아니라는 이야기. 필자의 이러한 포스팅으로 이 글을 읽는 여러분들도 자신만의 가치관을 정립해보는 기회가 되고 나아가 모두가 더 나은 개발자로 한걸음 올라서는 아름다운 세상을 꿈꾸는 마음으로 작은 날갯짓을 해본다. 개발자로 살아가는 데 있어 가장 중요한 게 무엇일까? 물론 개발할 수 있는 기술이 가장 중요하겠지만 몇 년 전부터 기술의 발전이 급변하는 세상 속에서 과연 기술만이 중요할까? 기술만 잘 알고 있으면 복잡하게 꼬인 스파게티 면 같은 문제 많은 코드를 술술 풀어헤치고, 언제 어디서든 개발자로써 행복한 삶을 영유할 수 있을까? 여러 가지 중요한 요소들 중 가장 첫 번째로 떠오르는 키워드는 바로 문화(Culture)가 아닐까 싶다. 그럼 왜 문화가 개발자에게 중요하고 어떤 식으로 문화를 만들어 가는 게 좋을지에 대해 정리해보고자 한다. 각 팀에 맞는 문화는 모두를 성장시킬 수 있다. 출처 : https://steemkr.com/kr-dev/@dreamisnowhere/5squ7b각 팀에 맞는 문화는 모두를 성장시킬 수 있다. 출처 : https://steemkr.com/kr-dev/@dreamisnowhere/5squ7b 개발자라는 직업을 가지고 있는 분들 중에 프리랜서나 1인 스타트업을 운영하는 분들은 제외하고. 대부분의 사람들은 여러 명과 함께 공동의 목표를 달성하기 위한 “팀”이라는 단위에 소속되어 개발을 하고 있다. 야근을 매일 밥 먹듯이 하는 조직도 있을 테고 이른바 워라벨을 잘 지키며 듣기만 해도 반가운 소리인 “칼퇴”를 밥 먹듯이 하는 조직도 있을 테고. 여기서 말하고자 함은 이러한 야근 vs 칼퇴처럼 “근무 시간의 양”에 대해 이야기하려는 건 아니다. 회사, 더 깊게는 팀 내에서 어떤 문화 안에서 개발자로 살아가고 있는지에 대해 이야기하려 한다. # 코드리뷰 팀에 속해서 개발을 하다 보면 같은 코드를 동시에 작업하곤 한다. 그래서 형상관리 도구 (요즘 git 을 안 쓰는 곳이 없을 정도…)를 사용해서 동시에 개발을 진행해도 전혀 무리가 없을 정도인데 결국 작업한 결과물을 한 곳으로 병합 (merge) 해야 하는 시점이 오기 마련이고 그때엔 (온라인/오프라인) 코드 리뷰를 하게 된다. 어떠한 사연으로 코드 리뷰 없이 빨리 merge 해야 하는 건 이해되지만 가급적 한 명 이상의 리뷰어가 승인을 한 뒤에 merge 가 돼야 한다고 생각한다. (pullRequest를 단순 merge 용으로 사용하는 건 정말 잘못된 방법 중 하나) 중복된 코드를 만들었거나 작업자가 예상하지 못한 부분들을 릴리스 전에 서로 이야기해보면서 버그를 수정하거나 팀 컨벤션, 설계/구조를 더 효율적으로 가져갈 수 있는 절호의 찬스. 여기서 중요한 포인트는 리뷰를 받는 ‘리뷰이’ 와 리뷰를 해주는 ‘리뷰어’들의 문화적인 측면에서 생각을 해볼 필요가 있다. 리뷰이(Reviewee) 리뷰어의 소중한 시간을 할애해서 자신의 코드가 이상이 없는지에 대한 ‘도움’을 요청하는 것이기 때문에 최대한 설명을 잘 적어서 리뷰하는 데 도움을 줄 수 있어야 한다. 작업을 하다 보면 한 번에 몰아서 코드 리뷰를 받는 경우가 대부분이지만 개발 생산성 측면과 코드 리뷰 시간을 줄이는 측면에서는 최대한 작은 단위로 리뷰를 요청해야 한다. 리뷰가 진행이 되지 못하여 다음 작업 또한 진행을 못하는 경우가 생기는 것을 방지하기 위해 최대한 코드 리뷰 받는 부분과 의존성이 없도록 작업이 돼야 하며 그도 아니라면 정중하게 리뷰어에게 ‘부탁’을 해야 한다. (요청 이 아니라는 점!) 리뷰어의 리뷰는 “지적” 이 아니라 “함께 작업하는 코드에 대한 조언”으로 받아들여야 한다. 혹 리뷰 내용이 자신의 생각과 다르다면 토론을 통해 합의점을 찾도록 해야 한다. (무조건 리뷰어가 리뷰했다고 기계적으로 고치는 것 또한 잘못된 부분) 리뷰어(Reviewer) 리뷰이는 당신의 한마디를 간절하게 기다리고 있을 수도 있으니 최대한 정성껏 리뷰를 해주자. 온라인 코드 리뷰를 하게 된다면 “텍스트”라는 제한적인 상황에서 최대한 유연한 멘트로 코드 리뷰를 해야 한다. (자칫 잘못하다간 서로 오해가 있을 수 있으니…) 코드 리뷰 관련된 문화에 대한 내용들은 잠깐만 검색해봐도 훌륭한 포스팅들이 나오니 참고해봐도 좋을 것 같다.Line | 효과적인 코드리뷰를 위해서Kakao | 코드 리뷰, 어디까지 해봤니?Jandi | 코드리뷰, 이렇게 하고 있습니다. # 공유 무언가를 공유한다는 건 정말 나 잘했어요~ 내가 최고예요~ 하는 ‘자랑’이 목적일까? 필자가 생각하는 공유의 목적은, 자신이 했던 부분들을 ‘다시 정리’함으로써 내 것으로 만드는 과정이 되고 이를 여러 사람들에게 공유함으로써 생각을 나누며 함께 고민한다는 부분이 가장 큰 것 같다. 거기에서 나오는 시시콜콜한 의견들은 공유했던 내용을 더욱 올바른 길로 성장시켜줄 수 있고, 공유라는 작은 날갯짓이 큰 바람을 일으킬 수도 있기에. 다시 말하지만 공유는 자기 자신을 돌아보기에 훌륭한 도구이다. 어떠한 문제로 운영하고 있는 서비스가 장애를 맞았다고 가정해보자. 관련 담당자는 부랴부랴 장애를 수습하기 바쁠 테고 시간이 지나 다시 원상복구를 하기 마련이다. 장애를 복구해서 끝났다고 생각하면? 안타깝지만 그걸로 끝인 거다. 무엇 때문에 장애가 났는지 장애 원인을 파악하고, 동일하거나 비슷한 문제에 대해 방지하기 위한 수단은 무엇이며, 이러한 장애가 발생했을 때 어떻게 처리했는지에 대해 공유가 이루어진다면 함께 일하는 조직원들은 적어도 그러한 문제에 대해 경각심을 느끼고 더 조심히 꼼꼼하게 개발할 수 있을 것이다. (공유를 받고 스팸처리한다면… 할많하않…) 신입이던 10년 차 개발자이던 개발을 하다 새로운 기술이나 어려웠던 부분을 해결했던 경험을 공유한다고 가정해보자. 공유를 하는 사람은 소가 뒷걸음치다 얼떨결에 쥐를 잡는 것처럼 되는 게 아니라 제대로 정리를 할 수 있는 기회가 될 것이고, 공유를 받는 팀원들은 가만히 있어도 공유 내용을 간접경험해 볼 수 있는 정말 좋은 기회가 될 수 있다. 무엇을 공유해야 하지 모를 땐, 아주 사소하게라도 오늘 알게 되었던 새로운 지식을 짤막하게라도 적어보는 습관을 길렀으면 한다. 나아가 팀에서 운영하는 서비스의 기술 부채를 파악하고 개선하여 공유를 하거나, 외부에서 들은 좋은 내용들을 팀 내에 도입한다거나. 다들 SODD 를 한 번쯤은 해봤을 테니 하루에 하나 이상은 새로운 사실을 (혹은 알고 있었는데 제대로 숙지하고 있지 못하는 부분) 알게 될 테니… 무엇을 공유할지 모르는 게 아니라 공유한다는 습관이 부족한 건 아닐까. # 개발 관점에서의 시야 회사는 언제나 바쁘다. 사업의 성공을 위해 하나의 팀에 속한 설계, 기획, 디자인, 개발, QA 등 다양한 직군들은 공동의 목표를 안고 열심히 달린다. 모든 직군들은 최종 결과물을 만들기 위해 사업에 필요한 여러 가지 복잡한 스펙들을 구현하기 바쁘고, 그러다 목표로 했던 일정에 맞추어 서비스를 릴리스 하는데 성공한다. 그렇게 출시를 하고 나면 이제 두발 뻗고 기다리기만 하면 될까? 아니다. 2차 3차를 넘어 또 다른 스펙들이 무섭게도 들어온다. (서비스 오픈 축하 회식조차 못하고…ㅠㅠ) 가장 이상적인 모습은 모든 직군들 이 일정과 스펙 협의가 끝나고 안정적으로 충분한 테스트를 거친 다음 서비스 릴리스가 돼야 하지만. 아쉽게도 서비스 릴리스 일정은 이상하게도 거꾸로 들어온다. 즉, ‘X 월 Y 일에 출시하겠습니다.’라고… 그러한 악순환은 결국 최 말단(?)인 개발자들에겐 숨 막히는 야근으로 변질되고 어찌어찌해서 개발은 하게 되지만, 하나의 스펙을 구현하기에 급급한 코드. 서비스의 스케일이 커졌을 때 확장하기 어려운 구조. 넘쳐나는 중복 코드와. 예상하지 못한 문제들까지. 이러한 ‘기술 부채’는 점점 커지고 무서워서 코드를 건드리지 못하다 결국 하나둘 팀을 떠나게 되는 안타까운 모습이 발생한다. 이러한 부분들은 도대체 언제 해결할 수 있을까? 그리고 누구의 탓일까? 한편으론 ‘한두 달만 서비스 유지만 하고 기술 부채 점검 좀 하겠습니다.’라는 도발적인 목표를 제시하고 싶으나 과연 그런다고 해결이 될까? 아무리 바빠도, 개발자는 개발 관점에서의 시야를 놓쳐서는 안 된다. 여기서 말하는 개발 관점에서의 시야는 모니터링, 중복 코드, 알림 (에러, 시스템, 기타 모니터링 지표 등), 구조/설계, CI/CD, 자동화 등 기능 개발 이외의 다양한 부분들. 이러한 부분을 리더, 혹은 일부 팀원만 관심을 갖는다면 개선이 될 거라는 기대는 너무 이기적인 생각이다.(누군가 하겠지 or 개발할 시간도 없는데! 이런 문화가 팽배한 조직은 멀리 가지 못할 것 같다.) 모두가 함께 코드와 개발적인 요소들에 대해 관심을 갖고 개선의 의지가 있어야 그 개발팀은 건강하게 성장하고 오래 있고 싶거나 외부에서 오고 싶은 팀이 될 수밖에 없을 것 같다. (물론, 자기가 한 게 아니라 신경을 끄겠다는 사람들이 있다면… 정말 묻고 싶다. 왜 이 ‘팀’에 있는 거냐고.) # 어떻게 하면 좋을까? 문화적인 측면에서 위에서 이야기한 것 말고도 정말 다양한 부분들이 많다. 애자 일부터 시작해서 DevOps, SRE, 팀 컨벤션 / 개발 규칙 등 개발 문화는 엄청난 시간 복잡도를 요하는 알고리즘과는 또 다른 측면에서 상당히 중요하다고 볼 수 있을 것 같다. 그렇다면 어떻게 하면 개발 문화를 좋은 방향으로 바꿀 수 있을까? 사실 위 임백준 님의 페이스북 글을 보고 뒤통수를 맞은 느낌이 들어 이 포스팅을 쓰기 시작하게 되었다. 한때 ‘여기는 도저히 있을 곳이 아니야’, ‘이 조직의 문화에 점점 지쳐간다’라며 이직을 생각해 보았던 필자 자신이 부끄러울 정도로. 물론 자신이 팀의 문화를 바꾸기 위한 위치(?)가 아니라면 (혹은 영향력이 없다면) 문화를 바꾸는데 힘들 수 있겠다고 말할 수 있겠지만 꼭 팀의 리더나 연차가 어느 정도 있어야지만 팀의 문화를 바꿀 수 있을까? 아니다. 위에서 이야기했던 부분들을 조금씩 ‘꾸준하게’ 해 나가면서 영향력을 펼친다면 언젠가는 그 팀의 문화가 바뀔 거라 자부한다. 물론 그 팀에 오랫동안 있던 분들(나쁘게 말하면 고인 물…)의 의견이 강하다면 그들을 설득하는 방법부터 바꿔봐야 한다. ‘레거시는 과거의 최선이었다’라는 명언은 무시할 수 없듯이 각 팀에 ‘맞춤형’ 문화를 만들어 나가야 한다. 이러한 ‘문화’ 적인 측면에서는 한 사람의 노력만으로는 절대 바뀔 수가 없다. 어떤 문화가 좋은 문화라는 정답은 없지만 적어도 현재에 안주하려 하고 변화를 싫어하는 모습들은 팀의 성장을 방해할 수밖에 없지 않을까? 회사와 팀 그리고 개인의 성장을 도모하는 문화 속에서 즐거운 개발을 할 수 있기를 바라본다. 우선 이 글을 읽는 여러분의 ‘팀’이 가지고 있는 다른 팀과 다른 문화가 있는가?부터 생각해보자. 없다면, 사소한 것부터 만들어 나가야 하고 있다면 과연 그 문화가 모두의 성장과 발전에 도움이 되는 문화인지 점검하는 과정이 필요할 것이다.","categories":[{"name":"essay","slug":"essay","permalink":"https://taetaetae.github.io/categories/essay/"}],"tags":[{"name":"curtule","slug":"curtule","permalink":"https://taetaetae.github.io/tags/curtule/"}]},{"title":"스프링 부트에 필터를 '조심해서' 사용하는 두 가지 방법","slug":"spring-boot-filter","date":"2020-04-06T14:59:36.000Z","updated":"2020-04-23T04:41:36.865Z","comments":true,"path":"2020/04/06/spring-boot-filter/","link":"","permalink":"https://taetaetae.github.io/2020/04/06/spring-boot-filter/","excerpt":"웹 어플리케이션에서 필터를 사용하면 중복으로 처리되는 내용을 한곳에서 처리할 수 있다거나 서비스의 다양한 니즈를 충족시키기에 안성맞춤인 장치인것 같다. 필터란 무엇인가 에 대한 내용은 워낙에 다른 블로그나 공식 도큐먼트에서 자세하게 그리고 다양하게 설명하고 있기에 기본 개념에 대해서는 설명하지 않도록 하려 한다.","text":"웹 어플리케이션에서 필터를 사용하면 중복으로 처리되는 내용을 한곳에서 처리할 수 있다거나 서비스의 다양한 니즈를 충족시키기에 안성맞춤인 장치인것 같다. 필터란 무엇인가 에 대한 내용은 워낙에 다른 블로그나 공식 도큐먼트에서 자세하게 그리고 다양하게 설명하고 있기에 기본 개념에 대해서는 설명하지 않도록 하려 한다. 이번 포스팅에서는 스프링 부트를 사용하면서 어노테이션이라는 간편함에 취해(?) “돌격 앞으로, 닥공” 의 자세로 개발을 하려했던 필자를 보고 “반성”의 자세로 필터를 등록하는 방법에 대해 명확하게 정리를 하고자 한다. 마지막으로는 아주 간단하면서도 엄청나게 위험한 필터 설정 사례에 대해서도 짚고 넘어가보자.그냥 넘어가면 아쉬우니, 한번이라도 ‘spring’ 이라는 framework 를 접해본 사람이라면 봤을법한 그림을 첨부하는것으로 필터란 무엇인가 에 대한 설명을 대신하는게 좋겠다. 출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/ 방법을 설명하기 전에 동일하게 사용될 필터와 컨트롤러 코드를 보면 다음과 같다. 필터 1234567891011121314151617181920@Slf4jpublic class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; log.info(\"init MyFilter\"); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; log.info(\"doFilter MyFilter, uri : &#123;&#125;\", ((HttpServletRequest)servletRequest).getRequestURI()); filterChain.doFilter(servletRequest, servletResponse); &#125; @Override public void destroy() &#123; log.info(\"destroy MyFilter\"); &#125;&#125; 테스트 할 컨트롤러 1234567891011121314@Slf4j@RestControllerpublic class SampleController &#123; @GetMapping(\"/test\") public String test() &#123; return \"test\"; &#125; @GetMapping(\"/filtered/test\") public String filteredTest() &#123; return \"filtered\"; &#125;&#125; # 방법 1 : FilterRegistrationBean아주 간단하게, 일반 url 하나와 필터에 적용할 url 두개를 만들고 설정하려 한다. FilterRegistrationBean 을 이용해서 위에서 만들었던 필터를 아래처럼 등록해보자. 123456789101112131415@SpringBootApplicationpublic class Method1Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Method1Application.class, args); &#125; @Bean public FilterRegistrationBean setFilterRegistration() &#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new MyFilter()); // filterRegistrationBean.setUrlPatterns(Collections.singletonList(\"/filtered/*\")); // list 를 받는 메소드 filterRegistrationBean.addUrlPatterns(\"/filtered/*\"); // string 여러개를 가변인자로 받는 메소드 return filterRegistrationBean; &#125;&#125; 위 주석에도 적었지만 filterRegistrationBean 의 “setUrlPatterns” 와 “addUrlPatterns” 의 차이는 별거 없다. list 자체를 받을건지 아니면 가변인자로 계속 추가 할것인지. 이렇게 되면 “/filtered/“으로 “시작”하는 패턴의 url의 요청이 오게 되면 등록한 필터를 통과하게 된다. 실행 : 필터 생성 123456789101112131415161718192021222324 /\\\\ / ___&apos;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &apos;_ | &apos;_| | &apos;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.6.RELEASE)2020-04-06 23:45:01.225 INFO 14672 --- [ main] c.t.s.method1.Method1Application : No active profile set, falling back to default profiles: default2020-04-06 23:45:02.153 INFO 14672 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2020-04-06 23:45:02.168 INFO 14672 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2020-04-06 23:45:02.168 INFO 14672 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.33]2020-04-06 23:45:02.361 INFO 14672 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2020-04-06 23:45:02.362 DEBUG 14672 --- [ main] o.s.web.context.ContextLoader : Published root WebApplicationContext as ServletContext attribute with name [org.springframework.web.context.WebApplicationContext.ROOT]2020-04-06 23:45:02.362 INFO 14672 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1082 ms2020-04-06 23:45:02.391 DEBUG 14672 --- [ main] o.s.b.w.s.ServletContextInitializerBeans : Mapping filters: filterRegistrationBean urls=[/filtered/*] order=2147483647, characterEncodingFilter urls=[/*] order=-2147483648, formContentFilter urls=[/*] order=-9900, requestContextFilter urls=[/*] order=-1052020-04-06 23:45:02.391 DEBUG 14672 --- [ main] o.s.b.w.s.ServletContextInitializerBeans : Mapping servlets: dispatcherServlet urls=[/]2020-04-06 23:45:02.409 DEBUG 14672 --- [ main] o.s.b.w.s.f.OrderedRequestContextFilter : Filter &apos;requestContextFilter&apos; configured for use2020-04-06 23:45:02.409 DEBUG 14672 --- [ main] s.b.w.s.f.OrderedCharacterEncodingFilter : Filter &apos;characterEncodingFilter&apos; configured for use// 필터가 생성 되었다!2020-04-06 23:45:02.410 INFO 14672 --- [ main] c.t.springbootfilter.method1.MyFilter : init MyFilter2020-04-06 23:45:02.410 DEBUG 14672 --- [ main] o.s.b.w.s.f.OrderedFormContentFilter : Filter &apos;formContentFilter&apos; configured for use2020-04-06 23:45:02.544 INFO 14672 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService &apos;applicationTaskExecutor&apos; 일반 url 123452020-04-06 23:45:27.526 DEBUG 14672 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/test&quot;, parameters=&#123;&#125;2020-04-06 23:45:27.528 DEBUG 14672 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method1.SampleController#test()2020-04-06 23:45:27.548 DEBUG 14672 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-06 23:45:27.548 DEBUG 14672 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;test&quot;]2020-04-06 23:45:27.555 DEBUG 14672 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK 필터링 url 12345678// 필터에 들어온것 확인!2020-04-06 23:45:37.455 INFO 14672 --- [nio-8080-exec-2] c.t.springbootfilter.method1.MyFilter : doFilter MyFilter, uri : /filtered/test2020-04-06 23:45:37.456 DEBUG 14672 --- [nio-8080-exec-2] o.s.web.servlet.DispatcherServlet : GET &quot;/filtered/test&quot;, parameters=&#123;&#125;2020-04-06 23:45:37.456 DEBUG 14672 --- [nio-8080-exec-2] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method1.SampleController#filteredTest()2020-04-06 23:45:37.457 DEBUG 14672 --- [nio-8080-exec-2] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-06 23:45:37.457 DEBUG 14672 --- [nio-8080-exec-2] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;filtered&quot;]2020-04-06 23:45:37.459 DEBUG 14672 --- [nio-8080-exec-2] o.s.web.servlet.DispatcherServlet : Completed 200 OK # 방법 2 : @WebFilter + @ServletComponentScan@ServletComponentScan 어노테이션을 @Configuration 어노테이션이 설정되어 있는곳에 걸어준 다음 위에서 설정한 필터에 @WebFilter 어노테이션을 설정해주면 아주 간단하게 끝이 난다. @ServletComponentScan 설정 1234567@ServletComponentScan@SpringBootApplicationpublic class Method2Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Method2Application.class, args); &#125;&#125; @WebFilter 설정 12345@Slf4j@WebFilter(urlPatterns = \"/filtered/*\")public class MyFilter implements Filter &#123; // 내용 동일&#125; 위와 같이 설정하고 동일하게 테스트를 해보면 다음과 같이 필터가 설정된 모습을 확인할 수 있다. 일반 url 123452020-04-06 23:54:34.330 DEBUG 23720 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/test&quot;, parameters=&#123;&#125;2020-04-06 23:54:34.332 DEBUG 23720 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method2.SampleController#test()2020-04-06 23:54:34.351 DEBUG 23720 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-06 23:54:34.352 DEBUG 23720 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;test&quot;]2020-04-06 23:54:34.357 DEBUG 23720 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK 필터링 url 12345678// 필터에 들어온것 확인! (위 복붙 아님...)2020-04-06 23:54:58.075 INFO 23720 --- [nio-8080-exec-3] c.t.springbootfilter.method2.MyFilter : doFilter MyFilter, uri : /filtered/test2020-04-06 23:54:58.076 DEBUG 23720 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : GET &quot;/filtered/test&quot;, parameters=&#123;&#125;2020-04-06 23:54:58.076 DEBUG 23720 --- [nio-8080-exec-3] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method2.SampleController#filteredTest()2020-04-06 23:54:58.077 DEBUG 23720 --- [nio-8080-exec-3] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-06 23:54:58.077 DEBUG 23720 --- [nio-8080-exec-3] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;filtered&quot;]2020-04-06 23:54:58.078 DEBUG 23720 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : Completed 200 OK # url 패턴 설정시 유의해야할 부분처음 url 패턴을 설정할때 “/filtered*“ 으로 설정했더니 아래와 같은 로그를 발견하게 된다. 1Suspicious URL pattern: [/filtered*] in context [], see sections 12.1 and 12.2 of the Servlet specification 로그가 “ERROR” 레벨이 아니라서 대수롭지 않게 여겼는데 (사실 보지도 않았다…) 막상 테스트를 해보니 필터가 적용이 안되는 것이다. 하지만 SODD 을 충실히 하는 필자인지라 정답을 금방 찾을 수 있었다. (자랑처럼 보이네…document 를 보는게 더 정확!!!)패턴의 형식이 잘못 되었다는 것. 결국 “/filtered*“ 을 “/filtered/*“ 으로 설정했더니 이상없이 성공. 모든 기술은 도큐먼트를 보고 좀 더 자세하게 확인한 다음 적용해야할 필요가 있다. # 필터에 @Component 가 설정되어 있다면?이 내용이 필자가 꼭 이야기 하고 싶었던 부분이다. 보통 “무엇”을 적용하기 위해서는 구글링을 해보거나 도큐먼트를 보기 시작한다. 그래서 적절한 예제코드나 방법을 찾게 되면 바로 적용. 돌려보고 이상없으면 “빨래 ~ 끝” 느낌으로 거기서 끝을 낸다. 이건 뭐 그냥 하면 되잖아? 출처 : https://www.popmatters.com/161123-strong-weightlifting-as-metaphor-2495831472.html이건 뭐 그냥 하면 되잖아? 출처 : https://www.popmatters.com/161123-strong-weightlifting-as-metaphor-2495831472.html 왜 그랬던건지는 모르겠지만 필자가 적용한 필터에 “@Component” 가 적용되어 있었고, 필터가 잘 걸리는 것 까지 확인했지만 오히려 모든 url 에 필터가 걸려버린 것이다. 왜일까? 필자가 적용한 필터의 모습은 다음과 같았고 12345@Component@WebFilter(urlPatterns = \"/filtered/*\")public class MyFilter implements Filter &#123; // 동일&#125; 실제로 실행을 해보면 init 이 두번 되는것을 확인할 수 있다.122020-04-07 02:22:22.250 INFO 27568 --- [ main] c.t.springbootfilter.method2.MyFilter : init MyFilter2020-04-07 02:22:22.250 INFO 27568 --- [ main] c.t.springbootfilter.method2.MyFilter : init MyFilter 위에서 테스트한 “/test” 를 호출해 보면 “doFilter”에서 한번 로깅이 되고 “/filtered/test” 를 호출해 보면 두번 로깅 되는걸 확인할 수 있다.12345678910111213142020-04-07 02:23:50.505 INFO 27568 --- [nio-8080-exec-1] c.t.springbootfilter.method2.MyFilter : doFilter MyFilter, uri : /test2020-04-07 02:23:50.507 DEBUG 27568 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/test&quot;, parameters=&#123;&#125;2020-04-07 02:23:50.510 DEBUG 27568 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method2.SampleController#test()2020-04-07 02:23:50.531 DEBUG 27568 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-07 02:23:50.531 DEBUG 27568 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;test&quot;]2020-04-07 02:23:50.537 DEBUG 27568 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK2020-04-07 02:24:03.571 INFO 27568 --- [nio-8080-exec-3] c.t.springbootfilter.method2.MyFilter : doFilter MyFilter, uri : /filtered/test2020-04-07 02:24:03.572 INFO 27568 --- [nio-8080-exec-3] c.t.springbootfilter.method2.MyFilter : doFilter MyFilter, uri : /filtered/test2020-04-07 02:24:03.572 DEBUG 27568 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : GET &quot;/filtered/test&quot;, parameters=&#123;&#125;2020-04-07 02:24:03.572 DEBUG 27568 --- [nio-8080-exec-3] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.springbootfilter.method2.SampleController#filteredTest()2020-04-07 02:24:03.573 DEBUG 27568 --- [nio-8080-exec-3] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-04-07 02:24:03.573 DEBUG 27568 --- [nio-8080-exec-3] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;filtered&quot;]2020-04-07 02:24:03.574 DEBUG 27568 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : Completed 200 OK 정답은 스프링 부트를 사용하다 보면 가장 처음으로 만나는 “@ComponentScan”와 “@Component”에 있다. “@SpringBootApplication”는 여러 어노테이션의 묶음이고 그 안에는 “@ComponentScan”가 있어서 빈들을 자동으로 등록해주는 역할을 하게 되는데 필터에 “@Component”가 설정되어 있어 자동으로 등록이 되었고, 두번째 방법인 “@WebFilter + @ServletComponentScan” 조합으로 한번 더 등록되어버린 것이다. 즉, 동일한 필터가 두번 등록된 상황.“/test” 에서 한번 로깅된건 “@Component” 에 의해 등록된 필터로 인해 urlPattern 이 적용되지 않았으니 한번 로깅이 되고, urlPattern 이 적용된 필터에서는 urlPattern에 맞지 않으니 로깅이 안되는건 당연. 그 다음 “/filtered/test” 은 “@Component” 에 의해 등록된 필터로 한번 로깅, 그다음 “@WebFilter”로 등록된 필터에서 urlPattern에 맞는 url 이다보니 로깅이 되서 총 두번 로깅이 되게 된다.즉, 모든 url에 필터를 적용 할 것이라면 “@ComponentScan + @Component” 조합으로 해도 될 것 같고, 명시적으로 특정 urlPattern 에만 필터를 적용한다거나 필터의 다양한 설정 (우선순위, 필터이름 등) 을 하게 되는 경우엔 위에서 알려준 “FilterRegistrationBean” 이나 “@WebFilter + @ServletComponentScan”을 사용해서 상황에 맞도록 설정하는게 중요할 것 같다. # 마치며좀 알고 쓰자. 실수는 두번하면 실력이다. 다음번엔 절대 실수 안해야지.왜 저렇게 무턱대고 설정했는지 부끄럽기 짝이 없지만 함께 디버깅을 해주며 문제를 해결하는데 도움을 준 black9p 님 덕분에 이렇게 필터 적용방법에 대해 정리를 할 수 있어서 한편으론 다행이라 생각이 든다. 이번에도 위에서 사용한 코드는 필자의 Github Repo 에서 확인이 가능하다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"filter","slug":"filter","permalink":"https://taetaetae.github.io/tags/filter/"}]},{"title":"조금 더 괜찮은 Rest Template 2부 - Circuit-breaker","slug":"better-rest-template-2-netflix-hystrix","date":"2020-03-29T14:09:16.000Z","updated":"2020-04-23T04:41:36.697Z","comments":true,"path":"2020/03/29/better-rest-template-2-netflix-hystrix/","link":"","permalink":"https://taetaetae.github.io/2020/03/29/better-rest-template-2-netflix-hystrix/","excerpt":"지난 포스팅에서는 Retryable 를 활용해서 간헐적인 네트워크 오류를 “재시도”를 함으로써 아주 간단하면서도 강력하게 해결할 수 있는 방법에 대해 알아보았다. 실제로 필자가 운영하는 서비스 에서도 Retryable 를 이용하기 전과 후를 비교해보면 간헐적인 네트워크 오류의 빈도수가 확실히 줄어든것을 확인할 수 있었다.","text":"지난 포스팅에서는 Retryable 를 활용해서 간헐적인 네트워크 오류를 “재시도”를 함으로써 아주 간단하면서도 강력하게 해결할 수 있는 방법에 대해 알아보았다. 실제로 필자가 운영하는 서비스 에서도 Retryable 를 이용하기 전과 후를 비교해보면 간헐적인 네트워크 오류의 빈도수가 확실히 줄어든것을 확인할 수 있었다. 이렇게 “재시도”를 해서 요청했을때 성공 응답을 받을 경우엔 문제가 안되지만 네트워크 오류가 아닌 실제로 호출을 받는 해당 서버에서 문제가 발생했다면 어떨까? 예컨대, 해당 서버에서 DB를 조회하는 API를 호출한다고 가정했을때 DB 자체에서 어떠한 오류가 난다면. 이런 경우는 단순히 “재시도”로 해결할 수 없는 문제다.물론 Retryable 의 Recover 어노테이션을 활용했기 때문에 클라이언트 즉, 사용자에게는 오류응답이 발생을 안했겠지만 호출 받는 서버 자체에서의 에러가 발생하는데 이런식의 재시도를 계속 시도한다면 호출 받는 서버 입장에서는 이 “재시도” request 또한 “부하” 로 받게 되고 결국 2차, 3차 장애가 이어질 수 밖에 없다.기존 한덩어리로 관리되던 Monolithic Architecture 에서는 자체적으로 관리하기 때문에 이러한 에러 컨트롤 또한 자체적으로 관리를 할 수 있지만, 모듈이 모듈을 호출하게 되는 Microservice Architecture 로 바뀌다보니 이런 “연쇄 장애(?)” 같은 현상이 발생하게 되는 경우가 있다. 호출을 받는 서버의 상태가 이상하면 (에러응답이 지정한 임계치를 벗어나는 수준으로 맞춰서 발생한다면) 적절하게 호출을 하지 않고 (2차 장애를 내지 않도록 호출 자체를 하지 않고) 어느정도 기다리다 클라이언트에게는 에러응답이 아닌 미리 정해둔 응답을 내려주고, 에러가 복구되면 다시 호출하도록 하는 “무언가” 가 필요하지 않을까? 연쇄 장애. 제발 멈춰... 출처 : http://dpg.danawa.com/mobile/community/view?boardSeq=175&listSeq=4066389연쇄 장애. 제발 멈춰... 출처 : http://dpg.danawa.com/mobile/community/view?boardSeq=175&listSeq=4066389 지난 포스팅에 이어 이번 포스팅 에서는 그 “무언가”. 즉, Circuit-breaker 에 대해 알아보고 직접 구현 및 테스트 하면서 돌아가는 원리에 대해 이해 해보고자 한다. 막상 개념은 머릿속에 있지만 직접 구현해보지 않으면 내것이 아니기에, 직접 구현하고 설정값들을 바꿔가면서 언젠가 필요한 순간에 꺼내서 사용할 수 있는 나만의 “무기” 를 만들어 보고자 한다. # Circuit breaker ?(한국 발음으로) 서킷브레이커를 검색해보면 주식시장 관련된 내용이 꽤 나온다. (앗, 잠깐 눈물좀…) 서킷 브레이커. 이 용어는 다양한 곳에서 사용되는데 “회로 차단기” 라고도 검색이 된다. 해당 내용을 발췌해보면 다음과 같다. 회로 차단기는 전기 회로에서 과부하가 걸리거나 단락으로 인한 피해를 막기 위해 자동으로 회로를 정지시키는 장치이다. 과부하 차단기와 누전 차단기로 나뉜다. 퓨즈와 다른 점은, 차단기는 어느 정도 시간이 지난 뒤, 원래의 기능이 동작하도록 복귀된다. 여기서 가장 중요한 문장은 “피해를 막기 위해 자동으로 회로를 정지시키는”, “어느정도 시간이 지난뒤 원래의 기능이 동작하도록 복귀된다” 이 부분이 가장 중요한 것 같다. 시스템 구성이 점점 Microservice Architecture 로 바뀌어 가는 시점에서 이러한 “서킷브레이커”는 자동으로 모듈간의 호출 에러를 감지하고 위에서 말한 “연쇄 장애”를 사전에 막을 수 있는 아주 중요한 기능이라 생각된다.“circuit breaker spring” 이라는 키워드로 검색해보면 이러한 고민을 이미 Netflix 라는 회사에서 Hystrix 라는 이름으로 개발이 된것을 알 수 있다. 이 core 모듈을 Spring 에서 한번 더 감싸서 Spring Boot 에서 사용하기 좋게 spring-cloud-starter-netflix-hystrix 라는 이름으로 만들어 둔 것이 있는데 이것을 활용해 보기로 하자. # 구현늘 그랬듯이 SpringBoot 프로젝트를 만들고 테스트할 Controller 를 만들어 주자. 원래대로라면 호출을 하는 모듈과 호출을 받는 모듈, 2개의 모듈을 만들어서 테스트 해야 하지만 편의를 위해 하나의 모듈에서 두개의 Controller 을 만들고 테스트 해보는 것으로 하자.1234567891011121314151617181920212223242526272829303132333435363738394041424344@RestControllerpublic class MainController &#123; private final MainService mainService; @GetMapping(\"index\") public String index(String key)&#123; return mainService.getResult(key); &#125; public MainController(MainService mainService) &#123; this.mainService = mainService; &#125;&#125;@Slf4j@Servicepublic class MainService &#123; private RestTemplate restTemplate; public String getResult(String key) &#123; return restTemplate.getForObject(\"http://localhost:8080/target?key=\" + key, String.class); &#125; public MainService(RestTemplate restTemplate) &#123; this.restTemplate = restTemplate; &#125;&#125;@Slf4j@RestControllerpublic class TargetContoller &#123; @GetMapping(\"/target\") public String target(String key) &#123; log.info(\"input key : &#123;&#125;\", key); if (!StringUtils.equals(\"taetaetae\", key)) &#123; throw new RuntimeException(\"Invalid key\"); &#125; return \"target\"; &#125;&#125; 사실 설명할 부분도 없긴 하지만 그래도 적어보면, /index라는 주소와 key라는 파라미터로 요청을 하면 /target이라는 컨트롤러가 받아서 key 값에 따라 정상 응답을 줄지 아니면 에러를 응답하는 코드이다. 목표로 하는건, 파라미터를 일부러 잘못줘서 에러를 발생하는데 Hystrix를 적용해서 설정한 기준에 따라 응답은 미리 정해둔 응답을, TargetContoller에서는 잠시동안 요청을 받지 않다가 조금있다가 다시 호출하면 요청을 받는 시나리오로 작성해 보려 한다.우선 필요한 dependency를 추가해준다.123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 처음에 한참 했갈렸는데 “spring-cloud-starter-hystrix”가 아니고 “spring-cloud-starter-netflix-hystrix”를 적용해야 이상없이 돌아간다. (참고 : https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-hystrix) 그 다음 실제 적용할 코드에 Hystrix 설정을 적용해 주자. 1234567@EnableCircuitBreaker // 1@SpringBootApplicationpublic class HystrixApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(HystrixApplication.class, args); &#125;&#125; SpringBoot를 사용하면서 비슷하게 적용되는 EnableXXX 어노테이션, 이번에도 @EnableCircuitBreaker 어노테이션을 적용해 주면서 CircuitBreaker 를 사용하겠다고 지정해 주자. 1234567891011121314151617181920212223242526@Servicepublic class MainService &#123; private RestTemplate restTemplate; @HystrixCommand( // 1 fallbackMethod = \"fallbackMethod\", // 2 commandProperties = &#123; @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"500\"), // 3 @HystrixProperty(name = \"circuitBreaker.requestVolumeThreshold\", value = \"2\"), // 4 @HystrixProperty(name = \"circuitBreaker.sleepWindowInMilliseconds\", value = \"2000\"), // 5 @HystrixProperty(name = \"circuitBreaker.errorThresholdPercentage\", value = \"30\") // 6 &#125;) public String getResult(String key) &#123; return restTemplate.getForObject(\"http://localhost:8080/target?key=\" + key, String.class); &#125; public String fallbackMethod(String key) &#123; log.info(\"fallbackMethod, param : &#123;&#125;\", key); return \"defaultResult\"; &#125; public MainService(RestTemplate restTemplate) &#123; this.restTemplate = restTemplate; &#125;&#125; 스프링 가이드에 따르면 @Component 나 @Service 하위에서 @HystrixCommand어노테이션을 찾아 작동한다고 나와있다. 때문에 우리가 적용할 “MainService.getResult()”에 지정해 주도록 하자. Spring Cloud Netflix Hystrix looks for any method annotated with the @HystrixCommand annotation and wraps that method in a proxy connected to a circuit breaker so that Hystrix can monitor it. This currently works only in a class marked with @Component or @Service. fallbackMethod 을 이용하여 호출했을때 에러가 발생하면 실행할 메소드를 지정해주자. 즉, 설정한 기준에 도달하여 에러응답 대신 지정한 응답을 내려줄 경우 해당 메소드를 사용하게 된다. 타임아웃을 지정해 준다. 여기서는 500ms 안에 응답이 없을 경우 에러라고 판단. 서킷브레이커가 open 되는 최소 단위. 필자는 테스트를 위해 극단적(2)으로 지정하였지만 상황에 따라 해당 값을 커스터마징 할 필요가 있다. 서킷브레이커가 지속되는 시간. 2초동안 지속되도록 한다. 서킷브레이커가 open 되는 에러율. 다른값들과 우선순위를 잘 따져봐야 한다. 테스트를 위해 무조건 한번이라도 에러 발생할 경우 open 해도록 지정해두었다. 이 외에도 정말 다양한 설정값들이 있는데 기본 설정값으로 했을때 시스템 성격에 맞지 않는다면 커스터마이징을 통해 현 시스템에 가장 적절한 값을 찾아야 할 것이다. # 테스트자. 뭔가 여러가지 설정을 했는데 이제 테스트를 해보자. 테스트를 하면서 설정값들을 이해하는게 가장 빠르다. 자세한 로그를 확인하기 위해 편의상 application.properties 에 debug=true 설정을 해두었다.우선 정상응답을 얻기 위해 “/index?key=taetaetae” 라고 호출해보면 response 또한 “ok” 라고 나오는 것을 알 수 있다.1234567891011121314152020-03-30 01:55:29.866 DEBUG 13496 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taetaetae&quot;, parameters=&#123;masked&#125;2020-03-30 01:55:29.871 DEBUG 13496 --- [nio-8080-exec-9] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 01:55:30.057 DEBUG 13496 --- [x-MainService-1] o.s.web.client.RestTemplate : HTTP GET http://localhost:8080/target?key=taetaetae2020-03-30 01:55:30.059 DEBUG 13496 --- [x-MainService-1] o.s.web.client.RestTemplate : Accept=[text/plain, application/json, application/*+json, */*]2020-03-30 01:55:30.065 DEBUG 13496 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : GET &quot;/target?key=taetaetae&quot;, parameters=&#123;masked&#125;2020-03-30 01:55:30.065 DEBUG 13496 --- [io-8080-exec-10] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.target.TargetContoller#target(String)2020-03-30 01:55:30.066 INFO 13496 --- [io-8080-exec-10] c.t.hystrix.target.TargetContoller : input key : taetaetae2020-03-30 01:55:30.073 DEBUG 13496 --- [io-8080-exec-10] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/plain&apos;, given [text/plain, application/json, application/*+json, */*] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 01:55:30.074 DEBUG 13496 --- [io-8080-exec-10] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;ok&quot;]2020-03-30 01:55:30.077 DEBUG 13496 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : Completed 200 OK2020-03-30 01:55:30.079 DEBUG 13496 --- [x-MainService-1] o.s.web.client.RestTemplate : Response 200 OK2020-03-30 01:55:30.080 DEBUG 13496 --- [x-MainService-1] o.s.web.client.RestTemplate : Reading to [java.lang.String] as &quot;text/plain;charset=UTF-8&quot;2020-03-30 01:55:30.087 DEBUG 13496 --- [nio-8080-exec-9] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 01:55:30.087 DEBUG 13496 --- [nio-8080-exec-9] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;ok&quot;]2020-03-30 01:55:30.089 DEBUG 13496 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : Completed 200 OK 자 이제 실패가 되도록 잘못된 파라미터를 호출하게 되면 어떻게 될까? “/index?key=taekwan”1234567891011121314151617181920212223242020-03-30 01:57:48.025 DEBUG 19084 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 01:57:48.031 DEBUG 19084 --- [nio-8080-exec-9] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 01:57:48.222 DEBUG 19084 --- [x-MainService-1] o.s.web.client.RestTemplate : HTTP GET http://localhost:8080/target?key=taekwan2020-03-30 01:57:48.224 DEBUG 19084 --- [x-MainService-1] o.s.web.client.RestTemplate : Accept=[text/plain, application/json, application/*+json, */*]2020-03-30 01:57:48.229 DEBUG 19084 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : GET &quot;/target?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 01:57:48.229 DEBUG 19084 --- [io-8080-exec-10] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.target.TargetContoller#target(String)2020-03-30 01:57:48.230 INFO 19084 --- [io-8080-exec-10] c.t.hystrix.target.TargetContoller : input key : taekwan2020-03-30 01:57:48.237 DEBUG 19084 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : Failed to complete request: java.lang.RuntimeException: Invalid key2020-03-30 01:57:48.244 ERROR 19084 --- [io-8080-exec-10] o.a.c.c.C.[.[.[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException: Invalid key] with root causejava.lang.RuntimeException: Invalid key at com.taetaetae.hystrix.target.TargetContoller.target(TargetContoller.java:19) ~[classes/:na] ... 중략2020-03-30 01:57:48.247 DEBUG 19084 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : &quot;ERROR&quot; dispatch for GET &quot;/error?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 01:57:48.250 DEBUG 19084 --- [io-8080-exec-10] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController#error(HttpServletRequest)2020-03-30 01:57:48.256 DEBUG 19084 --- [io-8080-exec-10] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Using &apos;application/json&apos;, given [text/plain, application/json, application/*+json, */*] and supported [application/json, application/*+json, application/json, application/*+json]2020-03-30 01:57:48.257 DEBUG 19084 --- [io-8080-exec-10] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Writing [&#123;timestamp=Mon Mar 30 01:57:48 KST 2020, status=500, error=Internal Server Error, message=Invalid ke (truncated)...]2020-03-30 01:57:48.267 DEBUG 19084 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : Exiting from &quot;ERROR&quot; dispatch, status 5002020-03-30 01:57:48.267 DEBUG 19084 --- [x-MainService-1] o.s.web.client.RestTemplate : Response 500 INTERNAL_SERVER_ERROR2020-03-30 01:57:48.277 INFO 19084 --- [x-MainService-1] com.taetaetae.hystrix.main.MainService : fallbackMethod, param : taekwan2020-03-30 01:57:48.282 DEBUG 19084 --- [nio-8080-exec-9] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 01:57:48.282 DEBUG 19084 --- [nio-8080-exec-9] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;defaultResult&quot;]2020-03-30 01:57:48.283 DEBUG 19084 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : Completed 200 OK (로그가 길지만…) 위 내용을 보면 fallbackMethod 가 호출된 것을 확인할 수 있고, 응답 또한 200 OK 에 미리 지정해 둔 “defaultResult”를 내려준 것을 확인할 수 있다. 하지만 TargetContoller 에서 요청이 들어오면 로깅하려고 한 부분이 찍힌것을 보면, 에러 응답에 대한 fallbackMethod 는 호출 되었지만 실제로 서킷브레이커는 open 이 안된것을 확인할 수 있다. 그럼 좀더 빠른 시간내에 여러번 호출해보면 어떻게 될까?? 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647482020-03-30 02:03:28.040 DEBUG 19084 --- [nio-8080-exec-5] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:28.040 DEBUG 19084 --- [nio-8080-exec-5] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 02:03:28.042 DEBUG 19084 --- [x-MainService-5] o.s.web.client.RestTemplate : HTTP GET http://localhost:8080/target?key=taekwan2020-03-30 02:03:28.042 DEBUG 19084 --- [x-MainService-5] o.s.web.client.RestTemplate : Accept=[text/plain, application/json, application/*+json, */*]2020-03-30 02:03:28.046 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/target?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:28.047 DEBUG 19084 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.target.TargetContoller#target(String)2020-03-30 02:03:28.047 INFO 19084 --- [nio-8080-exec-1] c.t.hystrix.target.TargetContoller : input key : taekwan2020-03-30 02:03:28.047 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Failed to complete request: java.lang.RuntimeException: Invalid key2020-03-30 02:03:28.048 ERROR 19084 --- [nio-8080-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException: Invalid key] with root causejava.lang.RuntimeException: Invalid key at com.taetaetae.hystrix.target.TargetContoller.target(TargetContoller.java:19) ~[classes/:na] ... 중략 2020-03-30 02:03:28.048 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : &quot;ERROR&quot; dispatch for GET &quot;/error?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:28.048 DEBUG 19084 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController#error(HttpServletRequest)2020-03-30 02:03:28.049 DEBUG 19084 --- [nio-8080-exec-1] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Using &apos;application/json&apos;, given [text/plain, application/json, application/*+json, */*] and supported [application/json, application/*+json, application/json, application/*+json]2020-03-30 02:03:28.049 DEBUG 19084 --- [nio-8080-exec-1] o.s.w.s.m.m.a.HttpEntityMethodProcessor : Writing [&#123;timestamp=Mon Mar 30 02:03:28 KST 2020, status=500, error=Internal Server Error, message=Invalid ke (truncated)...]2020-03-30 02:03:28.049 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Exiting from &quot;ERROR&quot; dispatch, status 5002020-03-30 02:03:28.049 DEBUG 19084 --- [x-MainService-5] o.s.web.client.RestTemplate : Response 500 INTERNAL_SERVER_ERROR2020-03-30 02:03:28.050 INFO 19084 --- [x-MainService-5] com.taetaetae.hystrix.main.MainService : fallbackMethod, param : taekwan2020-03-30 02:03:28.051 DEBUG 19084 --- [nio-8080-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 02:03:28.051 DEBUG 19084 --- [nio-8080-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;defaultResult&quot;]2020-03-30 02:03:28.052 DEBUG 19084 --- [nio-8080-exec-5] o.s.web.servlet.DispatcherServlet : Completed 200 OK2020-03-30 02:03:28.679 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:28.679 DEBUG 19084 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 02:03:28.680 INFO 19084 --- [nio-8080-exec-1] com.taetaetae.hystrix.main.MainService : fallbackMethod, param : taekwan2020-03-30 02:03:28.681 DEBUG 19084 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 02:03:28.681 DEBUG 19084 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;defaultResult&quot;]2020-03-30 02:03:28.682 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK2020-03-30 02:03:29.327 DEBUG 19084 --- [nio-8080-exec-5] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:29.327 DEBUG 19084 --- [nio-8080-exec-5] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 02:03:29.328 INFO 19084 --- [nio-8080-exec-5] com.taetaetae.hystrix.main.MainService : fallbackMethod, param : taekwan2020-03-30 02:03:29.328 DEBUG 19084 --- [nio-8080-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 02:03:29.328 DEBUG 19084 --- [nio-8080-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;defaultResult&quot;]2020-03-30 02:03:29.329 DEBUG 19084 --- [nio-8080-exec-5] o.s.web.servlet.DispatcherServlet : Completed 200 OK2020-03-30 02:03:30.071 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/index?key=taekwan&quot;, parameters=&#123;masked&#125;2020-03-30 02:03:30.072 DEBUG 19084 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.hystrix.main.MainController#index(String)2020-03-30 02:03:30.072 INFO 19084 --- [nio-8080-exec-1] com.taetaetae.hystrix.main.MainService : fallbackMethod, param : taekwan2020-03-30 02:03:30.073 DEBUG 19084 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;text/html&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]2020-03-30 02:03:30.073 DEBUG 19084 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;defaultResult&quot;]2020-03-30 02:03:30.073 DEBUG 19084 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK 로그를 자세히 보면, 처음엔 TargetContoller에서 찍은 로그가 있으면서 fallbackMethod 의 로그가 있지만 그 뒤에는 TargetContoller에서 찍은 로그는 없고 fallbackMethod 의 로그만 있는 것을 확인할 수 있다. 즉, 위에서 설정한 타임아웃 500ms 안에 2번 에러가 발생하였기 때문에 서킷브레이커가 동작해서 TargetContoller 로의 요청을 하지 않았기 때문이다. 이렇게 되면 TargetContoller입장에서는 연속적으로 요청이 오지 않기 때문에 부담이 줄어드는 효과가 있다. # 모니터링서비스를 운영하면서 서킷브레이커가 동작되었는지를 확인하기 위해서 위에서 했던것 처럼 곳곳에 심어둔 로그를 눈이 빠져라 보고 있어야만 할까? Netflix 의 Hystrix 는 아주 우아하게 이를 웹에서 모니터링 할 수 있게도 구현해놨다. (참 대단하다…) 그럼 모니터링을 해보자.모니터링을 하기 위해서는 dependency 를 추가해야 한다.123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; SpringBoot에서 제공하는 actuator 를 활용하여 정보를 받도록 해주고, application.properties 설정에서도 Hystrix 관련된 정보를 받아볼 수 있게 아래처럼 추가해주자.1management.endpoints.web.exposure.include=hystrix.stream 그럼 끝이다. (응? 이게 끝이라고?) 그렇다. 필자도 아주 놀랬는데 생각해보니 모니터링을 위해 별도의 코드가 들어가는 자체가 이상한 부분같다. 아주 심플하게 모듈과 설정만 추가해주면 모니터링이 가능하다. 그럼 진짜 모니터링을 해보자. 일부러 여러번 호출해서 서킷브레이커를 발동시키면 모니터링 페이지에서는 어떤식으로 나오는지 확인해보자. 우선 이렇게 설정한뒤 실행을 하면 /hystrix로 접근이 가능하고 귀엽지만 뭔가 놀랜 표정의 곰돌이가 반기는 것을 확인할 수 있다. 그다음 url 에 http://localhost:8080/actuator/hystrix.stream 를 입력후 “Monitor Stream”을 클릭하면 아래와 같은 화면을 볼 수 있다. 곰돌이가 조금 화난것 같다. (코딩좀 잘하라고?)곰돌이가 조금 화난것 같다. (코딩좀 잘하라고?) 뭔가 로딩중인것 같은데 올바른 요청 /index?key=taetaetae 을 해보면 그래프가 바뀌고 잘못된 요청 /index?key=taekwan 을 여러번 해보다가 잠시 멈추고 를 반복해보면 서킷브레이커가 open 되었다가 다시 close 된것을 확인할 수 있다. 에러가 발생하면 open, 설정한 시간이 지나고 정상 응답이 되면 close에러가 발생하면 open, 설정한 시간이 지나고 정상 응답이 되면 close # 마치며물론 위에서 설정한 내용은 서비스에 적용하기도 부끄러울만큼 아주 극단적이고 다양한 상황이 전혀 고려되지 않은 설정값이다. 현 시스템에 맞춰 설정값을 커스터마이징 하며 최적의 설정값을 찾아야 할 것이다. 또한 무조건 “설정한 값에 의해서 서킷브레이커가 잘 동작 하겠지” 라고 믿는것(?)보다 모니터링을 추가로 설정해서 실제로 언제 서킷브레이커가 작동을 하는지 확인을 해봐야 할 것 같다. 나아가서 이 모니터링 페이지가 있다고 해서 주식 차트 보는것 마냥 계속 보고 있을수는 없는일. 모니터링 페이지를 봐야할 시점이 오게 된다면 자동으로 알림을 받도록 해서 필요할 때만 모니터링을 할 수 있도록 해야 할 것 같다. (이상하게 주식으로 시작해서 주식으로 끝나는 것 같은 느낌은 뭐지…) 물론 이번에도 위에서 사용한 코드는 필자의 Github Repo에서 확인이 가능하다. 참고 url https://github.com/Netflix/Hystrix https://spring.io/guides/gs/circuit-breaker/ https://woowabros.github.io/experience/2017/08/21/hystrix-tunning.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"circuit breaker","slug":"circuit-breaker","permalink":"https://taetaetae.github.io/tags/circuit-breaker/"},{"name":"Netflix","slug":"Netflix","permalink":"https://taetaetae.github.io/tags/Netflix/"},{"name":"Hystrix","slug":"Hystrix","permalink":"https://taetaetae.github.io/tags/Hystrix/"}]},{"title":"매니저는 정말 개발자의 무덤일까? (리뷰 - 개발자 7년차, 매니저 1일차)","slug":"7-years-of-development-1st-day-of-manager","date":"2020-03-26T14:37:17.000Z","updated":"2020-04-23T04:41:36.637Z","comments":true,"path":"2020/03/26/7-years-of-development-1st-day-of-manager/","link":"","permalink":"https://taetaetae.github.io/2020/03/26/7-years-of-development-1st-day-of-manager/","excerpt":"개발자로서의 커리어는 정말 다양하지만 필자가 보고 들은 경험을 아주 일반화 시켜 정리해 보자면 다음과 같다.처음엔 전공/비전공을 불문하고 신입으로 개발을 시작하여 다양한 개발 경험을 하게 된다.","text":"개발자로서의 커리어는 정말 다양하지만 필자가 보고 들은 경험을 아주 일반화 시켜 정리해 보자면 다음과 같다.처음엔 전공/비전공을 불문하고 신입으로 개발을 시작하여 다양한 개발 경험을 하게 된다. 사수에게 혼나기도 해보고 또는 혼내줄 사수가 없어 혼자 끙끙 밤도 새보고, 다크서클과 거북목을 겸비한 이른바 “삽질”을 하며 고통의 시절을 보내고 나면 어느덧 승진(진급)을 하며 일정 규모의 “팀장(혹은 관리자)”이 된다. 그게 자의든 타의든. 개발자는 다소 “기술”이라는 특수성을 가지고 있지만 어느 직군이든 간에 이러한 커리어 패스의 흐름은 매우 비슷하게 흘러가는 것 같다. 적어도 필자가 보고 들은 것만 보면 말이다. (예외 케이스는 항상 있지만…)하루는 팀장님과의 면담 중에 “이제는 마냥 눈앞에 있는 개발만 할 것이 아니다. 기술을 좀 더 깊게 들여다보는 자리와 사람을 관리하며 주어진 과제를 진행하는 자리, 둘 중 선택해야 하는 시기가 온 것 같다. 더 높고 더 멀리, 그리고 더 넓게 볼 줄 알아야 한다.”라는 말씀을 듣게 된다. 어느덧 “그 시점”이 다가온 것이다. 개인적으로 필자는 팀장님이 말씀하신 두 가지 중 전자에 좀 더 가깝게 다가가고 싶다. 그만큼 오래오래 “실무 개발”을 하고 싶고, 또 그만큼 개발이 재밌기 때문이다. 아직도 눈앞의 문제를 해결하기 위해 개발하며 시간 가는 줄 모를 만큼 밤을 새우는 게 재미있는 걸 보면… 요리하는 걸 좋아하지만 이상하게 치킨집은 하고 싶지 않다. 출처 : https://catapult.tistory.com/entry/%EC%B9%98%ED%82%A8%EC%A7%91%EC%9D%B4%EB%82%98-%EC%B0%A8%EB%A0%A4%EC%95%BC%EC%A7%80요리하는 걸 좋아하지만 이상하게 치킨집은 하고 싶지 않다. 출처 : https://catapult.tistory.com/entry/%EC%B9%98%ED%82%A8%EC%A7%91%EC%9D%B4%EB%82%98-%EC%B0%A8%EB%A0%A4%EC%95%BC%EC%A7%80 어느 날 SNS 피드에 개발 관련된 소식들을 받아보다가 개발 7년차. 매니저 1일차라는 제목의 책을 보게 된다. 뭐야, 이거 내 이야기 아니야? 하며 귀신에 홀린 듯 사서 읽어보려는 찰나, 마침 한빛미디어 에서 주최하는 나는 리뷰어다 라는 이벤트를 발견하게 된다. 결국 리뷰어에 당첨이 되고 운 좋게 해당 책을 받아볼 수 있었다. (이 책을 읽게 해준 한빛미디어 측에게 이 글로나마 감사의 인사를 전하고 싶다.) 필자의 SNS를 장식했던 개발 7년차, 매니저 1일차필자의 SNS를 장식했던 개발 7년차, 매니저 1일차 이번 포스팅에서는 우선 책에 대한 리뷰를 간단히 적어보고 거기에 필자의 생각을 조금 더 얹어보고 싶다. 필자를 두고 만들어진 책 같아서 아직도 책 표지만 봐도 신기하고 설렌다. 일단 책 표지나 제목이 맘에 든 건 감출 수 없는 사실이다. # 신입 혹은 주니어 개발자가 읽어봐도 좋을 책.제목만 보면 이제 갓 팀장 혹은 매니저를 하게 되는 사람에게만 해당되는 책으로 보인다. 표지 상단에 “개발만 해왔던 내가, 어느 날 갑자기 ‘팀’을 맡았다!” 적혀있기도 했으니까. 하지만 책을 읽다 보면 꼭 그렇지마는 않다. 멘토링을 할 때엔 멘토와 멘티 각자의 위치에서 어떤 자세로 서로를 맞이해야 하는 방법에 대해서도 알려주기도 하고 무작정 눈앞에 있는 기능 개발만을 하며 안갯속을 걷는 주니어 개발자가 미리 미래를 경험해보는 좋은 사례를 들어 알려주고 있기 때문이다.꼭 누군가 혹은 무언가를 “관리”하는 입장이 아닌 “팀”이라는 공동체 사회, 특히 개발 팀에서 팀원들과 협력하는 방법론을 살펴보고 있고, 경력이 낮으면 안 보이는 부분들까지 마치 멀리 있는 것을 대신 망원경으로 보여주는 느낌이 들었다. 앞부분에는 “이 책을 읽는 방법”이라며 상황별로 읽는 챕터를 가이드 해주고 있지만 사실 어느 하나 중요하지 않을 내용이 없어서 처음부터 무언가에 홀린 듯 읽을 수밖에 없었고 선배님이 앞서 지나간 길을 올바르게 지나갈 수 있도록 가이드 해주는 느낌으로 중간중간 사례가 있어서 현업에 있어서 그런지 좀 더 쉽게 읽힐 수 있었다. # 다 읽고서야 알아차린 번역서(?)라는 사실.어떠한 XX 기술 서적에서는 Method를 ‘방법’, Overriding 을 ‘과적’이라고 번역한 책들이 있는가 반면, 이 책은 읽는 내내 국내 어떤 분이 쓰신 거라 생각하고 읽어내려 갔지만 다 읽고 보니 외국에 어느 CTO가 쓴 책을 옮겨서 다시 써진 책이었다. 그만큼 전혀 특유의 번역 느낌(?)은 없었고 오히려 한국 문화에 맞춰 다시 써진 건 아닐까 싶을 정도로 너무 술술 잘 읽혔다.더불어 책 중간중간에 이 바닥(?)에서 유명하신 분들이 기고해 주신 소중한 경험담과 생각들을 덤으로 읽어볼 수 있어서 너무 좋았고 챕터가 끝날 때 즈음이면 생각해 볼 만한 질문을 던지면서 무작정 읽지 말고 깊게 생각하고 읽으라고 하는 것만 같은 저자의 목소리를 들을 수가 있어 좋았다. # 기타.특이한 건, 책 맨 뒤에 보면 키워드를 다시 찾아서 읽어볼 수 있도록 “찾아보기” 코너가 있어서 언제든지 위기 상황(?)에서 가이드처럼 활용해 볼 만한 부분인 것 같아 좋았고, 시간이 지나고 정말 “관리자”가 된다면 다시 한번 처음부터 정독을 하며 보다 나은 “관리자”가 되어보고 싶은 맘도 들었다. 정곡을 찌르는 문구들, 사실 이것 말고도 무궁무진하다.정곡을 찌르는 문구들, 사실 이것 말고도 무궁무진하다. # 기억에 남는 매니저.짧은 개발자 경력 중에 필자가 만나보았던 “매니저” 중 그래도 괜찮았다고 생각나는 분은 세분 정도다. C : 마이크로 매니징의 끝판왕. 신입시절인 필자가 Java 코딩을 할 때 StringBuilder 을 쓰는지 StringBuffer을 쓰는지까지 확인하거나 SpringFramework의 일부 모듈 코드를 A4로 출력해서 보는 것을 추천하신 분. 어떻게 보면 정말 토나오게(?) 힘들었지만 지나고 보면 그러한 관심도 요즘은 가뭄에 콩 나듯 있을까 하기에 나름 뜨거운 정이 있으셨던 분으로 기억한다. J : 필자에게 관심이 있는 듯 없는듯하면서 가끔 지나가다 뒤통수를 치는 듯한 몸 쪽 깊숙한 멘트로 성장하는데 엄청난 원동력이 되어주신 분. 회사 업무 그 이상을 주문하고 필자가 스스로 개발/개선 포인트를 찾아서 하는 방법을 알려주신 분. S : 아, 이 분은 정말 천재구나 할 정도로 높이 그리고 멀리 보는데 일가견이 있으신 분. 필자가 개발하는 데 어려움을 겪고 있으면 키워드 몇 개를 던져주면서 감을 주지 않고 감을 따는 방법을 알려주신 분. 물론 다른 분들도 필자에게 영향력이 있던 분들이지만, 필자가 생각하는 바라직한 “매니저”의 역할은 팀이 목표에 방해요소를 최대한 줄이고 팀원들의 성장을 도와주며 비로소 나아가고자 하는 방향을 명확하고 신뢰 있는 말과 행동이라 생각한다. 워낙에 유명한 짤이라 다른 미사여구가 필요 없지 않을까 싶다. 출처 : https://twitter.com/hazaraouad/status/451603771869900800워낙에 유명한 짤이라 다른 미사여구가 필요 없지 않을까 싶다. 출처 : https://twitter.com/hazaraouad/status/451603771869900800 # 마치며“매니저”라는 안 좋은 인식을 벗어나게 해줄 내용들이 정말 많이 있고 필자 또한 “오랫동안 개발만 하고 싶어”라는 생각을 조금 달리해볼 수 있는 안경을 쓰게 해준 것 같아 좋았던 책이다.더불어, 멘토링 하던 시절이 생각나게 하는 책이다. 멘토로써 준비를 하고 필자 자신에게, 그리고 멘티에게 보다 더 좋은 촉진제 역할이 되었더라면 하는 아쉬움이 남는 멘토링. 다음에 기회가 되면 조금 더 “준비”를 해서 이 책에 나와있는 “좋은 매니저”의 가이드를 기반으로 비록 작은 날갯짓이지만 멀리 보면 팀에 도움이 되는 그런 사람이 되고 싶게 만드는 책으로 오래 기억에 남을 것 같다.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"book","slug":"book","permalink":"https://taetaetae.github.io/tags/book/"}]},{"title":"조금 더 괜찮은 Rest Template 1부 - Retryable","slug":"better-rest-template-1-retryable","date":"2020-03-22T06:30:35.000Z","updated":"2020-04-23T04:41:36.693Z","comments":true,"path":"2020/03/22/better-rest-template-1-retryable/","link":"","permalink":"https://taetaetae.github.io/2020/03/22/better-rest-template-1-retryable/","excerpt":"웹 어플리케이션을 만들면서 꼭 한번 쯤 만나게 되는 “RestTemplate”. 접근 가능한 외부 HTTP URL(보통 API)을 호출하는 방법중에 하나로 springframework 에서 제공해주는 모듈이다. 특히 큰 한덩어리로 관리되던 Monolithic Architecture 에서 요청을 하고(client) 응답을 주는(server)","text":"웹 어플리케이션을 만들면서 꼭 한번 쯤 만나게 되는 “RestTemplate”. 접근 가능한 외부 HTTP URL(보통 API)을 호출하는 방법중에 하나로 springframework 에서 제공해주는 모듈이다. 특히 큰 한덩어리로 관리되던 Monolithic Architecture 에서 요청을 하고(client) 응답을 주는(server) 즉, Endpoint가 작은 단위로 분리되는 Microservice Architecture 로 바뀌면서 각 서비스간 호출방식이 HTTP 일 경우 자주 사용되곤 하는 것 같다. (webClient 등 다른 여러 호출 방법들이 있다.)만약, 요청을 하는 클라이언트 입장에서 응답을 주는 서버의 상태가 불안정 하다고 가정했을때, 어떤식으로 처리해야 할까? 예컨대, 요청 10번에 한번은 어떠한 이슈로 응답이 지연되거나 서버에러가 발생한다고 하면 클라이언트를 사용하는 사용자 입장에서는 간헐적인 오류응답에 답답함을 호소할 수도 있다. 그럼 잠시 눈을 감고 생각해보자.가볍게 생각하면 아래처럼 아주 간단하게 “예외처리”를 이용할 수도 있다.12345try &#123; // http call&#125; catch (Exception e)&#123; // 서버에러가 아닌 약속된 에러응답을 리턴&#125; 하지만 이것도 정답이 아닐수 있는게, “간헐적인 오류”로 인해 사용자는 오류화면을 봐야하기 때문에 클라이언트에 대한 신뢰를 저버릴 수밖에 없다. 그럼 어떻게 해야할까? 여러가지 해결방법이 있겠지만 간단하면서도 강력하다고 생각되는 방법이 바로 “재시도” 라고 생각한다. 클라이언트를 사용하는 사용자가 눈치 못챌만큼 빠르게 재시도를 한다면 에러가 나도 다시한번 호출해서 성공할 수 있는 가능성이 높기 때문이다. (그치만 근본적인 원인은 해결해야…) 실제로 조금있다 해보면 되는 경우가 많으니 안될때는 조금 (천천히) 시도해보자. 출처 : http://www.segye.com/newsView/20200302504384실제로 조금있다 해보면 되는 경우가 많으니 안될때는 조금 (천천히) 시도해보자. 출처 : http://www.segye.com/newsView/20200302504384 이번 포스팅에서는 RestTemplate 를 이용할때 “재시도” 할 수 있는 방법에 대해 알아보고자 한다. 아주 간단할지 모르지만 노력에 비해 효과가 상당하다고 생각하기 때문에 정리해 두고 싶었다. # Spring Retry공식 Github에 소개를 빌리자면, Spring 어플리케이션에 대한 재시도 지원을 제공한다고 한다. 위에서 이야기 했던 “RestTemplate”과는 사실 무관하고, 이를 활용해서 재시도 하는 “RetryRestTemplate”를 구현해보려 하는것이다. 우선 이 “Spring-Retry”의 예제를 보면 아주 심플하게 사용할 수 있다. 우선 pom에 구현에 필요한 dependency 를 추가하고 아래 코드를 보자.12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910111213141516171819202122@Configuration@EnableRetry // 1public class Application &#123; @Bean public Service service() &#123; return new Service(); &#125;&#125;@Serviceclass Service &#123; @Retryable(RemoteAccessException.class) // 2 public void service() &#123; // ... do something &#125; @Recover // 3 public void recover(RemoteAccessException e) &#123; // ... panic &#125;&#125; @EnableRetry 어노테이션을 @Configuration을 지정한 클래스 중 하나에 추가한다. 재시도 하려는 메소드에 @Retryable 어노테이션을 지정해준다. 재시도가 완료되는 시점에서 실행하고 싶을때 선언하는 어노테이션, @Retryable 동일한 클래스에서 선언되어야 하고 return type 은 @Retryable을 지정한 메소드와 동일해야 한다. # Retry Rest Template이렇게 springframework 에서 제공해주는 spring-retry 를 이용해서 이번 포스팅의 목표인 재시도를 하는 Retry Rest Template 를 구성해보자. 우선, RestTemplate 를 Bean 으로 등록하고, 위에서 이야기 한 어노테이션들로 구성해보자.123456789101112131415161718192021222324252627@EnableRetry@Configurationpublic class RetryableRestTemplateConfiguration &#123; @Bean public RestTemplate retryableRestTemplate() &#123; SimpleClientHttpRequestFactory clientHttpRequestFactory = new SimpleClientHttpRequestFactory(); // 1 clientHttpRequestFactory.setReadTimeout(2000); clientHttpRequestFactory.setConnectTimeout(500); RestTemplate restTemplate = new RestTemplate(clientHttpRequestFactory) &#123; @Override @Retryable(value = RestClientException.class, maxAttempts = 3, backoff = @Backoff(delay = 1000)) // 2 public &lt;T&gt; ResponseEntity&lt;T&gt; exchange(URI url, HttpMethod method, HttpEntity&lt;?&gt; requestEntity, Class&lt;T&gt; responseType) throws RestClientException &#123; return super.exchange(url, method, requestEntity, responseType); &#125; @Recover public &lt;T&gt; ResponseEntity&lt;String&gt; exchangeRecover(RestClientException e) &#123; return ResponseEntity.badRequest().body(\"bad request T.T\"); // 3 &#125; &#125;; return restTemplate; &#125;&#125; SimpleClientHttpRequestFactory 를 만들고 각 타임아웃을 설정해준 다음 RestTemplate 파라미터로 넘겨준다. 사용하는 곳에서 exchange 메소드를 이용할 것이므로 해당 메소드를 오버라이드 해준다. 먼저 해당 메소드에서 “RestClientException”이 발생할 경우 Retry 로직을 수행한다고 정해주고, 최대 시도는 3번, backoff 설정중 delay를 1000ms(1초)로 지정해서 재시도가 진행되도록 해준다. 2 에서 지정한 재시도가 끝나면 (재시도를 전부 다 하면) 해당 메소드를 수행하게 되어있고, 임의로 응답에 지정한 문구를 넘겨준다. 이렇게 하고 실제로 사용하는 로직에서 일부러 잘못된 URL을 호출해 보도록 하자. 그리고서 로그를 자세히 보도록 application.properties 에 “debug=true” 설정을 해준다.123456789101112131415@RestControllerpublic class TestController &#123; @Autowired private RestTemplate retryableRestTemplate; @GetMapping(value = \"/employees\", produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String employees() throws URISyntaxException &#123; final String baseUrl = \"http://dummy.restapiexample.com/api/v1/employeeszzz\"; // zzz 가 빠져야 한다. URI uri = new URI(baseUrl); ResponseEntity&lt;String&gt; exchange = retryableRestTemplate.exchange(uri, HttpMethod.GET, null, String.class); return exchange.getBody(); &#125;&#125; 이렇게 하고 실행을 시켜보면 다음과 같이 재시도 관련 로깅이 찍히는 것을 볼 수 있고123456789101112131423:05:50.893 DEBUG 21016 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : GET &quot;/employees&quot;, parameters=&#123;&#125;23:05:50.898 DEBUG 21016 --- [nio-8080-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.taetaetae.retryableresttemplate.TestController#employees()23:05:50.994 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : HTTP GET http://dummy.restapiexample.com/api/v1/employeeszzz23:05:50.999 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Accept=[text/plain, application/json, application/*+json, */*]23:05:51.861 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Response 404 NOT_FOUND23:05:52.869 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : HTTP GET http://dummy.restapiexample.com/api/v1/employeeszzz23:05:52.869 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Accept=[text/plain, application/json, application/*+json, */*]23:05:53.603 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Response 404 NOT_FOUND23:05:54.605 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : HTTP GET http://dummy.restapiexample.com/api/v1/employeeszzz23:05:54.606 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Accept=[text/plain, application/json, application/*+json, */*]23:05:55.305 DEBUG 21016 --- [nio-8080-exec-1] org.springframework.web.HttpLogging : Response 404 NOT_FOUND23:05:57.192 DEBUG 21016 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Using &apos;application/json;q=0.8&apos;, given [text/html, application/xhtml+xml, image/webp, image/apng, application/xml;q=0.9, application/signed-exchange;v=b3;q=0.9, */*;q=0.8] and supported [application/json]23:05:57.193 DEBUG 21016 --- [nio-8080-exec-1] m.m.a.RequestResponseBodyMethodProcessor : Writing [&quot;bad request T.T&quot;]23:05:57.202 DEBUG 21016 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed 200 OK 브라우저에서 해당 url을 접근해보면 @Recover 에서 지정했던 결과를 볼 수 있게 된다. 사실 이러한 방법에 대해 삽질하기 전에 다른 방법들을 찾아봤지만 restTemplate 을 사용하는 곳에서 각각 retry관련 로직을 추가해야 했기에 뭔가 어글리해 보여서 삽질을 시작하게 되었다. 다행히 성공. 예제로 임의 문자열을 리턴했지만 상황에 따라 얼마든지 커스터마이징이 가능하다.예제로 임의 문자열을 리턴했지만 상황에 따라 얼마든지 커스터마이징이 가능하다. # 마치며어떻게 보면 너무나 당연하게 “여러번 재시도 요청하면 되지?” 라는 말을 할 수 있지만, “입코딩” 하는 것과 실제로 코드를 구현하는건 다른 이야기인것 같다. 정말 작은 로직 추가로 꽤 큰 효과를 볼 수 있어 다행이라 생각한다.이러한 “재시도” 말고도 요청하고자 하는 곳의 서버의 상태가 안좋을 때 서버에러가 아닌 다른 명시적인 에러를 반환할 수 있는 방법이 다양할 것 같다. 모든것엔 정답이 없는 것 처럼.제목에서 알 수 있듯이 다음 “2부” 에서는 “Retry”가 아닌 “Circuit Breaker”를 사용하여 “재시도”의 방법보다 조금 다른 측면에서 조금 더 괜찮은 방법으로 RestTemplate 를 사용해 보고자 한다. 위에서 사용한 코드는 필자의 Github Repo에서 확인이 가능하다. 참고 url https://dzone.com/articles/how-to-use-spring-retry https://www.baeldung.com/spring-retry https://github.com/spring-projects/spring-retry https://docs.spring.io/spring-batch/docs/current/reference/html/retry.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"retryable","slug":"retryable","permalink":"https://taetaetae.github.io/tags/retryable/"}]},{"title":"SpringRestDocs를 SpringBoot에 적용하기","slug":"spring-rest-docs-in-spring-boot","date":"2020-03-08T14:16:59.000Z","updated":"2020-04-23T04:41:36.887Z","comments":true,"path":"2020/03/08/spring-rest-docs-in-spring-boot/","link":"","permalink":"https://taetaetae.github.io/2020/03/08/spring-rest-docs-in-spring-boot/","excerpt":"API를 개발하고 제공하기 위해서는 그에 해당하는 API 명세를 작성해서 사용하는 곳에 전달하게 된다. 어떤 URL에 어떤 파라미터를 사용해서 어떻게 요청을 하면 어떤 결과를 응답으로 내려주는지에 대한 관련 정보들. 이러한 “API 문서” 를 제공하는 방식은 상황에 따라 다양한 방법으로 사용되곤 한다.","text":"API를 개발하고 제공하기 위해서는 그에 해당하는 API 명세를 작성해서 사용하는 곳에 전달하게 된다. 어떤 URL에 어떤 파라미터를 사용해서 어떻게 요청을 하면 어떤 결과를 응답으로 내려주는지에 대한 관련 정보들. 이러한 “API 문서” 를 제공하는 방식은 상황에 따라 다양한 방법으로 사용되곤 한다. API 코드와 해당 문서의 동기화가 자동으로 되어야 조금 편해질것 같다는 생각이 들었다. 출처 : https://dribbble.com/shots/3386291-API-DocumentationAPI 코드와 해당 문서의 동기화가 자동으로 되어야 조금 편해질것 같다는 생각이 들었다. 출처 : https://dribbble.com/shots/3386291-API-Documentation 필자는 주로 “위키”(또는 일반 문서)를 활용해서 전달하곤 했었는데 API의 형태가 달라질 때마다 해당 위키를 수정해야만 하는 번거로움이 있었다. API 수정하면 위키도 수정하고. 깜박하고 위키 수정을 안하게 될 경우 왜 API 명세가 다르냐는 문의가… 그러다 알게된 Spring Rest Docs. (아무리 좋은 기술, 좋은 툴 이라 해도 실제로 본인이 필요로 하고 사용을 해야하는 이유가 생길때 비로소 빛을 발하는것 같은 느낌이다.) 이 포스팅에서는 swegger 와 비교하는 내용은 제외할까 한다. 워낙 유명한 두 양대 산맥(?)이라 검색해보면 각각의 장단점이 자세히 나와있기에… 최근 들어 TestCode 의 중요성을 절실하게 느끼고 있었고, TestCode 를 작성하면 자연스럽게 문서를 만들어 주는 부분이 가장 매력적이라고 생각이 들었다. 이를 반대로 생각하면, TestCode 가 실패할 경우 빌드 자체가 안되기에 어쩔수 없이 TestCode를 성공시켜야만 하고, 자연스럽게 정상적인(최신화 된) API 문서가 만들어지게 된다.이번 포스팅에서는 다음과 같은 목표를 두고 실무에서 언제든지 활용이 가능한 약간의 “가이드” 같은 내용으로 작성해 보고자 한다. Spring Boot 최신 버전에서 Spring Rest Docs 를 설정한다. 임의의 API 를 만들고 그에 따른 TestCase 를 작성한다. Spring.profile 에 따라 Spring Rest Docs Url 을 접근 가능/불가능 할 수 있게 한다. 물론 필자의 방법이 다를수도 있지만, 이러한 방법을 토대로 보다 더 우아하고 아름다운 방법을 알아갈수 있지 않을까 하는 기대로. # Spring Boot 에 Spring Rest Docs 셋팅하고 TestCase 작성하기우선 Spring Boot 프로젝트를 만든다. https://start.spring.io/ 에서 만들어도 되고 IDE 에서 제공하는 툴로 만들어도 되고. 만드는 방식은 무방하다. 그 다음 필요한 dependency 를 추가해 준다.12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.restdocs&lt;/groupId&gt; &lt;artifactId&gt;spring-restdocs-mockmvc&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 임의로 API를 작성하고 모델 1234567@Getter@Setterpublic class Book &#123; private Integer id; private String title; private String author;&#125; 컨트롤러 123456789101112@RestControllerpublic class BookController &#123; @GetMapping(\"/book/&#123;id&#125;\") public Book getABook(@PathVariable Integer id) &#123; Book book = new Book(); book.setId(id); book.setTitle(\"spring rest docs in spring boot\"); book.setAuthor(\"taetaetae\"); return book; &#125;&#125; 해당 컨트롤러에 대한 TestCase 를 작성하자.12345678910111213141516171819202122232425262728@WebMvcTest(BookController.class)@AutoConfigureRestDocs // (1)public class BookControllerTest &#123; @Autowired private MockMvc mockMvc; // (2) @Test public void test_책을_조회하면_null이_아닌_객체를_리턴한다() throws Exception &#123; mockMvc.perform(get(\"/book/&#123;id&#125;\", 1) .accept(MediaType.APPLICATION_JSON)) .andDo(MockMvcResultHandlers.print()) .andExpect(MockMvcResultMatchers.status().isOk()) .andDo(document(\"book\", // (3) pathParameters( parameterWithName(\"id\").description(\"book unique id\") // (4) ), responseFields( fieldWithPath(\"id\").description(\"book unique id\"), fieldWithPath(\"title\").description(\"title\"), fieldWithPath(\"author\").description(\"author\") ) )) .andExpect(jsonPath(\"$.id\", is(notNullValue()))) // (5) .andExpect(jsonPath(\"$.title\", is(notNullValue()))) .andExpect(jsonPath(\"$.author\", is(notNullValue()))); &#125;&#125; (1) Spring Boot 에서는 해당 어노테이션으로 여러줄에 걸쳐 설정해야 할 Spring Rest Docs 관련 설정을 아주 간단하게 해결할 수 있게 된다. (참고)(2) 공식 도큐먼트 에서는 4가지 방식을 말하고 있는데 이 포스팅 에서는 “MockMvc” 을 사용하고자 한다.(3) “book” 이라는 identifier 를 지정하면 해당 TestCase 가 수행될때 snippets 가 생성되는데 해당 identifier 묶음으로 생성이 된다.(4) request의 파라미터 필드, response의 필드의 설명을 적어줌으로써 이 정보를 가지고 snippets 가 생성이 되고 결과적으로 API 문서가 만들어 진다.(5) 필자가 가장 매력적이라 생각되는 부분. 이 부분에서 테스트를 동시에 함으로써 응답이 달라지거나 잘못된 응답이 내려올 경우 TestCase가 실패하게 되어 API문서 또한 생성되지 않게 된다. 여기까지 한 뒤 빌드시 문서가 제대로 만들어 지도록 빌드 플러그인을 추가해 준다.1234567891011121314151617181920212223242526272829&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctor-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;generate-docs&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;process-asciidoc&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;backend&gt;html&lt;/backend&gt; &lt;doctype&gt;book&lt;/doctype&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.restdocs&lt;/groupId&gt; &lt;artifactId&gt;spring-restdocs-asciidoctor&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-restdocs.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 빌드 플러그인에 의해 snippets를 조합하여 최종 API 문서가 만들어질수 있도록 src/main/ 하위에 asciidoc 이라는 폴더를 만들고 그 하위에 적당한 이름의 adoc 파일을 작성한다. 파일 이름은 나중에 만들어질 API문서의 html 파일 이름이 된다.12345678910111213141516171819= RESTful Notes API Guide:doctype: book:icons: font:source-highlighter: highlightjs:toc: left:toclevels: 4:sectnums::sectlinks::sectanchors:[[api]]== Book Apiapi 에 관련된 설명을 이곳에 적습니다.include::&#123;snippets&#125;/book/curl-request.adoc[]include::&#123;snippets&#125;/book/http-request.adoc[]include::&#123;snippets&#125;/book/path-parameters.adoc[]include::&#123;snippets&#125;/book/http-response.adoc[]include::&#123;snippets&#125;/book/response-fields.adoc[] 위 adoc 파일 작성은 생소하니 asscidoc document를 참조하면서 작성하면 도움이 될것같다.그 다음 메이븐 빌드를 하면 TestCase 가 돌면서 snippets 이 생성되고 이를 가지고 빌드 플러그인에 의해 최종 API 문서가 생성이 된다.123456789101112131415161718mvn install[INFO] -------------------------------------------------------[INFO] T E S T S[INFO] -------------------------------------------------------[INFO] Running com.taetaetae.springrestdocs.books.BookControllerTest...[INFO] Results:[INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO] [INFO] [INFO] --- asciidoctor-maven-plugin:1.5.8:process-asciidoc (generate-docs) @ spring-rest-docs ---[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.[INFO] Copying 0 resource[INFO] Rendered 경로\\spring-rest-docs\\src\\main\\asciidoc\\index.adoc 그러면 target/generated-docs 하위에 index.html 파일이 생성된 것을 확인할 수 있다.생성된것을 볼 수 있고 자동으로 만들어진 snippets들도 볼 수 있다.생성된것을 볼 수 있고 자동으로 만들어진 snippets들도 볼 수 있다. 이 파일을 열어보면 다음처럼 이쁘게 문서가 작성된 것을 확인할 수 있다.얼마든지 입맛에 맞게 수정이 가능하다.얼마든지 입맛에 맞게 수정이 가능하다. 그러면 이 파일을 어떻게 사용하는 곳에 제공할 수 있을까? html 만 따로 전달해야 할까? 메이븐 플러그인 중 “maven-resources-plugin”을 활용하여 API서버를 띄우면 외부에서 URL 로 접근할 수 있도록 설정해보자.1234567891011121314151617181920212223242526272829&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-resources&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; $&#123;project.build.outputDirectory&#125;/static/docs &lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt; $&#123;project.build.directory&#125;/generated-docs &lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 쉽게말해, 만들어진 문서를 특정 경로로 복사하게 되면 외부에서도 해당 경로로 접근시 볼 수 있게 된다. 지금은 “{domain}/docs/index.html” 을 접근하면 문서를 볼 수 있다. # Spring.profile 에 따라 Spring Rest Docs Url 을 접근 가능/불가능 할 수 있게 한다.필요에 따라 운영환경에서는 해당 API 명세를 보여주게 되면 보안상으로 좋지 못하므로 API 문서의 접근을 막아야 한다. 뭔가 아주 간단하게 spring boot 의 설정을 통해 처리 할 수 있을꺼라 생각했지만, 아무리 찾아봐도 그런 기능은 없어보인다.ㅠㅠ 어쩔수 없이 필터를 추가하여 해당 url을 운영환경에서는 FORBIDDEN 처리를 해보도록 하자.1234567891011121314151617181920212223242526272829@Component@WebFilter(urlPatterns = \"/docs/index.html\")public class SpringRestDocsAccessFilter implements Filter &#123; @Value(\"$&#123;spring.profiles.active&#125;\") private String phase; @Override public void init(FilterConfig filterConfig) &#123; &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; if (StringUtils.equals(\"release\", phase)) &#123; // (1) HttpServletResponse response = (HttpServletResponse)servletResponse; response.sendError(HttpServletResponse.SC_FORBIDDEN); filterChain.doFilter(servletRequest, response); return; &#125; filterChain.doFilter(servletRequest, servletResponse); &#125; @Override public void destroy() &#123; &#125;&#125; (1) spring profile 이 release 일 경우 일부러 FORBIDDEN 처리를 해준다. 이렇게 되면 개발환경에서만 접근이 가능하고 운영환경에서는 접근이 불가능한, 테스트 케이스를 성공한 동기화 + 자동화 된 API 문서를 만들 수 있게 되었다. # 마치며언제나 그렇듯, 초기 설정은 실제 비즈니스 로직 개발보다 훨씬 공수가 더 드는 건 어쩔 수없다. 하지만 약간의 노력으로 안정되고 자동화된 개발 프로세스를 구축한다면 그 작은 날갯짓이 나중엔 큰 바람을 일으킬 수 있지 않을까 하는 기대를 해본다. 참, 위에서 만든 코드는 Github 에 있으니 참고 바란다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"spring rest docs","slug":"spring-rest-docs","permalink":"https://taetaetae.github.io/tags/spring-rest-docs/"}]},{"title":"Jupyter 설치하고 원격접속까지 (for 파.알.못)","slug":"jupyter-install","date":"2020-02-09T11:06:15.000Z","updated":"2020-04-23T04:41:36.785Z","comments":true,"path":"2020/02/09/jupyter-install/","link":"","permalink":"https://taetaetae.github.io/2020/02/09/jupyter-install/","excerpt":"파이썬이라는 언어는 다른 프로그래밍 언어들에 비해 쉽고 직관적이라 그런지 프로그래밍을 처음 시작하는 사람들에게 더욱이 주목을 받고 있는것 같다. 정말 다양한 모듈들이 많아 여러분야에서 활용되고 있고","text":"파이썬이라는 언어는 다른 프로그래밍 언어들에 비해 쉽고 직관적이라 그런지 프로그래밍을 처음 시작하는 사람들에게 더욱이 주목을 받고 있는것 같다. 정말 다양한 모듈들이 많아 여러분야에서 활용되고 있고 특히 언제부터인가 핫! 해진 분야(?)라 해도 과언이 아닐정도인 “머신러닝” 분야에서도 다양하게 사용되고 있는것 같다.마침 필자가 속해 있는 팀 내에 머신러닝 스터디가 시작이 되었고, 그에 파이썬을 이용하여 스터디를 해야하는 상황. 하지만 스터디를 하는 팀원 절반 이상이 파이썬을 이용한 개발 경험이 없었고, 서로 배운것을 공유를 하면서 스터디를 하면 더 좋겠다는 생각이 들때 즈음. 언제 어디선가 봤던것이 머릿속을 스쳐 지나간다. 그건 바로 Jupyter(이하 주피터). 출처 : https://jupyter.org/출처 : https://jupyter.org/ 주피터는 수십 개의 프로그래밍 언어에서 대화 형 컴퓨팅을위한 오픈 소스 소프트웨어, 오픈 표준 및 서비스를 개발하기 위한 툴이라고 한다. 이 포스트를 작성하기 전까지만 해도 “주피터 == 파이썬 웹 개발툴” 이라고만 알고있었는데 좀더 찾아보니 다양한 언어를 지원하는것 같다.그럼 이러한 주피터를 특정 서버에 설치하고 로컬에 파이썬을 설치하지 않아도 원격으로 파이썬 코딩을 해보면 좀더 스터디에 도움이 되지 않을까 하는 마음이 들었다. 또한 학교에서 운동장에 잔디를 깔아서 맘껏 뛰놀수 있게 하는 느낌으로 팀원들을 위해 설치를 해두고 원격으로 접속할 수 있게 해두면 모두가 편하고 쉽게 파이썬에 대해 경험을 해볼 수 있지 않을까 하는 마음으로 주피터를 설치를 해 보고자 한다.본 포스팅의 목표는 다음과 같다. 환경 : CentOS 7.4 64Bit, python 2.7 (기본) 목표 anaconda 를 활용하여 시스템 기본 파이썬을 건드리지 않는 가상환경을 구축한다. 주피터를 설치하고 원격으로 접속할 수 있도록 설정한다. 여기까지 보면 필자가 엄청나게 파이썬에 대해 잘 아는것처럼 보일수도 있어 미리 말하지만 필자는 찐 자바 개발자이면서 파이썬 개발 수준은 기본적인 스크립트를 작성하는 정도이다. 그러니 이 포스트를 읽고 있는 필자같은 파알못(?) 분들도 충분히 설치가 가능하다. (최대한 따라할수 있을 정도의 치트키 수준으로 작성 하고자 한다.) # 아나콘다 설치 (덤으로 설치되는 주피터)우선 아나콘다를 설치하자. 아나콘다는 Anaconda(이전: Continuum Analytics)라는 곳에서 만든 파이썬 배포판으로, 수백 개의 파이썬 패키지를 포함하고 있다고 한다. 즉, 아나콘다를 설치하고 만들어진 가상환경에서 파이썬 개발을 하면 다양한 모듈이 이미 설치되어 있기 때문에 편리하다는 이야기. 출처 : https://www.anaconda.com/출처 : https://www.anaconda.com/ 더불어 시스템에 기본으로 설치되어 있는 파이썬을 건드리면 여러 복잡한 문제가 발생할 수 있기에. 아나콘다를 활용하여 파이썬 3을 사용하는 가상환경을 만들어 보자.설치는 아주 간단하다. 아나콘다 설치파일을 다운받고 이를 실행하면 끝.(user 레벨이 root 면 sudo 명령어를 생략해도 된다.)123456789101112131415161718192021222324252627282930313233$ wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh$ sudo bash Anaconda3-2019.10-Linux-x86_64.shWelcome to Anaconda3 2019.10In order to continue the installation process, please review the licenseagreement.Please, press ENTER to continue&gt;&gt;&gt;===================================Anaconda End User License Agreement===================================Copyright 2015, Anaconda, Inc.~~~ 중략 ~~~Do you accept the license terms? [yes|no][no] &gt;&gt;&gt; yes # yes!!Anaconda3 will now be installed into this location:/root/anaconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below[/root/anaconda3] &gt;&gt;&gt; /home/anaconda3 # 설치될 경로를 설정해주고 기본 설정값에 설치하려면 그냥 엔터~~~뭐가 엄청 설치된다. 물 한잔 먹고 오자.~~~installation finished.Do you wish the installer to initialize Anaconda3by running conda init? [yes|no][no] &gt;&gt;&gt; yes # yes!! 이렇게 되면 설치는 끝. 환경변수를 설정해서 기본 파이썬 환경을 아나콘다에 의해 설정되도록 맞춰주자.123456789101112131415sudo vi .bashrc __conda_setup=&quot;$(&apos;/home/anaconda3/bin/conda&apos; &apos;shell.bash&apos; &apos;hook&apos; 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/home/anaconda3/etc/profile.d/conda.sh&quot; ]; then . &quot;/home/anaconda3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/home/anaconda3/bin:$PATH&quot; fi fi unset __conda_setupsource .bashrc 그러면 다음과 같이 프롬프트가 변경된것을 확인할 수 있다.12345환경변수 변경 전[user@server ~]$ 환경변수 변경 후(base) [user@server ~]$ 참, 아나콘를 설치하면 주피터가 같이 설치가 된다. 좀전에 설치된 내용을 보면 주피터가 설치된것을 확인할 수 있다.123456789101112131415...jinja2 pkgs/main/noarch::jinja2-2.10.3-py_0joblib pkgs/main/linux-64::joblib-0.13.2-py37_0jpeg pkgs/main/linux-64::jpeg-9b-h024ee3a_2json5 pkgs/main/noarch::json5-0.8.5-py_0jsonschema pkgs/main/linux-64::jsonschema-3.0.2-py37_0jupyter pkgs/main/linux-64::jupyter-1.0.0-py37_7jupyter_client pkgs/main/linux-64::jupyter_client-5.3.3-py37_1jupyter_console pkgs/main/linux-64::jupyter_console-6.0.0-py37_0jupyter_core pkgs/main/noarch::jupyter_core-4.5.0-py_0jupyterlab pkgs/main/noarch::jupyterlab-1.1.4-pyhf63ae98_0jupyterlab_server pkgs/main/noarch::jupyterlab_server-1.0.6-py_0keyring pkgs/main/linux-64::keyring-18.0.0-py37_0kiwisolver pkgs/main/linux-64::kiwisolver-1.1.0-py37he6710b0_0... # 주피터 원격 설정다음으로 주피터 환경설정을 만들어 주자. 기본 설정이 아닌 별도 설정을 만드는 이유는 원격으로 띄울때 입맛에 맞도록 띄우기 위함이다. 설정파일을 생성하고 수정을 해주자.123456$ sudo jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py$ sudo vi /root/.jupyter/jupyter_notebook_config.py c.NotebookApp.notebook_dir = &apos;/home/data&apos; # 주피터에서 만든 결과물들이 저장되는 경로 c.NotebookApp.ip = &apos;0.0.0.0&apos; # 외부에서 접속하기 위한 설정 c.NotebookApp.port = 80 # 서버 ip (혹은 설정한 도메인) 으로 바로 접속하기 위해 이렇게 하면 설정 끝! 이제 아래 명령어로 드디어 주피터를 실행시켜 보자.12345678910111213141516$ sudo jupyter notebook --config=/root/.jupyter/jupyter_notebook_config.py --allow-root --no-browser[I 21:30:18.278 NotebookApp] JupyterLab extension loaded from /home/anaconda3/lib/python3.7/site-packages/jupyterlab[I 21:30:18.278 NotebookApp] JupyterLab application directory is /home/anaconda3/share/jupyter/lab[I 21:30:18.279 NotebookApp] Serving notebooks from local directory: /home/data[I 21:30:18.280 NotebookApp] The Jupyter Notebook is running at:[I 21:30:18.280 NotebookApp] http://server:80/?token=82db9~~~~~~be6b9d5[I 21:30:18.280 NotebookApp] or http://127.0.0.1:80/?token=82db9~~~~~~be6b9d5[I 21:30:18.280 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 21:30:18.283 NotebookApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-9366-open.html Or copy and paste one of these URLs: http://server:80/?token=82db9~~~~~~be6b9d5 or http://127.0.0.1:80/?token=82db9~~~~~~be6b9d5 서버 ip로 접속을 해보면 로그인 페이지가 나오고 토큰을 입력하라고 한다. 그럼 당황하지 말고 서버에 나온 토큰을 입력하고 로그인을 하면 아래처럼 브라우저(?)같은 화면을 맞이할 수 있다. 기나긴 여정끝에 맞이하는 로그인 페이지!기나긴 여정끝에 맞이하는 로그인 페이지! 그런데 기다란 토큰을 입력하는 것보단 외우기 쉬운 패스워드를 입력하는게 더 편할테니, 하단 영역에서 패스워드를 설정해두면 다음번엔 (토큰보다는 외우기 쉬운) 패스워드를 입력하고 로그인이 가능하다. 여기서 80 port 로 띄우기 위해서는 root 권한이 있어야 한다.처음엔 80 port 가 아닌 다른 port 로 띄우고 앞단에 아파치를 둬서 프록시 태우려 했으나 프록시가 되는 과정에서 뭔가 정보전달이 잘 안되는 느낌이었다. 그렇기에 앞서 말한대로 root 권한으로 띄워야 한다. 모든 foreground 프로세스들은 그 프로세스를 띄운 세션이 종료되면 해당 프로세스 또한 종료가 되어버린다. 리눅스의 “&amp;” 명령어를 이용하여 backgrund로 띄워주자.1sudo jupyter notebook --config=/root/.jupyter/jupyter_notebook_config.py --allow-root --no-browser &amp; # 주피터 활용팁IDE처럼 엄청난 기능을 제공하진 않지만 외부에서 누구나 간단히 접속하여 파이썬 코딩을 할 수 있는 상태가 되었다. 폴더 기능도 제공하니 각자의 환경(폴더)을 만들어서 코드를 작성할 수 있다.여기서는 “notebook” 이라는 개념으로 불리우는데 파이썬 파일을 만들고 언제나 그랬듯 hello world 를 출력한뒤 Shift+Enter 을 누르면 바로 결과물이 나오는 것을 확인할 수 있다. 위 메뉴에서 다양한 포맷으로 다운을 받을 수 있다.위 메뉴에서 다양한 포맷으로 다운을 받을 수 있다. 자동완성은 “Tab”, 어느정도 시간이 지나면 자동으로 저장이 되고, File → Download as → Notebook(.ipynb) 로 추출을 한뒤 gist 에도 업로드가 가능하다. 아주 이쁘게. (gist 에 올려진 주피터 노트북 결과물 : 링크) 더 자세한 내용은 주피터 도큐먼트를 참고하자. 링크 # 마치며이로써 팀원들이 파이썬을 가지고 놀아볼 수 있는(?) 운동장이 만들어 졌다. 우리는 개발자니까. 환경이 없으면 직접 만들면 되니까. 개발자로 살아가면서 이런 부분이 참 매력적인 것 같다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"jupyter","slug":"jupyter","permalink":"https://taetaetae.github.io/tags/jupyter/"}]},{"title":"스프링 부트로 멀티모듈 셋팅하기","slug":"spring-boot-maven-multi-module","date":"2020-01-19T01:29:03.000Z","updated":"2020-04-23T04:41:36.869Z","comments":true,"path":"2020/01/19/spring-boot-maven-multi-module/","link":"","permalink":"https://taetaetae.github.io/2020/01/19/spring-boot-maven-multi-module/","excerpt":"서비스를 처음 만들기 시작할때면 각 직군별로 생각하는 포인트가 다양하다. 설계, 기획, 디자인, 개발. 여기서 개발은 프로젝트 셋팅을 어떻게 해야하지? 하는 고민을 하기 마련이다. 아주 간단하게 하나의 모듈로 모든 기능을 담당하도록 만들 수 있지만 기능별로 모듈을 나눠서 셋팅하는게 관리측면에서 장점이라 생각한다.","text":"서비스를 처음 만들기 시작할때면 각 직군별로 생각하는 포인트가 다양하다. 설계, 기획, 디자인, 개발. 여기서 개발은 프로젝트 셋팅을 어떻게 해야하지? 하는 고민을 하기 마련이다. 아주 간단하게 하나의 모듈로 모든 기능을 담당하도록 만들 수 있지만 기능별로 모듈을 나눠서 셋팅하는게 관리측면에서 장점이라 생각한다.예를 들어보자. 도서관의 들어온 책 정보를 외부에 제공하는 “API”, 주기적으로 책 정보를 업데이트 하는 “Batch”. 이렇게 크게 두가지의 모듈이 있어야 한다고 가정했을때 어떤식으로 모듈을 설계할 수 있을까?이번 포스팅에서는 스프링 부트와 메이븐을 활용해서 하나의 프로젝트(컴포넌트)에서 여러 모듈을 관리할 수 있는 Spring Multi Module을 셋팅하는 방법에 대해 알아보고자 한다. 필자도 셋팅하기 전에는 “그냥 하면 되는거 아니야?”라며 우습게 보다 아주 사소한 부분들에서 엄청난 삽질을 해서 그런지 꼭 포스팅으로 남겨놔야 겠다고 다짐했고 이렇게 정리를 할 수 있게 되어서 다행이라 생각한다. 어쩌면 우리가 있는 팀도 멀티모듈이 아닐까? 출처 : https://bcho.tistory.com/813어쩌면 우리가 있는 팀도 멀티모듈이 아닐까? 출처 : https://bcho.tistory.com/813 # 왜 멀티모듈로 셋팅할까?위에서 예시로 이야기 한것처럼 현재 우리가 셋팅해야할 모듈은 크게 두가지 이다. API : 외부에 도서관에 들어온 책 정보를 알려주는 모듈 Batch : 주기적으로 도서관의 책 정보를 갱신하는 모듈 한번 생각을 해보자. 위에서 말한 모듈들 중에 동시에 사용할것만 같은 정보가 있다. “책 정보”. 각 모듈마다 “책 정보”를 가져오는 로직을 작성하는것 보다 한곳에서 해당로직을 구현하고 이를 여러곳에서 사용하는게 사용하는게 중복코드를 방지할수 있는 방법이란건 쉽게 알아차릴수 있다. 그렇다면 어떻게 모듈을 분리할수 있을까?필자의 경험으로 미루어 볼때 크게 두가지 방법이 있는것 같다. 공통으로 사용하는 모듈을 jar로 만들고 이를 메이븐 원격 저장소에 deploy, 사용하는 모듈에서 디펜던시에 추가하여 사용 멀티모듈로 구성하고 사용하는 모듈에서 디펜던시에 추가하여 사용 첫번째 방법의 가장 큰 단점은, 공통으로 사용하는 모듈이 변경될때마다 버전을 바꿔주고 (안바꿔도 되지만 사용하는 모듈에서 캐시 갱신을 해야하는 불편함이 생긴다.) 메이븐 원격 저장소에 deploy를 해줘야 한다. 그에 반해 두번째 방법은 이런과정없이 함께 빌드만 해주면 끝나고 IDE에서 개발시 한 모듈에서 동시에 수정과 사용이 가능하기 때문에 훨씬 편리하다.은총알은 없다 라는 말처럼, 정답은 없다. 하지만 이런저런 방법들을 미리 알아두면 적시적소에 사용할 수 있는. 필자가 다른글들에서도 언급을 자주하던 “나만의 무기”가 되지 않을까? # 멀티모듈 셋팅하기위에서 이야기 했던 “API”, “Batch”와는 별도로 공통으로 사용하는 모듈인 “Core” 이렇게 총 3개의 모듈을 만들예정이다. 다른 이야기지만, 공통으로 사용할 것 “같아서” 미리 공통로직을 작성하는 습관은 좋지 않는것 같다. 그러다보면 쓸데없이 공통로직이 무거워지므로 실제로 사용하면서 중복코드가 발생할때 그때 공통로직으로 리펙토링 해도 늦지 않는것 같다. (꼰데인가…) 구현하는 환경은 다음과 같다. Spring Boot 2.2.3 Maven IntelliJ 우선 IDE의 힘을 빌려 하나의 스프링 부트 프로젝트를 생성해본다.다음 > 다음 > 다음다음 > 다음 > 다음 그 다음 만든 프로젝트에서 우클릭 후 새로운 모듈을 선택. Maven 모듈을 선택하고 적당한 이름을 적어준다.다음 > 다음 > 다음 222다음 > 다음 > 다음 222 “API”, “Batch”, “Core” 라는 모듈을 추가하고 실제 모듈이 되는 “API”, “Batch”에 Build plugin 을 셋팅해주자. 그렇게 하고 각 Pom.xml을 보면 아래와 같다. (“API” 모듈에 대해서만 집중적으로 이야기 하려 한다. “Batch” 모듈도 동일한 형식으로 작성하기 때문.) 최 상위 Pom.xml (library) modules 하위에 멀티모듈로 설정한 모듈들의 이름이 들어가 있는것을 확인할 수 있다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;api&lt;/module&gt; &lt;module&gt;core&lt;/module&gt; &lt;module&gt;batch&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.3.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.taetaetae&lt;/groupId&gt; &lt;artifactId&gt;library&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;library&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; API Pom.xml parent 부분이 설정되어 있는 모습을 볼수 있고, core 모듈을 사용하기 위해 dependency 에 추가를 해준다. 1234567891011121314151617181920212223242526272829303132&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;library&lt;/artifactId&gt; &lt;groupId&gt;com.taetaetae&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.taetaetae&lt;/groupId&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;library-api&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Core Pom.xml Core 모듈은 “jar”로 패키징 되어 다른 곳에서 사용되어야 하기 때문에 packaging 만 설정해준다. 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;library&lt;/artifactId&gt; &lt;groupId&gt;com.taetaetae&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 이렇게 하고서 Core 모듈에 공통으로 사용될 로직을 작성하고, API 모듈에서 이를 사용하는 로직을 작성한뒤, 빌드를 해보면 에러 없이 정상 작동을 하는 모습을 볼 수 있다. 메이븐 빌드 Goal : mvn clean install -pl api -am -pl [ ] : 지정된 이름의 모듈만 빌드한다.-am : 연결된 상위 모듈까지 같이 빌드한다.Reference 참고 1234567891011121314151617181920212223[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------------[INFO] Reactor Build Order:[INFO] [INFO] library [pom][INFO] core [jar][INFO] api [jar][INFO] [INFO] -----------------------&lt; com.taetaetae:library &gt;------------------------...중략...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for library 0.0.1-SNAPSHOT:[INFO] [INFO] library ............................................ SUCCESS [ 0.715 s][INFO] core ............................................... SUCCESS [ 3.005 s][INFO] api ................................................ SUCCESS [ 1.715 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 6.008 s[INFO] Finished at: 2020-01-19T11:26:46+09:00[INFO] ------------------------------------------------------------------------ 본문 최 하단에 해당 소스가 업로드 된 Github에서 확인이 가능하겠지만, (“도서관” 이라는 목적에는 안맞지만…) 정수를 더하고 빼는 유틸을 Core에 만들고 이를 Api에 있는 컨트롤러에서 사용하도록 만들어 보았다. (어디까지나 모듈에서 멀티모듈로 되어있는 다른 모듈에 접근이 가능한지를 보기 위함이라… 예시가 우아하진 않다.) # 마치며언제부터인가 단순 로직개발보다 구조관점에서 바라보는 연습을 하곤한다. 아무리 알고리즘이 잘 작성되고 우아한 코드일지라도 구조가 개발 생산성 측면과 유지보수 측면에서 분리하면 아무 소용 없는것 같다. 위에서도 이야기 했듯이 멀티모듈이 무조건적인 정답은 아니지만 시스템을 구성하는데 있어 다양한 선택지를 알고있는 여유를 갖는것도 좋아보인다. ※ Github 예제 소스","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"module","slug":"module","permalink":"https://taetaetae.github.io/tags/module/"},{"name":"structure","slug":"structure","permalink":"https://taetaetae.github.io/tags/structure/"},{"name":"maven","slug":"maven","permalink":"https://taetaetae.github.io/tags/maven/"}]},{"title":"조금은 무거운 2019 회고","slug":"review-2019","date":"2019-12-29T13:22:03.000Z","updated":"2020-04-23T04:41:36.839Z","comments":true,"path":"2019/12/29/review-2019/","link":"","permalink":"https://taetaetae.github.io/2019/12/29/review-2019/","excerpt":"“회고”는 비단 개발 블로그 뿐만 아니라 어떠한 과정의 마지막에는 꼭 해야할 중요한 시간인 것 같다. 앞만보고 달려가자! 닥공! 라는 말이 있지만 사실 이 말이 성립되기 위해선 지난 과거에 대한 정리와 반성 그리고 무엇을 하려고 했는데 어떤 이유로 못했는지와 그 동안의 나 자신을 바라볼 수 있는 이 “회고” 시간이 필요하다.","text":"“회고”는 비단 개발 블로그 뿐만 아니라 어떠한 과정의 마지막에는 꼭 해야할 중요한 시간인 것 같다. 앞만보고 달려가자! 닥공! 라는 말이 있지만 사실 이 말이 성립되기 위해선 지난 과거에 대한 정리와 반성 그리고 무엇을 하려고 했는데 어떤 이유로 못했는지와 그 동안의 나 자신을 바라볼 수 있는 이 “회고” 시간이 필요하다. 벌써 2019년도 마무리가 되어간다. 작년보다 더 정신없이 달려온 올해. 내년엔 올해보다 더 멋지고 힘차게 출발하기 위해 필자의 한 해를 돌아보고자 한다.그렇다면 회고는 어떻게 하는게 가장 좋을까? 무작정 타임라인 기반으로 1월엔 뭐했고 2월엔 뭐했고… 이 방법이 틀린건 아니지만 타임라인 기반으로 정리를 한 뒤 키워드별로 다시 정리하는 방식이 가장 맞을것 같다는 생각이다. 무엇을 했고, 뭐가 좋았고 어떤건 아쉬웠고. 그래서 내년엔 어떻게 할 것이고. 각자의 회고 방식에는 차이가 있겠지만 회고를 하는 이유, 그리고 회고라는 목표 중에 공통점은 “뒤를 돌아보고, 앞을 보기위한 힘을 찾는것” 이 아닐까 싶다. 내년 회고를 할때는 흑백이 아닌 컬러 사진을 넣을 수 있는 분위기가 될까?... 출처 : http://www.nanum.com/site/poet_walk/820914내년 회고를 할때는 흑백이 아닌 컬러 사진을 넣을 수 있는 분위기가 될까?... 출처 : http://www.nanum.com/site/poet_walk/820914 # 회사는 성장의 공간이 아닌것을 깨닳는 순간.(이야기에 앞서 필자는 현재 서비스 개발자임을 밝힌다.)내년이 되면 컴퓨터쟁이가 된지 벌써 8년차. 매년 성장의 그래프를 그려보면 작년까지만 해도 우상향이었다. (그래프의 기울기는 매년 달랐지만) 허나 올해는 기울기가 0 이거나 오히려 마이너스가 된 것 같은 느낌이다. 왜일까. 키는 왜 더이상 성장을 안할까? (쓰읍...) 출처 : http://www.guro1318.or.kr/bbs/board.php?bo_table=data&wr_id=1723키는 왜 더이상 성장을 안할까? (쓰읍...) 출처 : http://www.guro1318.or.kr/bbs/board.php?bo_table=data&wr_id=1723 회사를 다니다 보면 아주 일반적으로 “시키는 일”을 하곤 한다. 주어진 업무를 정해진 기간 안에 스펙에 맞춰 개발하는. 아주 극단적으로 나쁘게 말하면 “도구”로 전락되어버릴 수도 있는 시간들. (개발자가 도구가 된다는 말은 너무나도 듣기 싫은 말중에 하나.) 흔히 말하는 CRUD(Create, Read, Update, Delete) 성의 개발 업무를 하곤 한다. 하지만 꼭 성과에 align(더 좋은 한국말을 찾고 싶은데…) 하는 일 말고도 허드렛일(일종의 서스테이닝?)을 할 경우도 있는데 그게 만약 재미없는 일이라면 어떨까?필자는 그렇게 “시키는 일만 하며 재미없는 회사생활” 보다 “재미있게 개발하며 성장을 할 수 있는 회사생활” 이라는 기준을 가지고 한 해를 지내온 것 같다. 즉, “시키는 일”이 아닌 “시키지도 않은 일”을 찾아서 해가며. 예컨대, 처음에 잡았던 서비스 구조가 사용자가 많아지고 요구사항이 많아짐에 따라 복잡하고 성능을 저해하는 상황을 발견하고 미리 구조개선을 통해 성능과 효율이라는 두마리의 토끼를 잡는다거나. 지난 외부 세미나에서 듣고 인사이트를 얻어 팀내에도 적용해본 배치 무중단 배포 기능. 팀 내 코드리뷰의 활성화와 수동으로 해야할 업무들을 메신저 봇을 활용하여 자동화 한다거나. 서비스 지표 대시보드를 만들어 한눈에 서비스 상황을 볼 수 있게 별도의 개발 페이지를 만들어 보는 등. 다양한 업무 내/외 적으로 일을 찾아가며 + 필자의 개인 시간을 할애해 가면서 정말 재미있게 보내온 것 같다.하지만 뒤를 돌아보면 “성장 했는가?” 라는 질문이 있다면 “그렇게 하고있는것 같아서 신나게 해왔는데 돌아보니 막상 뭘했나 하는 느낌이 든다” 라고 말할 수 있을 정도로 여러가지를 많이 하며 다양한 “경험”을 얻긴 했지만 실질적인 “성장”은 아쉽지만 부족한 한 해 였던것 같다.회사가 원하는, 연차에 맞는 업무 역량과 개발 팀에서의 위치를 충족시키기엔 회사 안에서 성장하기엔 한계가 있다고 판단이 들었다. (이 생각이 왜 이제서야 들었을까.) 오픈소스나 새로운 언어를 회사 밖에서 혼자서 공부 하던지 여러명이서 스터디를 통해 습득을 해야하고 토이프로젝트 또한 회사와 별도로 진행하며 개발 스킬을 늘려야 할것 같다. 그 이유는 회사에서의 성장이 결국 나의 성과로 잡힐 수는 없는데 괜시리 기대를 하게 되기도 하고 특히 서비스를 운영하는 팀에서는 요즘 핫 하다는 개발 방법론이나 솔루션을 도입하기에는 다소 무리가 있기 때문이다. (물론 회사일도 하면서 성장을 할 수 있는 상황이라면 금상첨화. 이를 찾는건 정말 어려운 일 같다.)내년에는 좀더 회사 밖에서 새로운 지식도 쌓으려 노력하고, 외부 활동도 찾아가며 주니어도 시니어도 아닌 “매너리즘에 빠질 애매한 연차”를 슬기롭게 극복하려 노력해봐야 겠다. # 개발 커리어 쌓기. 꾸준함이 정답!작년보단 줄어들었지만 다양한 외부활동을 해왔다. 지금의 연차에 어울리진 않지만 한번도 제대로된 해커톤을 해보지 않아 GDG에서 주최했던 해커톤에 참여를 하며 마지막에 결과물에 대해 발표도 해보고, 필자의 토이프로젝트인 기술블로그 구독서비스 에 대한 일련의 개발 히스토리에 대해서 발표를 할 수 있었던 좋은 기회가 있었다. 또한 팀 분들께 글쓰기에 대한 인사이트를 전달하고자 지난 포스팅 에 대한 내용을 간추려 발표를 하기도 하였다. (TMI : Deview 에서도 발표를 하려 지원을 했지만 아쉽게도 사내 탈락을 하고 ㅠ…) 발표는 글쓰기를 넘어 사람들 앞에서 라이브로 이야기하는 엄청난 활동인 것 같다. 내년에도 기회가 된다면 이번엔 좀 준비를 잘해서 여유로운 발표를 해보고 싶다. (그렇게 하기 위해서는 먼저 나 자신을 디벨롭 해야겠지?) 앞이 안보이던 발표. 아 물론 내 눈앞. 출처 : https://lalwr.blogspot.com/2019/07/toy-story-side-project-by-gdg-campus.html앞이 안보이던 발표. 아 물론 내 눈앞. 출처 : https://lalwr.blogspot.com/2019/07/toy-story-side-project-by-gdg-campus.html 블로그 포스팅은 작년 수준으로 작성한 것 같다. 더 많이 쓰려고 했는데… 이점은 이 포스팅을 통해 반성을 해본다. “글또” 라는 모임에도 참여를 하며 적어도 2주에 글 하나는 써야지 했지만 올해 하반기에 개인적인 큰 이벤트도 있었고, 단순히 글 개수만을 채우기 위한 포스팅은 하기 싫었기 때문이다. (이거봐, 또 변명일색. 정신차려 태태태!) 하루에 한시간, 아니 30분만 투자하면 조금이라도 작성할 수 있는데 왜 이렇게 힘들어 했는지. 조금 더 신경써서 고퀄리티 기술블로그 포스팅을 해보려고 노력해야겠다. (왜 개발자가 바쁜데 글까지 써야하는 이유는 지난 포스팅을 참고)운이 좋아 서평도 쓰게 되었다. 어떻게 필자를 알고 연락을 주셨는지 출판사에서 페이스북 메신저로 연락이 와서 쓰게 되었다. 전문적인 기술서적은 아니었지만 프로그래머로써의 꼭 한번즈음은 읽어볼만한 책에 대한 서평이었다. 처음으로 서평을 써보게 되어서 상당히 재밌었고, 서점이나 인터넷 책 구매 사이트에 필자 이름이 있다는 것에 감동의 연속이었다. 지금도 가끔씩 책 쓸 생각이 있냐는 연락이 종종 오지만, 내가 그럴 능력이 될까 싶다가도. 한번즈음 도전해보고 싶은. 글쓰기는 필자 삶을 바꿔놨다고 해도 과언이 아닐 정도로 정말 좋은 영역인것 같다.필자의 토이프로젝트인 기술블로그 구독서비스 에 구독하는 사람이 어느덧 2,300여명을 넘어섰다. 어떻게 알고 다들 구독하시는지. 덕분에 메일 발송속도는 처음과는 현저하게 느려졌고 (사용자가 많아짐에 따라 구조개선을 해야하는건 당연한 이야기), 이제는 무언가 다른 기능을 추가해야하지 않을까 싶은 생각이 든다. 현재는 장님 코끼리 만지듯 python + flask 로 개발되었는데 내년엔 java 기반으로 바꾸면서 성능개선 + 기타 다른 기능을 만들어 볼까 한다. 더불어 한달에 약 3만원가량 AWS 서버비용이 나가고 있는데 후원을 받는것도 한계가 있고. 비지니스 모델을 찾거나, 사용자가 만명을 넘어서도 비용없이 돌아가는 구조를 생각해 봐야겠다. (1년이면 약 3~40만원, 무시 못할 비용이다…후..후원좀…) 구독자 수 그래프, 뭔가 방법을 찾아야 한다.구독자 수 그래프, 뭔가 방법을 찾아야 한다. # 핑계와 타협이 많았던 올해. 내년엔 어떤 도전을 할까?유독 올해는 작년, 제작년보다 필자 자신과 타협을 많이 했던 것 같다. 너무 바빠서라는 부끄러운 핑계부터 시작하여, 밥먹듯 야근하며 일 열심히 했으니까 라는 말도 안되는 타협까지. 우선 건강부터 챙겨야 겠다. 컴퓨터쟁이의 고질병인 거북목과 라운드숄더. 몸짱까진 아니더라도 늙어서도 코딩을 하려면 지금부터 몸관리를 해야하지 않을까 싶다.앞서 이야기 한 것처럼 회사 밖에서의 나를 찾아보고자 한다. 그에 토이프로젝트 2.0 도 출시해보고, 새로운 언어, 새로운 오픈소스도 공부해보며 기술블로그도 열심히 포스팅해야지.작년에는 “Coder 가 아닌 Programmer 가 되고 싶다.” 며 그럴싸한 계획이 있었는데 돌이켜 보면 그렇게 지낸것 같다. 단순히 도구가 되는 개발자가 아니라 단순 반복적인 일이나 허드렛일을 하면서도 그속에서 성장 포인트를 찾으려 애를 쓰는. 이 부분은 내년에도 유지하는 것으로.목표를 뚜렷하게 잡는 일도 중요하지만, 적어도 내년엔 올해보다는 더 성장한 내가 되었으면 하고, 뒤 돌아봤을 때 부끄러움이 없는 내가 되었으면 좋겠다는 말로 올해 회고를 마무리 하고자 한다.고생했다 태태태. 내년에도 잘 달려주길.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"}]},{"title":"개발하기 바쁜데 글까지 쓰라고? (글쓰는 개발자가 되자.)","slug":"a-reason-for-writing","date":"2019-10-27T04:51:16.000Z","updated":"2020-04-23T04:41:36.682Z","comments":true,"path":"2019/10/27/a-reason-for-writing/","link":"","permalink":"https://taetaetae.github.io/2019/10/27/a-reason-for-writing/","excerpt":"신입시절. 배워야 할 것도 회사 업무도 많아 허우적대던 때가 있었다. 그렇게 하루에 3~4시간 자며 정신없이 하루를 보내던 날 문득 동기 형이 “개발자는 기술 블로그를 해야 돼!”라는 전혀 이해가 안 되는 말을 해온다. 이렇게 바빠 죽겠는데 블로그에 글까지 쓰라고?","text":"신입시절. 배워야 할 것도 회사 업무도 많아 허우적대던 때가 있었다. 그렇게 하루에 3~4시간 자며 정신없이 하루를 보내던 날 문득 동기 형이 “개발자는 기술 블로그를 해야 돼!”라는 전혀 이해가 안 되는 말을 해온다. 이렇게 바빠 죽겠는데 블로그에 글까지 쓰라고? 말이 되는 소릴 하라며 반박하다 못내 이기는 척 하나 둘 글을 쓰기 시작했고, 다른 유명 블로거처럼 엄청나진 않지만 하루에 1,000~2,000명 정도 들어오며 점점 성장해 가는 나만의 기술 블로그가 되었다. 미약하지만 처음보다는 성장하고 있는 블로그 PV(Page View)미약하지만 처음보다는 성장하고 있는 블로그 PV(Page View) 또한 필자의 개발자 경력(?)을 돌이켜 보자면 기술 블로그를 하기 전과 하고 난 후로 나뉠 만큼 기술 블로그는 개인적으로 엄청난 영향력이 되었다. 이 기회를 빌어 동기 형에게 감사의 인사를 전하고 싶다. 형. 보고 있죠? ;] 이번 포스팅은 꼭 “블로그를 하자” 라기 보다 “글을 왜 써야 하고 어떻게 써야 하는지”에 대해 이야기해보고자 한다. 처음 이 글을 쓰려고 마음먹었을 땐 개발자라는 직군에 국한되지 않고 누구에게나 적용될 정도의 범용적인 글을 쓰려 했으나 “S”의 조언으로 독자(타깃)을 최대한 개발자에 맞춰 써보고자 한다. thanks to “S”사실 조금만 검색을 해보면 특히 개발자에게 글쓰기가 얼마나 중요한지 찾아볼 수 있을 정도로 다양한 글들에서 “개발자가 왜 글을 써야 하는가”에 대한 내용이 언급이 되곤 했었다. 글을 쓰지 않던 개발자. 하지만 지금은 글쓰기가 정말 중요하다고 느끼며 적어도 2주에 하나 이상의 글을 쓰려는 현업 개발자의 시선에서 정리를 해보고자 한다.그리고 마침 멘토링 해주고 있는 분께도 글 쓰는것에 대한 중요성을 알려주고 싶었고, 팀 내에도 공유를 하고 싶어 겸사겸사. # 왜 글을 써야 할까?✓ 비로소 내 것이 되기 위한 과정프로그래밍 언어를 처음 배울때 꼭 만나는 문구 Hello World를 출력하시오. 이게 의미하는 의미가 무엇일까? 정말 새로운 세계를 알려주려 하는 것 일까?(그럴수도 있다…) 우리가 살아가며 “배움”이라는 과정은 대부분 비슷하겠지만 특히 IT 기술은 책을 다 읽었다든지, 동영상 강의를 다 들었다고 해서 내 것이 되었다고 말하기는 어려울 것 같다. 직접 키보드를 두드려 가며 거기서 얻을 수 있는 또 다른 “인사이트” 가 생길 수도 있기 때문이다.다른 예로, 운영하던 시스템이나 서비스에서 장애를 맞았다고 가정해보자. 하지만 우리는 늘 그래왔듯 어떻게든 장애를 해결할 것이다. 이러한 상황에서 분명 “문제의 원인”이 있었을 테고 “해결 과정”이 있기 마련인데 이곳에서도 “인사이트”가 분명 있을 것이다.이러한 “인사이트”를 글로 적다 보면 그냥 “아~ 그렇구나, 그랬었지” 하는 머릿속에서의 기억보다는 훨씬 더 오래 남을 것이고 혹여 글에서 정리를 잘못해 다른 사람들의 피드백이 있다면 더할 나위 없이 좋은 효과라고 생각이 된다. (이것이 바로 공유의 힘!)더불어 글을 쓸 때 올바른 정보에 기반하여 쓰는 습관이 중요한데 그러다 보면 원래 쓰려고 했던 내용보다 더 깊게 알아가는 과정 속에서 또 다른 배움을 얻을 수 있는 반강제적 기회가 생길 수 있다. 누가 시키지 않았어도 배운 것에 대한 활용을 하고 싶은 생각이 들고 이를 또 글로 쓰고. 긍정적인 순환 속에 생겨나는 작은 발자국일지라도 성장해가는 자신을 느낄 수 있을 것이다. ✓ 몸이 기억하는 정리하는 습관개발을 하다 보면 정말 간단한 “CRUD”(Create, Read, Update, Delete) 부터 시작해서 엄청나게 복잡한 도메인 지식에 기반하여 개발을 해야 하는 상황이 생긴다. 그럴 때면 머릿속으로 정리하는 것보다 그림이나 글을 써가면서 정리하는 게 좋다는 건 굳이 말하지 않아도 아는 사실. 글을 쓰다 보면 기승전결의 정리 방법과 목적이 무엇이고 근거가 무엇인지에 대해 구분하는 스킬이 늘어나는 것 같다.(적어도 필자는 기술 블로그를 운영하면서 정리하는 스킬이 그전보다 엄청나게 늘어났다고 자부한다.)중국 속담중에 하나, 머릿속에 박혀 나오질 않는다.중국 속담중에 하나, 머릿속에 박혀 나오질 않는다.구조가 보기 어렵게 꼬여버린 스파게티 코드나 기능(스펙)이 너무 복잡한 도메인 지식도 글을 쓰며 갈고닦은 “정리 스킬”이 있다면 보다 깔끔한 코드로, 복잡하지만 간결한 스펙으로 정리하는 데 도움이 될 수 있다. 이러한 스킬은 비단 개발할 때나 스펙 정리할 때 뿐만 아니라 상대방과의 이야기를 할 때나 어떠한 계획을 세울 때. 고민이 생겼을 때 등 정말 다양한 곳에서 사용할 수 있는 정말 “나만의 무기”가 될 수 있다. ✓ 나를 브랜딩하는 수단 (a.k.a 기술블로그)특히 이 글을 읽고 있는 독자가 학생이시라면 “글쓰기”, 나아가서는 “기술 블로그”를 강력 추천하고 싶다. (그렇다고 학생이 아니라면 늦었다는 소리는 아니다. 지금 당장 시작하자.)늦었으니 지금당장 시작하라는 소리일꺼다. 그쵸 명수형? 출처 : http://blog.besunny.com/?p=6111늦었으니 지금당장 시작하라는 소리일꺼다. 그쵸 명수형? 출처 : http://blog.besunny.com/?p=6111자신이 어떤 생각을 가지고 어떤 기술에 관심을 가지며 어떤 문제 해결을 해왔는지에 대해 나만의 개발 히스토리로 한눈에 볼 수 있는 수단이 된다고 생각하기 때문이다. 참, 요즘 채용시에 Github 계정이나 기술 블로그를 제출해야 하는 곳이 많이 생길 정도로 기술 블로그에 대한 관심이 부쩍 늘어난 것 같다. (적어도 필자가 취업할 때보다는… 아. 옛날이여)필자는 기술 블로그를 운영하면서 집필, 추천평 등 전혀 예상하지 못한 경험을 할 수 있었다. (그중에 한 것도 있고 거절한 것도 있지만…) 그에 발표를 할 수 있었던 좋은 기회도 생겼고, 개인 메일로 이직 제안이나 기술 문의 등 “회사”라는 명찰을 떼고 외부에서 오롯이 나 혼자 일어설 수 있는 힘이 조금씩 생겨나고 있는것 같다. (그렇다고 이직 의사가 있다는 건 전~혀 아니니 오해는 말자. 회사님 사랑해요.)취업이나 이직을 할 때. 나에 대해 누군가에게 알리는 순간이 있을 때 구구절절 이런저런 기술들을 할 줄 알고 이런저런 경험을 해봤어요라고 말하는 것도 방법이 될수 있지만 우아하게 기술 블로그 링크하나 딱! 전달해 보는건 어떨까? 뭔가 더 있어 보이지 않을까? # 글을 쓸때 중요한 핵심 6가지✓ 그래서 너가 말하고 싶은게 뭔데?글을 쓰다 보면 이야기하고 싶은 게 많아서(잘 쓰고 싶어서) 결론보다는 그 결론을 말하기 위한 보충 설명이나 근거를 먼저 말하곤 한다. 하지만 글을 읽는 독자 입장에서는 정답(=결론)이 가장 궁금한데 그것이 글의 말미에 있다면 자칫 글의 퀄리티가 아무리 좋더라도 지루한(?) 과정을 거치는 수고가 필요할 수밖에 없다. 가급적 글의 무게중심은 서두에 두는 게 “글”이라는 목적에 부합하는 것 같다. 결론을 앞에서 이야기하고 근거를 이야기한 후 마지막에 한 번 더 결론을 이야기하는 것도 하나의 방법이 될 수 있겠다.그래도 뚜렷한 결론이 있다면 다행이다. 결론마저 없는 글은 독자로 하여금 왜 글을 썼는지 모를 느낌을 안겨줄 수 있다.(최악의 경우 읽다가 중단하게 된다…ㅜㅜ) 글쓰기에 있어 결론도 중요하지만 이 글을 쓰는 목적이 명확해야 설령 목표가 글 뒤에 배치되었다고 해도 끝까지 읽을 수 있는 힘이 생기지 않을까?반전에 반전을 거듭하며 결론을 도무지 알수없는 레파토리는 김치싸대기를 던지던 아침 드라마가 어울린다.출처 : https://m.post.naver.com/viewer/postView.nhn?volumeNo=14289808&memberNo=12508720반전에 반전을 거듭하며 결론을 도무지 알수없는 레파토리는 김치싸대기를 던지던 아침 드라마가 어울린다.출처 : https://m.post.naver.com/viewer/postView.nhn?volumeNo=14289808&memberNo=12508720 ✓ 누가 읽게 되는 글인가?글을 쓰는 사람(필자)이 있으면 글을 읽는 사람(독자)이 있기 마련. 대부분의 글들은 필자가 독자를 “설득”하기 위한 내용이 주를 이룬다. 독자가 한정적이라면. 예컨대, 주간 보고를 쓴다고 가정했을 때 독자는 오롯이 팀장님이 된다. 이런 경우 주저리주저리 쓰거나 다시 한번 묻게 되는 문장들보다는 팀장님이 정말 궁금해할 내용을 적어주는 게 좋다. 한 번 더 안 물어볼 수 있게 작성한 글을 팀장님의 위치에서 다시 한번 읽어보는 것도 하나의 방법이 될 수 있다.만약 독자의 스펙트럼이 넓거나 기술 블로그처럼 불특정 다수라면 가장 지식이 없는 사람에게 쓰는 것처럼 글을 써보자. 가끔 너무 쉽고 자세히 써서 당신이 아마추어처럼 보일 것 같다는 우려를 할 수도 있다. 하지만 지금 글을 쓰는 당신이 적어도 글을 안 쓰고 읽기만 하는 독자보다는 가장 프로에 가깝다.독자가 다 알 거라는 생각은 하지 말자. 최대한 쉽게. 처음 보는 사람도 보고 따라 하거나 이해가 되도록 눈높이를 낮춰서 쓰는 습관을 길러보자. 무려 당신의 글을 시간을 할애하면서까지 읽어주는데 최대한 친절해야 하지 않을까? ✓ 앵무새가 되지 말자.링크만 복붙하거나 소위 말해 펌 글, 정작 내용은 없고 코드만 덩그러니 있거나 단순히 “글쓰기”를 위해 쓰는 글들은 오히려 안 쓰는게 좋다.글에는 자신만의 생각이 녹아있어야 한다고 생각한다. 그렇지 않고서는 따라쟁이 앵무새와 다를 게 없다. 어떠한 오픈소스를 도입하는 과정을 글로 썼다고 생각해보자. Step By Step으로 따라 할 수 있게 작성한 글일지라도 최소한 마지막에는 자신만의 생각이 정리되어 있어야 글을 쓰는 자신도, 글을 읽는 독자도 “마무리”가 될 수 있기 때문이다. 왜 오픈소스를 도입하게 되었고, 도입하는 과정에서의 문제, 도입하고 나서의 장점과 단점 등 이야기할 거리는 무궁무진하다. ✓ 글쓰기에도 호흡이 중요TV나 인터넷 영상들을 보고 있노라면 편집의 기술이 엄청나게 발전된 것을 체감할 수 있다. 혹시 체감하지 못했다면 화면의 전환이나 자막 등 너무 자연스러워서일 수 있다. 글쓰기에서도 이러한 전환이나 문장의 흐름, 호흡은 정말 중요하다.이러한 글쓰기에서의 호흡은 문장 쪼개기, 단락 구분하기, 적절한 그림 및 표 활용 등 독자가 읽을 때 지루하지 않을 정도의 말 그대로 “숨 쉴 수 있는 타이밍”을 제공해야 한다. 읽을 때 집중이 잘 안되거나 어디까지 읽었지 하며 흐름이 끊긴 경우를 경험해 봤을 거라 생각이 든다. 사실 이 부분은 필자도 잘 안되긴 하지만 이번 포스팅처럼 말하고자 하는 메인 키워드 단위로 나눈다거나 약간의 위트를 더하기 위한 짤 같은 것도 이러한 “호흡”의 기술이라 생각한다. ✓ 퇴고. 글쓰기의 가장 중요한 단계지금 이 글을 쓰는 순간에도 퇴고를 10번 이상 하는 것 같다. 필자가 생각하는 퇴고라 함은 쓴 글의 처음부터 끝까지 읽어보며 맞춤법이나 띄어쓰기 교정, 실제로 소리 내어 읽어보며 숨이 차거나 집중이 흐려지진 않은지 하는 일련의 과정을 말한다.퇴고를 꼭 몇번 해야 한다는 정해진 규칙은 없지만 최소 3번은 하는 것 같다. 그러면서 더 중요한 것을 위로 올리고 불필요하게 글자 수만 늘린 건 없는지. 글의 목적과는 거리가 있는 문장은 없는지 등 우리가 서비스를 출시하기 위해 개발 환경에서 테스트를 하고 QA 단계를 거쳐 최종 운영환경에 릴리즈 하는것 처럼.글쓰기는 말하고자 하는 것을 “텍스트”로 전달하는 아주 기본적이며 제한적인 수단이기 때문에 몇 번이고 읽어보면서 고칠 수 있는 부분은 최대한 고치자. 그러면서 글을 썼던 자신을 되돌아보며 무슨 생각으로 이런 글을 썼나 돌아보는 기회도 되고. ✓ 나만의 글쓰기 플랫폼을 찾자.정말 다양한 글쓰기 플랫폼이 있다. Github, 네이버 블로그, 티스토리, 워드프레스 등 서버호스팅 비용 없이도 무료로 제공해주는 곳들인데 각 플랫폼 마다의 장단점이 있으니 자신에게 맞는 곳을 찾아서 글을 써보자. 특히 Github 블로그는 정말 다양한 방법으로 블로그를 만들 수 있고 테마 또한 무궁무진하며 웹에 대한 지식이 있다면 얼마든지 커스터마이징이 가능하다.블로그를 만들었으면 검색에 잘 되도록 SEO 설정을 해두고 RSS를 만들어 국내 기술블로그를 모아둔 awesome-devblog에 자신의 블로그 정보에 대해 PullRequest를 날려보자. 그러면 필자가 만든 기술블로그 구독서비스에서 여러 사람들에게 매일 오전 10시에 친절하게, 그것도 무료로 홍보를 해주기 때문이다. (갑 분 서비스 홍보, 후원좀…)GA(Google Analytics)를 붙여 어디서 유입되고 얼마나 들어오는지 보는 재미도 쏠쏠하다. 필자도 처음엔 많아야 10명(그게 전부 필자였다는건 비밀)이었지만 점점 방문자수가 늘어나니 글을 좀더 재미있고 잘 써야겠다는 사명감과 책임감도 생겨서 초창기에 썼던 글과 요즘의 글을 비교를 해보면 글의 퀄리티가 훨씬 늘어난것 같다. # 마치며무엇을 말하려는지의 글쓴이의 목적은 충분히 전달되었다. 오글거림도 함께. 출처 : http://m.news.zum.com/articles/25408896무엇을 말하려는지의 글쓴이의 목적은 충분히 전달되었다. 오글거림도 함께. 출처 : http://m.news.zum.com/articles/25408896 우리는 사실 어렸을때부터 글쓰기를 해왔다. 어렸을적 그림일기부터 시작하여 사랑하는 사람에게 손편지를 쓰고 싸이월드에 흑역사를 만들었던 시절들. 개발자가 된, 혹은 이제 개발자가 되려는 사람들이 있다면 그냥 글이 아닌 자신이 가지고 있는 기술에 대한 글을 써보는건 어떨까. Stack Overflow Driven Development (SODD) 라는 말이 있듯이 개발은 사실 엄청난 성능과 최적의 알고리즘을 요하는게 아니라면 개발자 간의 경쟁력은 일반적인 개발실력 이외엔 시간과 경험의 차이인것 같다. 여기에 글쓰기 연습을 하며 보다 논리적이고 정리하는 습관을 기른다면 이또한 남들과는 다른 나만의 무기가 될수 있지 않을까 하는 생각을 해본다.이 글을 읽는 독자분들 중 자신만의 기술블로그가 없다면 지금 당장이라도 시작하라고 권하고 싶다. 첫 시작은 어렵겠지만 자신만의 스타일로 “글쓰는 개발자”가 되는데 건투를 빈다. # 참고좋은 기술 블로그를 만들어 나가기 위한 8가지 제언강원국의 글쓰기 : 남과 다른 글은 어떻게 쓰는가","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"writing","slug":"writing","permalink":"https://taetaetae.github.io/tags/writing/"},{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/tags/blog/"}]},{"title":"더이상 기다리지 않아도 되는 배치 무중단 배포","slug":"batch-nondisruptive-deploy","date":"2019-10-13T06:46:12.000Z","updated":"2020-04-23T04:41:36.690Z","comments":true,"path":"2019/10/13/batch-nondisruptive-deploy/","link":"","permalink":"https://taetaetae.github.io/2019/10/13/batch-nondisruptive-deploy/","excerpt":"지난 포스팅, 그러니까 우아한 형제들에서 초대를 받아 Spring batch 에 대한 테크세미나에 다녀 왔다. 그 중 가장 인상깊었던 부분이 바로 무중단 배포. 차일피일 미루다 필자가 속한 팀에서도 배포때마다 가장 불편을 느끼고 있었던 부분이었기도 했고","text":"지난 포스팅, 그러니까 우아한 형제들에서 초대를 받아 Spring batch 에 대한 테크세미나에 다녀 왔다. 그 중 가장 인상깊었던 부분이 바로 무중단 배포. 차일피일 미루다 필자가 속한 팀에서도 배포때마다 가장 불편을 느끼고 있었던 부분이었기도 했고, 그런가보다 하며 개념만 알고 넘어가기엔 무언가 양심에 찔려 직접 무중단 배포를 할 수 있도록 구성을 해보고 테스트까지 해보고자 한다. # 상황 및 문제점리눅스 서버에 Jenkins가 설치되어 있고, Spring batch 모듈을 실행시키고 있다. 수동으로 실행을 하거나, Jenkins RestApi를 이용해서 실행을 할 수 있지만 주로 정해진 시간 즉, 스케쥴링에 의해 실행되곤 한다. 스케쥴링의 가장 작은 단위는 1분단위 배치도 있기 때문에 24시간 멈추지 않고 실행되고 있다고 무방하다. 하지만 배치 모듈이 수정되고, 배포를 하기 위해서는 다음과 같은 시나리오로 진행이 된다. Jenkins 설정의 끄기전 준비 를 실행하여 더이상 Jenkins에 의해 Spring batch 모듈(이하 Job)이 실행되지 않도록 한다. 새로운 Job은 더이상 실행되지 않지만 이미 실행중이였던 Job 은 강제로 중단을 하거나 Job 이 끝날때까지 기다린다. 실행중인 Job이 없을 경우 이제 배포를 진행한다. 배포가 완료되면 Jenkins 설정의 끄기전 준비를 해제한다. 실행중인 Job이 안끝나면 마냥 기다릴텐가? 출처 : https://m.post.naver.com/viewer/postView.nhn?volumeNo=14100660&memberNo=2032633실행중인 Job이 안끝나면 마냥 기다릴텐가? 출처 : https://m.post.naver.com/viewer/postView.nhn?volumeNo=14100660&memberNo=2032633 실행되는 Job을 중단하지 못하는 상황 즉, 실행중에 중단하면 트랜잭션이 깨져 무조건 기다려야만 하는 상황이라면 배포 또한 계속 지연될 수 밖에 없는 상황인 것이다. Spring boot에 java config 를 활용하고 딱 jar 파일 하나를 실행하는 방식이라면 jar파일을 바꿔치기 하는 식으로 고민을 해볼수도 있을것 같다. 하지만 Legacy 코드가 아직 존재하여 일반 Spring 에 xml 로 config 하는 방식으로 운영중이라 jar파일 하나만 바꿔치기 하기엔 무리가 있는 상황. 은총알처럼 어디에서나 사용이 가능한 만병통치약 같은 방법은 없다. 언제나 그랬듯 현재 시스템(xml config 방식)에 가장 최적화된 방법, 그리고 java config 방식에서도 사용이 가능할것 같은 방법을 생각해 보았다. # 무중단 배포를 가능케 하는 3가지 핵심1. 배포를 매번 새로운 경로에 배포한다.각 회사마다, 그리고 서비스마다 정말 다양한 배포 시스템이 있다. 그들의 공통점은 원격서버의 특정 경로에 빌드된 파일들을 밀어 넣어준다는 것. 시나리오는 다음과 같다. 배포할때마다 별도의 디렉토리를 생성한뒤 심볼릭 링크를 연결해준다. 배포는 1에서 연결한 심볼릭 링크에 배포되도록 설정, 결국 매번 만들어지는 디렉토리에 배포가 되게 된다. 여기서 중요한점은 “배포할 때마다 새로운 디렉토리에 배포가 된다” 와 배포시에는 항상 심볼릭 링크에만 배포를 하면 되기 때문에 “배포시스템이 새로 만들어지는 디렉토리의 경로를 몰라도 무방하다”는 점이다.123456#!/bin/shcd /~~~/deploy/# 임시 디렉토리DIRECTORY_NAME=batch_$(/bin/date +%Y%m%d%H%M%S)mkdir $DIRECTORY_NAME 위 쉘 스크립트를 실행하면 batch_20191012205218 와 같은 디렉토리가 생성이 된다. 심볼릭 링크 관련해서는 바로 아래 이어서 설명하겠다. 2. 심볼릭 링크의 원래 링크를 즉시 변경보통 심볼릭 링크 (즉, 바로가기) 의 경로를 변경하기 위해서는 아래처럼 지웠다가 삭제하는 식으로 했었는데123456789101112131415$ mkdir directory_a$ mkdir directory_b$ ln -s directory_a asdf$ llasdf -&gt; directory_adirectory_adirectory_b# directory_a 에서 directory_b 로 바꾸는 경우 (심볼릭 링크 자체를 삭제하고 다시 심볼릭 링크 생성)$ rm asdf$ ln -s directory_b asdf$ llasdf -&gt; directory_bdirectory_adirectory_b 이렇게 되면 삭제하고 ~ 다시 만들어지는 타이밍에 배포가 되거나 실행이 되는 즉, 해당 경로에 엑세스 하는 경우 이전의 경로를 바라본다거나 의도했던 방식으로 실행이 되지 않는 상황이 발생한다. (찰나의 타이밍 이지만 필자는 이러한 문제로 이전의 경로를 바라보는 문제가 발생했었다.) 그래서 ln 의 옵션중인 -Tfs옵션으로 즉시 변경을 해주도록 하자. (ln man 참고)12# 만든 임시 디렉토리로 배포될수 있도록 설정한다.ln -Tfs /deploy/$DIRECTORY_NAME /~~~/deploy/batch 3. 심볼릭 링크가 가리키는 원래 링크에서 실행리눅스 명령어 중에 readlink라는게 있다. 실제 링크를 얻어오는 명령어 인데 이를 활용하여 위에서 설정해둔 심볼릭 링크의 실제 링크(최신 배포된 경로)를 가져오고 그곳에서 Spring batch 모듈을 실행하는 식으로 구성을 해보자.1234#!/bin/bashBASEDIR=`readlink -f $(dirname $0)` # -f 옵션 : 전체경로cd $BASEDIR # 이후 Spring batch jar 실행 이렇게 되면 Job이 실행중이라도 기존에 실행중인 Job은 기존 모듈을 바라보고 실행이 되고, 도중에 새로 배포가 되어도 기존 실행되는 Job에는 영향을 주지 않으며(심볼릭 링크에 연결되었던 과거 배포 경로에서 실행되고 있기 때문) 새롭게 배포된 후 Job이 실행될때도 배포된 경로의 &gt; 심볼릭 링크의 &gt; 실제 링크 즉, 새롭게 배포된 경로에서 실행되기 때문에 무중단 배포가 가능하게 된다. # 전체 흐름핵심만 설명하다보니 전체적으로 어떻게 돌아가는지 이해를 못하셨을 분들을 위해 전체 흐름에 대해 설명을 해보고자 한다.1. 배포 전 임시 디렉토리를 생성하고, 그곳에 배포가 될 수 있도록 심볼릭 링크를 연결해준다. 123456789#!/bin/shcd /~~~/deploy/# 임시 디렉토리DIRECTORY_NAME=batch_$(/bin/date +%Y%m%d%H%M%S)mkdir $DIRECTORY_NAME# 만든 임시 디렉토리로 배포될수 있도록 설정한다.ln -Tfs /~~~/deploy/$DIRECTORY_NAME /~~~/deploy/batch 2. 배포 배포 시스템에 의해 /~~~/deploy/batch 로 배포되도록 한다. 3. 배포 후 이후 배포 실행은 새롭게 배포된 경로에서 실행되도록 심볼릭 링크를 수정해준다. 12345678#!/bin/shcd /~~~/deploy/# 배포된 경로를 실행할 경로로 변경해준다.REAL_DIRECTORY_PATH=$(readlink -f /~~~/deploy/batch)ln -Tfs $REAL_DIRECTORY_PATH /~~~/deploy/batch# 예전에 배포된 폴더들을 삭제해준다. (최근 몇개까지만 지울것인가는 상황에 따라) 4. 배치 실행 1234#!/bin/bashBASEDIR=`readlink -f $(dirname $0)` # -f 옵션 : 전체경로cd $BASEDIR # 이후 배치 실행 ( e.g. batch.jar xxxJob ) # 마치며jar파일이 실행되고 JVM에 올라가게 되면 jar파일을 삭제한다거나 위치를 이동시켜도 에러가 나거나 하지는 않지만 코드 내에서 상대경로같은 설정들이 있기 때문에 폴더 전체를 심볼릭 링크로 연결하고 그 안에서 실행되도록 수정하였다. 앞서 이야기 했지만 이러한 설계는 어디까지나 필자가 운영하고 있는 상황에 맞춘것이기 때문에 이를 어떻게 잘 활용하는가가 이번 포스팅에 주요 핵심이 될 수 있을것 같다.항상 배포 할때마다 예전에 그렇게 해왔기 때문에 라는 핑계로 Job이 돌고있으면 기다렸다가 배포해야만 했던 필자 자신이 부끄러워진다. 시도조차 안해보고 그런가보다 하고 적응만 하려 하거나, 불편하지만 안불편한척 하는 그런 태도를 버려야 하지 않을까 하는 반성을 해보는 시간이 되었다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"batch","slug":"batch","permalink":"https://taetaetae.github.io/tags/batch/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"linux","slug":"linux","permalink":"https://taetaetae.github.io/tags/linux/"}]},{"title":"우아한 스프링 배치 테크세미나 정리 및 후기 (by 우아한 형제들)","slug":"woowabros-spring-batch","date":"2019-09-29T08:55:50.000Z","updated":"2020-04-23T04:41:36.898Z","comments":true,"path":"2019/09/29/woowabros-spring-batch/","link":"","permalink":"https://taetaetae.github.io/2019/09/29/woowabros-spring-batch/","excerpt":"지난주 우아한 형제들에서 진행하였던 “9월 우아한 테크 세미나 - 우아한 스프링 배치” 에 다녀왔다. 필자에게 이번 9월은 정신이 어디에 있는지 모를만큼 바쁘고 힘들었지만 예전부터 궁금하기도 했고","text":"지난주 우아한 형제들에서 진행하였던 “9월 우아한 테크 세미나 - 우아한 스프링 배치” 에 다녀왔다. 필자에게 이번 9월은 정신이 어디에 있는지 모를만큼 바쁘고 힘들었지만 예전부터 궁금하기도 했고 요즘들어 관심을 갖던 “배치 어플리케이션”을 어떻게 하면 “우아한 방법”으로 사용할 수 있을지에 대해 여러 생각들이 있었기에 큰 기대를 가지고 지옥철을 견디며 잠실 근처에 있는 우아한 형제들 작은집으로 가게 되었다.어떤 내용을 발표하였는지에 대해 기억잘하는 똑똑한 앵무새가 되어 정리하기 보다 주요 포인트에 대한 생각과 함께 참여를 못한 분들 위해서라기 보다 내 스스로 정리를 하기 위해 포스팅을 작성해 보고자 한다.(이번에도 불러주셔서 감사합니다 ^=^) # 인트로연사자 분은 워낙에 유명하신 분이라 별도의 설명이 필요 없이 운영하시는 블로그 주소로 대체를 해본다. 이번 행사에 초대되신 분들은 한번이라도 스프링 배치를 써분 분들을 대상으로 진행하게 되었다고 했는데 마침 필자도 팀 내에서 운영하고 있는 배치 어플리케이션을 보다 효율적이고 우아하게 바꿔보고자 하는 니즈가 있었기에 아마 초대된게 아닐까 싶다.아기자기한 우아한 형제들 건물 내부아기자기한 우아한 형제들 건물 내부 더불어 발표전에 간략히 회사가 원하는 인재에 대하여 언급해주셨는데 그게 어찌나 공감이 가던지. 역시 생각이 남다른 회사구나 하고 다시한번 생각을. 자기보다 경험이 “적은” 사람에게 “설득을 당할 수” 있어야 하고, 자기보다 경험이 “많은 사람을 설득” 시킬 수 있어야 한다. # 기본편배치 어플리케이션이란 컴퓨터에서 사람와 상호작용없이 이어지는 프로그램(작업)들의 실행이라고 위키피디아에 간결&amp;명료하게 정리되어 있다. 그만큼 일반적인 웹 어플리케이션과의 차이가 있는데 웹 어플리케이션은 실시간 처리가 기본이고 요청에 대한 응답을 제공해야 하니 아무래도 속도가 상대적이며 QA시 편한 부분이 있다. 그에 반해 배치 어플리케이션은 웹 어플리케이션에서 말하는 요청이라는 개념보다 후속처리에 가깝고, 속도 또한 절대적이며 QA가 복잡하다는게 특징이다. 따라서 테스트코드는 웹 어플리케이션 보다 배치 어플리케이션이 더 필요하다고 볼 수 있다.배치 어플리케이션이 필요한 상황은 크게 두가지로 나눠 볼 수가 있다고 한다. 일정 주기로 실행 되어야 할 때 실시간 처리가 어려운 대량의 데이터를 처리 할때 평소 첫번째 상황만 생각하고 배치 어플리케이션을 작성하곤 했었는데 두번재 상황에 대해 생각에 생각을 더 해보니 스프링 배치를 간단하게만 (Tasklet) 사용하고 있는건 아닌가 하는 반성을 해보곤 했다. (Reader, Processor, Writer 등 다양한 레이어가 있는데도…) 특히 스프링 배치에서는 기본적으로 모든 데이터를 메모리에 쌓지 않는 조회방식라고 한다. (DB기준) Paging 혹은 Cursor로 pageSize만큼만 읽어오고 chunkSize만큼만 commit 하는 형태. 이러한 각 레이어별 size를 잘 조정하기만 해도 적은 노력으로 큰 성능을 얻을 수 있는 부분이 프레임워크를 사용하는 이유 아닐까 라고 생각해본다. 또한 @JobScope 나 @StepScope는 Late Binding 즉 배치 어플리케이션이 실행되는 시점이 아니라 Job 이 실행될때 생성이 되기 때문에 이를 활용하여 동적으로 reader / processor / wirter 레이어를 만들 수 있다고 한다. # 활용편스프링 배치를 이용한 배치 어플리케이션이 있고 이를 스케쥴링 등 관리를 해주는 도구들에 이야기를 해주셨다. Cron 리눅스를 어느정도 사용해봤다면 알만한 리눅스 기본 스케쥴링 프로그램인 Cron. 필자도 Cron 으로 주기적으로 실행하도록 설정해보기도 하였지만 배치 어플리케이션의 특성상 로그 및 실행/종료 등 제한사항이 많은 건 사실인것 같다. Spring MVC + API Call 주변에서 사용하고 있다고 하던 방식. 이 방식의 장점은 항상 떠있기 때문에 어플리케이션 구동시간이 별도로 필요 없다는 장점이 있지만 전반적인 관리가 어려운 단점이 있는것 같다. 물론 울며 겨자먹기 식으로 단점을 극복할 방법은 여러가지가 있겠지만 모든건 항상 Trade off Spring Batch Admin (Deprecated) 예전 팀분이 알려주셔서 잠깐 봤던 부분이긴 한데 어느사이에 Deprecated 되었다고 한다. Quertz + Admin http://www.quartz-scheduler.org/ 아주 오래전에 써본 기억이 있지만 배보다 배꼽이 더 큰 상황같았던 힘들었던 기억들만 남아있는 구현방법인것 같다. 여러 레이어를 혼용해서 쓰다보면 각 레이어간의 상호 연결성의 위배되는 경우가 많기에… CI Tools (Jenkins / Teamcity 등) 아무래도 가장 추천할만한게 CI Tool 인것 같다. 그중에 필자도 Jenkins라는 툴을 너무 좋아하고. 유료 툴 중에 Teamcity 를 잠깐 언급해주셨는데 찾아보니 한번즈음 써보고 싶을만한 기능들이 있어보였다. Jenkins 의 장점은 말해뭐해 정도로 배치 어플리케이션과 궁합이 너무 잘 맞는 툴인것 같다. (물론 다른 툴들도 있겠지만 필자개취라 넘어가도록 하자.) 특히 실행시 필요한 플러그인들이 다양하게 많이 있고, 실행방법 또한 수동/스케쥴링 으로 다양하게 할 수가 있으며 RestAPI 지원과 보안, 실행이력관리, 로그 등 최적화 되어있다고 해도 과언이 아닐정도로 다양한 장점들이 있는것 같다. Jenkins 설정중에 Global properteis 을 통해 환경변수를 설정하는것도 가능하다고 설명해 주셨다. 또한 환경변수들의 묶음을 다시 환경변수로 재 정의해서 사용할 수 도 있고. 참 대단한 Jenkins. 필자는 이제까지 하나의 쉘스크립트를 만들고 공통으로 사용할 파라미터들을 스크립트 단에서 설정후에 Jenkins 에서 쉘스크립트를 실행하는 방식으로 구성하곤 하였는데 이렇게 Jenkins 의 환경변수를 이용하는 방법도 다른 측면에서 활용범위가 높을것 같아 좋아보였다. 무중단 배포에 대해 설명을 해주셨다. 이는 사실 스프링배치 나 Jenkins 와는 관련이 없지만 이 둘을 사용하면서 배포를 할때 리눅스의 명령어니 readlink와 ln -s를 활용하여 중단없이 배포를 할 수 있도록 한다고 한다. 필자는 이제까지 Jenkins의 끄기전 준비를 실행 하고 스케쥴러에 의해 다음 Job이 실행되지 않는것을 확인 후에 배포를 하곤 했었는데 이러한 기능을 통해 충분히 무중단 배포를 구성 해볼수도 있을껏 같았다. 이 부분은 별도의 포스팅으로 정리를 해볼까 한다. 정말 중요한 Jenkins의 장점과 멱등성정말 중요한 Jenkins의 장점과 멱등성 멱등성에 대해 설명해 주셨다. 필자도 같은 생각인데 배치 어플리케이션을 구성하면서 가장 중요시 생각해야할 개념이 멱등성인것 같다. 우선 멱등성이란 연산을 여러번 적용 하더라도 결과가 달라지지 않는 성질을 의미하는데 코드 내에 LocalDateTime.now()같은게 있다면 과거 기준으로 실행하고 싶어도 해당 코드로 인해 수정/배포를 하지 않고서는 할 수 없는 경우가 생기는것 같다. 그래서 제어할수 없는 코드는 제어할 수 있도록 파라미터를 받아 처리하는 형식으로 구성해야 좀더 효율적으로 실행할 수 있는 것 같다. 배치 어플리케이션에 대한 책을 추천해주셨다. The Definitive Guide to Spring Batch와 내년 상반기에 책을 출간할 예정이라고 하시는데 꼭 들여다 보고 싶을 책인것 같다. (국내 최초 스프링 배치에 대한 내용의 책!) # 오늘의 질문이번에도 어김없이 이 시간을 내것으로 만들기 위해 질문을 하였다. 이 세미나에 온 목적이기도 하다.젠킨스로 스케쥴링을 하고, jar를 실행시켜 젠킨스에 로깅을 남기는게 일반적인것 실행방식 같은데 그러다보면 jar실행시 job과 관련없는 bean들이 뜨다보니 실행시간이 느려진다. 이부분은 어떻게 해소할수 있을까?​라는 질문에 @ConditionalOnProperty 어노테이션을 활용하게되면 해당 Job 에서 필요한 bean 만 띄울수 있다고 하셨다.필자가 운영하고 있는 스프링 배치 버전이 3.x 이기도 하고 멀티 모듈로 구성되어 있으며 (batch가 단독 컴포넌트가 아님…) 필요한 bean만을 지정하기에는 스파게티 코드가 될게 뻔한 상황인것 같아 질문의 답변에서 해법을 찾기에는 조금 힘들었지만 또 언제나 그랬듯 이러한 상황에서 해결방법을 찾아야 하는게 개발자의 숙명 아니겠는가. 좀더 고민해봐야할 부분인것 같다. # 마치며스프링 배치에 대한 기본개념과 관리도구를 활용해서 생생한 현장감과 함께 배치 어플리케이션의 운영 노하우를 들을 수 있어서 너무 좋았던 세미나였다. 이번에도 역시나 내가 고민하고 있던 문제는 누군가 이미 고민했던 문제라는것, 그리고 그러한 고민의 해결방법을 공유함에 있어 생겨나는 가치에 대해 다시한번 온몸으로 뜨거운 열정을 느낄수 있었던 날로 기억에 남을 것 같다.충분하면 만족해야만 할까?출처 : https://vryjam.com/gif.php?id=ODWEBD0bmRGzTq5충분하면 만족해야만 할까?출처 : https://vryjam.com/gif.php?id=ODWEBD0bmRGzTq5다만, 이제까지 A라는 구조로 구성된 어플리케이션에 단순 기능 추가만 할게 아니라 정말 A라는 구조가 최선일까, A의 구조보다 보다 더 효율적이고 유연한 B 나 C 의 구조는 없을까 하는 이러한 고민을 계속 하려는 자세를 가져야 겠구나 하는 생각을 해본다.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"batch","slug":"batch","permalink":"https://taetaetae.github.io/tags/batch/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"}]},{"title":"네트워크 모니터링이 궁금할땐 ? Packetbeat !","slug":"network-monitor-by-packetbeat","date":"2019-09-08T09:11:34.000Z","updated":"2020-04-23T04:41:36.808Z","comments":true,"path":"2019/09/08/network-monitor-by-packetbeat/","link":"","permalink":"https://taetaetae.github.io/2019/09/08/network-monitor-by-packetbeat/","excerpt":"모니터링은 서비스 로직 개발 만큼 한번씩 고민해보고 경험해 봤을 중요한 영역이라 할 수 있다. 그중 웹서버에서 제공해주는 엑세스 로그는 운영하고 있는 웹서비스에 대해 여러가지 측면에서 분석할 수 있는 가장 강력한 아이템 중에 하나라고 생각한다.","text":"모니터링은 서비스 로직 개발 만큼 한번씩 고민해보고 경험해 봤을 중요한 영역이라 할 수 있다. 그중 웹서버에서 제공해주는 엑세스 로그는 운영하고 있는 웹서비스에 대해 여러가지 측면에서 분석할 수 있는 가장 강력한 아이템 중에 하나라고 생각한다. 이를 통해 사용자들이 어떤 url을 많이 호출하고, 어떤 user-agent형태를 사용하는지 알게 되면 그에 따라 서비스 전략을 변경할수도 있고 악의적으로 공격적인 요청에 대해 웹서버단에서 차단을 할 수 있기 때문이다.이렇게 inbound 트래픽(외부에서 들어오는 요청)에 대해서는 엑세스 로그를 잘 분석하면 기존의 웹 어플리케이션과는 전혀 무관하게 모니터링이 가능하지만 반대로 outbund 트래픽(외부로 나가는 요청)에 대해서는 어떤식으로 모니터링을 할 수 있을까? 월급통장의 inbound 트래픽보다 outbound 트래픽이 너무 많은 요즘...이미지 출처 : https://www.app24moa.com/feedDetail/2/2002월급통장의 inbound 트래픽보다 outbound 트래픽이 너무 많은 요즘...이미지 출처 : https://www.app24moa.com/feedDetail/2/2002 예컨데, 날씨 서비스를 하기 위해 외부에서 서울날씨라는 페이지를 조회했을 경우 기상청 API에서 넘겨받은 데이터를 가공하여 보여준다고 가정해보자. 이때 기상청에서 제공해주는 특정 API중에 어느 하나가 늦게 응답이 온다거나, 특정시간대에 에러응답을 받을경우 과연 이를 어떤식으로 모니터링 할수 있을까? 어플리케이션 코드에 모니터링을 위한 코드를 추가할 것인가? 혹 하나의 서버에서 A모듈은 java로, B모듈은 python으로 개발되었을 경우 각각 모듈마다 모니터링을 위한 코드를 추가하는 식으로 하다보면 비지니스 로직을 방해하거나 오히려 추가한 코드 또한 관리해야 하는 배보다 배꼽이 더 커져버릴 상황도 생길수 있다.어플리케이션의 비지니스 로직과는 무관하게 서버 자체에서 외부로 나가는 네트워크 트래픽에 대해 모니터링을 할 수 있는 가벼우면서도 심플한 모듈을 찾고 싶었다. 어플리케이션의 개발언어가 무엇이든 상관없이 별도의 에이전트 형식으로 띄워두기만 하면 네트워크 트래픽을 수집 및 분석, 나아가서는 모니터링까지 할수있는… 그래서 찾다보니 역시나 이러한 고민을 누군가는 하고 있었고 오픈소스까지 되어있는 Elastic Stack 의 Beat중 Packetbeat라는 데이터 수집모듈을 알게 되었다. 역시 내가 하고있는 고민은 이미 누군가 했던 고민들… 이러한 고민에 대해 해결하는 방법을 보다 빨리 찾는게 경쟁력이 될텐데… 이번 포스팅에서는 Packetbeat 에 대해 간단히 알아보고 이를 활용하여 outbound 트래픽에 대해 모니터링을 해보며 어떤식으로 활용할 수 있는지에 대해 알아보고자 한다. # Packetbeat ?ElasticStack 중에 데이터 수집기 플랫폼인 Beats중 네트워크 트래픽 데이터에 대해 수집을 할 수 있는 데이터 수집기를 제공하고 있다. pcap라이브러리를 이용하여 서버의 네트워크 레벨에서 데이터를 수집 및 분석한 후 외부로(Elasticsearch, Logstash, Kafka 등) 전송해주는 경량 네트워크 패킷 분석기라고 공식 홈페이지에 소개되고 있다.몇번 사용해보면서 느낀 장점들은 다음과 같다. 설치 및 실행이 너무 간단하다. 설정값 튜닝을 통해 간단하지만, 그러한 간단함에 비해서 너무 강력한 수집이 가능하다. 앞서 이야기 했던 어플리케이션 코드와는 전혀 무관하게 작동한다. # 무엇을 해볼것인가?! (a.k.a. 목표)필자가 운영하는 Daily-DevBlog 라는 서비스가 있다. (갑분 서비스 홍보) 여러 사람들의 rss를 조회하고 파싱해서 메일을 보내주는 서비스 인데, packetbeat 사용 예시를 들기위해 조금 변형하여 모든 rss를 접근하고 가장 최신글의 제목을 출력하는 아주 간단한 python 스크립트로 outbound 트래픽을 발생시켜 보고자 한다.그리고 packetbeat 를 이용하여 외부로 호출되는 트래픽을 수집하고 Elasticsearch 로 인덱싱 하여 최종적으로는 어느 rss의 속도가 가장 느린지 실행되는 python코드와는 전혀 관련없이 모니터링 해보고자 한다.python 코드는 다음과 같다. 참고로 필자는 awesome-devblog의 운영자분께 해당 데이터 사용에 대해 허락을 받은 상태이다. 1234567891011121314151617import requests, yaml, feedparserblog_info_list_yml_url = 'https://raw.githubusercontent.com/sarojaba/awesome-devblog/master/db.yml'blog_info_list_yml = requests.get(url=blog_info_list_yml_url).textblog_info_yaml_parse_list = yaml.load(blog_info_list_yml)for blog_info in blog_info_yaml_parse_list : if 'rss' not in blog_info.keys() or not blog_info['rss']: continue rss_url = blog_info['rss'] try : parse_feed = feedparser.parse(rss_url) except : continue parse_feed_data = parse_feed.entries[0] print(blog_info['name'], '|', parse_feed_data['title'], '|', parse_feed_data['link']) 위 코드를 실행하면 아래처럼 아주 간단하게 블로그 주인의 이름과 최신글 제목, 링크가 출력이 된다. 그러고 보니 너무 오랜만에 글쓰네... (숙연)그러고 보니 너무 오랜만에 글쓰네... (숙연) # 백문이 불여일견? 백견이 불여일타!언제 어디서부터 유래된 이야기 인지는 모르지만 “백번 듣는것이 한번 보는것보다 못하고, 백번 보는것이 한번 타자 치는것보다 못하다” 라는 개발버전 속담이 있다. 자, 위에서 정의한 목표를 이루기 위해 실제로 각종 모듈을 설치해 보도록 하자! ( 필자가 테스트 했던 서버의 환경은 CentOS 7.4 64Bit 이니 참고 ) Elasticsearch 이왕 설치하는거 가장 최신버전인 7.3.1을 설치해보자! (버전업이 빨라도 너~무 빨라…) 12345678910111213// 다운을 받고wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.3.1-linux-x86_64.tar.gz// 압축을 푼다음tar -zxvf elasticsearch-7.3.1-linux-x86_64.tar.gzcd elasticsearch-7.3.1/conf// 각종 설정후vi elasticsearch.yml node.name: node-1 network.host: 0.0.0.0 discovery.seed_hosts: [&quot;localhost&quot;] cluster.initial_master_nodes: [&quot;node-1&quot;]// 실행bin/elasticsearch http://server-url:9200접근시 아래처럼 나오면 설치 성공 12345678910&#123; \"name\": \"node-1\", \"cluster_name\": \"elasticsearch\", \"cluster_uuid\": \"---\", \"version\": &#123; \"number\": \"7.3.1\", ~~~ &#125;, \"tagline\": \"You Know, for Search\"&#125; Kibana 1234567891011// 다운을 받고 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.3.1-linux-x86_64.tar.gz// 압축을 푼 다음tar -zxvf kibana-7.3.1-linux-x86_64.tar.gzcd kibana-7.3.1-linux-x86_64/config // 각종 설정후 vi kibana.yml server.host: &quot;~.~.~.~&quot; elasticsearch.hosts: [&quot;http://~.~.~.~:9200&quot;]// 실행bin/kibana http://server-url:5601접근시 키바나 화면이 나오면 설치 성공 Packetbeat 1234567891011// 다운을 받고wget https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.3.1-linux-x86_64.tar.gz// 압축을 푼 다음tar -zxvf packetbeat-7.3.1-linux-x86_64.tar.gz// 각종 설정후 실행 (root 권한으로 실행해야 함)sudo chown root:root packetbeat.ymlsudo vi packetbeat.yml output.elasticsearch: # Array of hosts to connect to. hosts: [&quot;~.~.~.~:9200&quot;]sudo ./packetbeat -e -c packetbeat.yml 이렇게 하고나서 키바나에 가보면 아래처럼 Packetbeat 인덱스 패턴을 만들수 있고 수집이 되고있는것 까지 확인 가능하다. 데이터가 엄~청 다양하고 많이 수집된다.데이터가 엄~청 다양하고 많이 수집된다. # 무엇을 모니터링 할 수 있을까?이제 각종 구성은 했으니 처음에 목표한 어느 rss가 가장 느린가를 체크해 볼 시간이다. python 스크립트를 돌리면 packetbeat 에 의해 네트워크 트래픽이 수집~분석~Elasticsearch에 인덱싱이 되고 이를 키바나의 비쥬얼라이즈를 통해 적절하게 만들어보면 아래처럼 너무나도 간단하게 어느 rss의 응답속도가 가장 느린지 확인할 수 있다. event.duration 필드는 기본적으로 nano second 이다보니 아래 그림에서는 2.6초가 가장 오래걸린 rss url 이라 볼 수 있다.보라, 키바나의 강력한 비쥬얼라이즈 기능을! (아 눈부셔)보라, 키바나의 강력한 비쥬얼라이즈 기능을! (아 눈부셔) 한가지 더, packetbeat를 설치하고 기본 설정으로 실행하게 되면 불필요한(outbound 트래픽만을 수집하겠다던 목표와는 무관한) 데이터들도 수집되다보니 아무래도 cpu에 불필요한 부하가 발생할수 있고(아무래도 모든 네트워크 트래픽을 트래킹 하고 분석해야하니…) Elasticsearch 에도 불필요한 데이터가 인덱싱 되곤 한다. 그래서 지금의 Packetbeat 뿐만 아니라 오픈소스를 사용할 경우엔 설정값들을 정확히 알고 목적에 맞는 커스터마이징은 필수인듯 하다. 필자는 http의 outbound 트래픽만을 보고 싶었기 때문에 아래처럼 packetbeat 설정을 하고 다시 실행 해보면 Elasticsearch 에 수집되는 도큐먼트 사이즈가 확연하게 차이나는 것을 확인할 수 있다.123456789packetbeat.protocols: # 아래 2개 이외에는 전부 주석처리- type: http ports: [80] # 80 port 의 http를 수집하겠다.- type: tls ports: - 443 # 443 의 tls를 수집하겠다.processors: - drop_event.when.equals.network.direction : &quot;inbound&quot; # inbound는 수집하지 않겠다. 좌측이 기본, 우측이 불필요 데이터 제외하고 나서의 수집 상태좌측이 기본, 우측이 불필요 데이터 제외하고 나서의 수집 상태 사실 위 설정값은 페이스북 한국 Elasticsearch 유저그룹에 문의해서 알게된 내용이다. 역시 커뮤니티 파워, 집단지성의 힘을 다시한번 느낄 수 있었다. (모르면 물어보자! + 문제에 대해 좀더 잘 검색하도록 노력하자!) # 마치며Packetbeat 을 사용하면서 가장 좋았던 점은 기존 로직과는 전혀 무관하게 작동하는 점이 가장 좋았다. 이러한 점은 어느 상황에서도 서비스 코드 디펜던시가 없어 자유롭게 활용이 가능하다는 뜻으로 해석을 해보곤 한다.마냥 좋다고 운영환경에 무작정 도입하면 이런 따사로운 눈빛을 받을 수 있으니 참고이미지 출처 : https://namu.wiki/w/%EB%82%98%EB%8A%94%20%EC%9E%90%EC%97%B0%EC%9D%B8%EC%9D%B4%EB%8B%A4마냥 좋다고 운영환경에 무작정 도입하면 이런 따사로운 눈빛을 받을 수 있으니 참고이미지 출처 : https://namu.wiki/w/%EB%82%98%EB%8A%94%20%EC%9E%90%EC%97%B0%EC%9D%B8%EC%9D%B4%EB%8B%A4필자는 최근 운영환경에도 packetbeat를 적용해서 outbound 트래픽을 모니터링 하고 문제가 있는 엔드포인트에 대해 자동으로 점검을 하는 시스템을 만들려고 하고 있는데, 네트워크 패킷을 전부 까보며(?) 아무래도 cpu 성능에 지장을 줄수밖에 없는 오픈소스 모듈이다보니 다양한 테스트를 통해 서비스 운영에 영향이 없도록 설정값들을 튜닝해 가며 적용해봐야 할 것 같다. (무작정 좋다고 적용하다 오히려 큰 화를 부를 수 있다…) 내가 맛있어 하는 음식이 남들도 맛있으리란 법 없듯, 소개팅에 나가기전 준비한 멘트가 전부 먹히리라는 법 없듯…모든 상황에는 튜닝은 필수다. 그 튜닝을 얼마나 잘, 그리고 센스있게 하냐가 포인트!","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"packetbeat","slug":"packetbeat","permalink":"https://taetaetae.github.io/tags/packetbeat/"},{"name":"network","slug":"network","permalink":"https://taetaetae.github.io/tags/network/"}]},{"title":"아파치 로드밸런싱으로 여러 WAS 운영하기","slug":"apache-load-balancing","date":"2019-08-04T10:50:43.000Z","updated":"2020-04-23T04:41:36.672Z","comments":true,"path":"2019/08/04/apache-load-balancing/","link":"","permalink":"https://taetaetae.github.io/2019/08/04/apache-load-balancing/","excerpt":"웹서버 하나만 사용하거나 WAS 하나만을 사용하며 웹서비스를 운영하는 경우는 극히 드물다. 웹서버의 장점과 WAS의 장점 그 두마리의 토끼를 다 잡기 위해 보통 앞단에 웹서버를 두고 그 뒤에 WAS를 두며 서비스를 운영하곤 한다. 헌데 운영하는 서비스가 인기가 많아져(?) 사용량이 많아지다면 그만큼 응답이 느려 (TPS 등) 서버를 늘려야 하는 상황이 생긴다고 가정해보자.","text":"웹서버 하나만 사용하거나 WAS 하나만을 사용하며 웹서비스를 운영하는 경우는 극히 드물다. 웹서버의 장점과 WAS의 장점 그 두마리의 토끼를 다 잡기 위해 보통 앞단에 웹서버를 두고 그 뒤에 WAS를 두며 서비스를 운영하곤 한다. 헌데 운영하는 서비스가 인기가 많아져(?) 사용량이 많아지다면 그만큼 응답이 느려 (TPS 등) 서버를 늘려야 하는 상황이 생긴다고 가정해보자. (물론 서버를 늘리는 것보다 캐시를 적용하거나 로직을 바꿔보는 노력이 선행되야 하겠지만…) 당연히 서버부터 구매하며 “Scale Out”을 하려고 할것이다. 만약 원래 운영하던 서버가 너무 좋아서 CPU나 메모리 사용률이 거의 바닥이여도 서버를 구매해야 할까?서버를 구매하게되면 결국 두개 이상의 서버가 운영될텐데 그 서버들을 앞에서 묶어주며 트래픽을 분산시켜주는 무언가가 필요하다. 그러한 기술을 바로 로드밸런싱 이라고 한다. 통상 L4 스위치를 활용하여 요청을 여러 서버들로 분산시키며 산술적으로는 서버 대수만큼 성능이 좋아지는 효과를 볼 수 있다.하지만 앞서 말했듯 서버의 자원 사용률이 바닥일 정도로 거의 사용을 안할경우 서버를 구매하는건 너무나 비효율적이다. 이번 포스팅에서는 서버를 늘리지 않으면서 웹서버 중 아파치를 활용하여 여러 WAS를 운영하는 방법에 대해 알아보고자 한다. 서버 늘려야 하는 상황에서 사용해 볼 수 있는 나만의 좋은 무기(?)가 생긴게 아닐까 생각이 든다. 아파치는 EOL이 되었기 때문에 2.4버전으로 설치하고, WAS는 편의상 톰켓 최신버전으로 설치해서 동일한 서버에 아파치 한대와 톰켓 3대를 연동하는것을 목적으로 한다. 로드밸런싱이 어떤식으로 이루어 지고 하위에 연결된 톰켓을 컨트롤 하는 방법 또한 알아볼 예정이다. 서버 환경 및 설치하게 될 각 버전은 다음과 같다.서버 : CentOS 7.4 64Bitapache : httpd-2.4.39tomcat : apache-tomcat-8.5.43tomcat-connectors(mod_jk) : 1.2.46 # Apache 와 Tomcat 설치필자의 포스팅에서 종종 나오는 부분이기도 하고, 구글링 해보면 바로 설치 방법을 쉽게 찾을 수 있겠지만 그렇다고 언급을 안하고 넘어가기엔 너무 불친절하니… 치트키처럼(?) 빠르게 정리해보자. Apache1234567891011$ wget http://apache.tt.co.kr//httpd/httpd-2.4.39.tar.gz$ tar -zxvf httpd-2.4.39.tar.gz$ ./configure --prefix=/home/~~~/apache$ make &amp;&amp; make install$ cd /home/~~~/apache/bin$ sudo chown root:계정명 httpd$ sudo chmod +s httpd$ vi /home/~~~/apache/conf/httpd.confUser 계정명Grop 계정명$ /home/~~~/apache/bin/apachectl start ← 실행 이렇게 설치를 한뒤 실행을 시키고 서버의 ip를 접속해보면 아래와 같은 화면을 볼 수 있다. Tomcat123$ wget http://mirror.apache-kr.org/tomcat/tomcat-8/v8.5.43/bin/apache-tomcat-8.5.43.tar.gz$ tar -zxvf apache-tomcat-8.5.43.tar.gz$ /home/apache-tomcat-8.5.43/bin/start.sh ← 실행 톰켓의 기본 http 포트인 8080으로 접속을 해보면 귀여운 고양이가 있는 톰켓 기본화면을 볼 수 있다. # 아파치와 톰켓 연동하기아파치와 톰켓의 연동은 mod_jk 와 mod_proxy 등 다양한 모듈로 연동을 할 수 있는데 이번 포스팅에서는 mod_jk 를 활용하는 방법에 대해 알아보고자 한다. 우선 mod_jk 를 설치하자. 간단히 mod_jk 는 컴파일, 설정 등 복잡하지만 톰켓 전용 바이너리 프로토콜인 AJP를 사용하기 때문에 높은 성능을 기대할수가 있다. mod_proxy 는 반면 기본으로 아파치에 탑재되어있는 모듈이기 때문에 별도의 모듈 설치가 필요 없고 설정도 간단하다는 장점이 있다. 각 연동방식의 장단점이 있기 때문에 본인이 운영하는 서버 상황에 맞추어 적용 할 필요가 있다. mod_jk 설치123456$ wget http://apache.tt.co.kr/tomcat/tomcat-connectors/jk/tomcat-connectors-1.2.46-src.tar.gz$ tar -zxvf tomcat-connectors-1.2.46-src.tar.gz$ cd tomcat-connectors-1.2.46-src/native$ ./configure --with-apxs=/home/~~~/apache/bin/apxs$ make &amp;&amp; make install$ /home/~~~/apache/modules 하위에 mod_jk.so가 생김 mod_jk 를 활용하면 AJP라는 통신으로 아파치와 톰켓이 연동되는데 톰켓의 기본 AJP 포트는 8009번임을 알고 다음처럼 설정을 해주자. apache/conf/workers.properties 12345worker.list=tomcat1worker.tomcat1.port=8009worker.tomcat1.host=localhostworker.tomcat1.type=ajp13worker.tomcat1.lbfactor=1 apache/conf/httpd.conf 1234567LoadModule jk_module modules/mod_jk.so&lt;IfModule jk_module&gt; JkWorkersFile conf/workers.properties JkLogFile logs/mod_jk.log JkLogLevel info JkMount /* tomcat1&lt;/IfModule&gt; 이렇게 하고서 아파치와 톰켓을 재시작 후에 서버의 ip로 접속해보면 (별도의 port 없이) 톰켓 설정페이지로 랜딩이 되는것을 확인할 수 있다. # 로드밸런싱을 위한 작업여기까지는 본 포스팅을 작성하기 위한 밑거름이라고 말할 수 있다. 이제 실제로 로드밸런싱을 해볼 차례.앞서 톰켓 하나만 설치했는데 편의상 톰켓 3개를 설치해두자. (하나를 설치하고 cp -r 명령어를 활용하는게 빠르다.) 그 다음 각 톰켓의 모든 포트를 셋다 다르게 설정해야 하는데 겹치지 않도록 설정해 두고 (필자는 앞자리를 1,2,3 이런식으로 다르게 설정하였다.) 워커(workers.properties)를 아래처럼 설정해주자. apache/conf/workers.properties12345678910111213141516171819worker.list=load_balancerworker.load_balancer.type=lbworker.load_balancer.balance_workers=tomcat1,tomcat2,tomcat3worker.tomcat1.port=18009worker.tomcat1.host=localhostworker.tomcat1.type=ajp13worker.tomcat1.lbfactor=1worker.tomcat2.port=28009worker.tomcat2.host=localhostworker.tomcat2.type=ajp13worker.tomcat2.lbfactor=1worker.tomcat3.port=38009worker.tomcat3.host=localhostworker.tomcat3.type=ajp13worker.tomcat3.lbfactor=1 이렇게 설정을 한 뒤 앞서 설정한 httpd.conf 에 JkMount 부분도 아래처럼 변경해주자. apache/conf/httpd.conf1JkMount /* load_balancer 위 설정을 다시한번 살펴보자면, /*으로 들어오는 요청을 load_balancer라는 워커로 넘기는데 워커 설정에서는 로드밸런싱이 설정되어 있기 때문에 tomcat1, tomcat2, tomcat3 골고루 요청을 분산해준다는 의미이다.tomcat 하위 logs 폴더에 보면 아래 기본 설정에 의해 엑세스 로그가 로깅이 되는데123&lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt; 실제로 테스트를 해보면 다음처럼 9번의 요청을 3대의 톰켓에 골고루 요청된 것을 확인할 수 있다. # 로드밸런싱을 컨트롤 하기 (jkmanager)위에서 알아본 mod_jk 를 활용한 로드밸런싱을 별도의 서버 재시작 없이 컨트롤이 가능하다고 한다. 이게 어떤것을 의미하냐면 연동된 톰켓 3대중에 한대를 별도의 서버 셧다운을 하지 않아도 제외시킬수 있으며 반대로 다시 투입도 가능하다는 이야기이다. 이를 활용해보면 서비스 배포를 할 경우 위와 같은 설정이 되어있을때 제외 &gt; 배포 &gt; 투입하는 식으로 서비스가 무중단 상태에서 배포가 될수 있는 효과를 얻을 수 있다.설치는 별도로 하지 않아도 되고 mod_jk 모듈 내에 있기 때문에 별도의 설정만 추가해주면 된다. apache/conf/httpd.conf 1234567891011121314&lt;IfModule jk_module&gt; JkWorkersFile conf/workers.properties JkLogFile logs/mod_jk.log JkLogLevel info JkMount /* load_balancer &lt;Location /jkmanager/&gt; JkMount jkstatus Order deny,allow Deny from all Allow from 127.0.0.1 Allow from &#123;접근 가능한 IP&#125; &lt;/Location&gt;&lt;/IfModule&gt; apache/conf/workers.properties 12worker.list=jkstatusworker.jkstatus.type=status 설정에서 볼 수 있듯이 해당 설정은 다른측면에서는 상당히 취약점이 많은 부분이다. 해당 설정이 외부에 노출이 되어있다면 그 컨트롤을 서버 관리자가 아닌 다른 누군가가 할수 있기 때문에 꼭 Allow 설정으로 접근 제한을 해둬야 한다. 이렇게 하고 서버 IP/jkmanager/ 을 접속해보면 “JK Status Manager” 이라는 문구와 함께 아파치에 연동된 톰켓의 상태를 한눈에 파악할 수 있다. 여기서 tomcat1 좌측에 있는 E(=edit)를 클릭하고 Activation 값을 “Disabled” 으로 바꿔본 뒤 앞서 테스트한 방법을 다시 해보면 tomcat1 에는 엑세스가 들어오지 않고 9번 엑세스가 골고루 tomca2 와 tomcat3 으로 로드밸런싱이 된것을 확인할 수 있다. # 마치며각 설정값들은 아무리 필자가 설명을 잘 해도 도큐먼트를 따라갈 수 없듯이 실제 각 도큐먼트를 보면서 설정값 하나하나를 조절해보며 운영하고 있는 서비스의 특징과 상황에 맞도록 맞춰가는것이 핵심일것 같다. (본 포스팅은 아주 가볍게 연동만 해보는 형태이고, 각 설정이나 워커들간의 우선순위 로드밸런싱 같은 경우는 직접 설정을 해가면서 확인이 필요하다. )사실 이부분은 머릿속으로는 어떻게 하는구나라고 알고만 있었는데 실제로 해보니 각 설정들이 어떤 의미이고 어떻게 조절하면 보다 더 좋은 성능이나 다양한 이득을 취할수 있을것 같다는 생각을 해본다. 참고 링크https://tomcat.apache.org/connectors-doc/reference/workers.htmlhttps://tomcat.apache.org/connectors-doc/common_howto/loadbalancers.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"tomcat","slug":"tomcat","permalink":"https://taetaetae.github.io/tags/tomcat/"},{"name":"Load Balance","slug":"Load-Balance","permalink":"https://taetaetae.github.io/tags/Load-Balance/"}]},{"title":"스프링을 활용한 대용량 파일 업로드 구현","slug":"spring-file-upload","date":"2019-07-21T13:09:58.000Z","updated":"2020-04-23T04:41:36.876Z","comments":true,"path":"2019/07/21/spring-file-upload/","link":"","permalink":"https://taetaetae.github.io/2019/07/21/spring-file-upload/","excerpt":"개발을 하다보면 실제로 직접 구현을 해본적은 없지만 여기저기서 들어본 지식과 그 동안의 짬밥(?)으로 추측해볼수 있는 부분들이 있다. 물론 모든일에 정답은 없겠지만 요즘 느끼는건 책에서 공부만 해본것과 다른 블로그들에서 눈으로만 보고 넘어가는것들 그리고 직접 손가락을 움직여가며 왜 여기서는 이 방법을 사용하지 고민하면서 구현을 해본다는건 정말 엄청나게 큰 차이가 있는것 같다.","text":"개발을 하다보면 실제로 직접 구현을 해본적은 없지만 여기저기서 들어본 지식과 그 동안의 짬밥(?)으로 추측해볼수 있는 부분들이 있다. 물론 모든일에 정답은 없겠지만 요즘 느끼는건 책에서 공부만 해본것과 다른 블로그들에서 눈으로만 보고 넘어가는것들 그리고 직접 손가락을 움직여가며 왜 여기서는 이 방법을 사용하지 고민하면서 구현을 해본다는건 정말 엄청나게 큰 차이가 있는것 같다. 웹 어플리케이션을 개발하다보면 한번 쯤 만나게 되는 파일 업로드 기능. 필자도 몇번 구현은 해봤지만 그냥 단순히 구현만 해본 상태였다가 최근에 그냥 파일 업로드가 아닌 대용량 파일 업로드에서의 문제가 발생하여 여기저기 삽질을 하게 되었고 정리도 해볼겸 스프링에서의 대용량 파일 업로드시 한번쯤 고려해봐야 할 부분에 대해 정리를 해보려고 한다. 물론 구글에서 검색을 해보면 아마 필자가 쓴것 보다 더 자세하고 좋은 글들이 있겠지만 필자는 보다 대용량에 집중에서 작성해 보고자 한다. 명심하자. “아무리 흐린 잉크라도 좋은 기억력보다 낫다” 라는 말이 있듯이 # 스프링을 활용한 파일 업로드 구현우선 완전 초기상태에서 시작하기 위해 스프링 부트 프로젝트를 만들고 간단하게 파일 업로드를 할 수 있는 form 페이지와 업로드 버튼을 눌렀을때 작동하게 되는 컨트롤러를 만들어 보자.12345678910111213141516171819202122232425262728293031323334353637import java.io.File;import java.io.IOException;import java.io.InputStream;import org.apache.commons.io.FileUtils;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.multipart.MultipartFile;import lombok.extern.slf4j.Slf4j;@Slf4j@Controllerpublic class FileUploadController &#123; // 너무 간단 ... @RequestMapping(\"/form\") public String form() &#123; return \"form\"; &#125; @RequestMapping(value = \"/upload\", method = RequestMethod.POST) public String upload(@RequestParam(\"file\") MultipartFile multipartFile) &#123; log.info(\"### upload\"); File targetFile = new File(\"/home1/irteam/\" + multipartFile.getOriginalFilename()); try &#123; InputStream fileStream = multipartFile.getInputStream(); FileUtils.copyInputStreamToFile(fileStream, targetFile); &#125; catch (IOException e) &#123; FileUtils.deleteQuietly(targetFile); e.printStackTrace(); &#125; return \"redirect:/form\"; &#125;&#125; upload 요청이 들어오면 file이라는 이름의 파라미터로 MultipartFile을 받고 파일의 이름을 확인 후 스트림을 읽어 특정 경로에 파일로 저장하는 로직이다. 그다음 /form을 접속하게 되면 나오는 폼 화면을 만들자. 이것도 아주 심플하게!12345&lt;h1&gt;파일 업로드&lt;/h1&gt;&lt;form action=\"/upload\" method=\"post\" enctype=\"multipart/form-data\"&gt; &lt;input type=\"file\" value=\"파일 선택\" name=\"file\"/&gt; &lt;input type=\"submit\" value=\"업로드\"/&gt;&lt;/form&gt; multipart/form-data 라는 Content-Type 을 명시해주고 파일을 선택하면 /upload로 POST요청을 하도록 설정한다. 이렇게 되면 너무 간단하게 + 이상없이 파일이 업로드가 잘 되니 이게 이야기 할 꺼리인가(?) 싶을정도로 심플하다. # 그런데 파일 크기가 크다면?설마 파일 업로드 하는 용량이 크겠어?... 왠지 파일의 용량이 크면 문제있을것 같은데... 출처 : https://m.blog.naver.com/naibbo0407/30170815180설마 파일 업로드 하는 용량이 크겠어?... 왠지 파일의 용량이 크면 문제있을것 같은데... 출처 : https://m.blog.naver.com/naibbo0407/30170815180 개발을 하다보면 항상 생각해야 할 부분중에 하나가 바로 확장성인것 같다. 이 부분에서 역시 문제가 되었던 것. 평소보다 용량이 큰 파일이 업로드가 되면서 (평소 3~400MB 였다가 3~4GB정도의 파일이 업로드가 되는 매직) 업로드가 안되는 상황이 발생하였다. 당연히 문제가 발생하면 누군가 말했듯 로그부터 살펴보았는데 Apache - (AJP) - Tomcat 으로 구성된 환경에서 tomcat 로그에 ### upload라는 로그가 없고 아파치 로그엔 502 에러가 발생한 것이었다. 왜 톰켓 로그도 안남고 그전에 에러가 발생하였을까?이때부터 (근거없는 추측을 하며…) 고난과 역경의 삽질을 하기 시작하게 된다. 톰켓 버전이 문제일까? 로그가 안찍혔다면 다른 필터나 인터셉터에서 무언가를 먹고(?)있는건 아닐까? 잠깐, 근데 원래 대용량 업로드가 되긴 해? 파일 업로드/다운로드 하는 사이트 보면 별도 프로그램으로 하던데… 꼬불꼬불 미로속을 헤메는것만 같았던 삽질의 문제는 결국 메모리에 있었다.파일을 업로드 하게 되면 해당 내용을 우선 메모리에 담게 되고 다 담은 후 메모리에 있는 내용을 was에 전달한 뒤 HttpServletRequest 로 넘어오게 된다.(Apache &gt; Tomcat) 그런데 파일을 업로드 하면서 메모리에 파일이 써지다가 메모리 부족으로 OOM이 발생하게 되버린 것이었다. 또한 스프링 파일 최대크기를 별도로 지정하지 않고 있었기 때문에 메모리가 충분했다 하더라도 에러가 발생했을 상황이었다. ( https://spring.io/guides/gs/uploading-files/ 참조 )12spring.servlet.multipart.max-file-sizespring.servlet.multipart.max-request-size 그러다보니 웹 서버인 아파치에서는 was 에러인 503이 아닌 502라고 에러를 발생하던 것이였고 지나고 보면 정말 아무것도 아닌 간단한 설정들을 놓친 문제였는데 꽤나 긴 시간을 허비해야만 했던 안타깝지만 보람찼던 (응?) 트러블 슈팅이었다. # 삽질은 곧 경험이 되고 시야가 된다.legacy 로직이다보니 was가 파일 업로드 처리를 하게 되었는데 가급적이면 was가 처리하는것 보다는 static 파일을 처리할 수 있는 별도의 웹서버를 만드는게 어떨까 생각이 든다. (조금 알아보니 nodejs 모듈인 multer 라는게 있다.) 물론 파일 업로드 한 뒤에 별도의 로직을 처리하려면 was가 관여를 해야겠지만 이 부분은 설계를 어떻게 하냐에 따라 충분히 해결할 수 있을것으로 보인다. (웹서버에서 파일을 업로드 한 뒤 비동기로 파일 업로드 완료여부에 따라 was에서 처리를 한다거나 등…)더불어 항상 어플리케이션을 만들때에는 예외처리라는 것을 생각하면서 개발해야한다고 느끼게 되었다. NPE 같은 사소한 로직에서의 예외처리부터 파일 업로드시 서버의 메모리를 생각할수 있는 시야. 이런게 경험이 아닐까 싶다.또한 (잘 돌아가니까) 환경설정 값을 수정하지 않고 배포하는 것보단 가급적 어떤 설정값들에 의해서 어플리케이션이 돌아가는지 특히, 스프링 같은 프레임워크의 도움을 받는다면 해당 프레임워크의 설정값들을 수정하며 성능에 이득을 취할 부분들은 없는지 꼼꼼하게 개발하는 습관을 길러야 할 것 같다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"fileupload","slug":"fileupload","permalink":"https://taetaetae.github.io/tags/fileupload/"}]},{"title":"2019 상반기 리뷰 (feat. 글또)","slug":"review-first-half-2019","date":"2019-07-07T08:52:20.000Z","updated":"2020-04-23T04:41:36.843Z","comments":true,"path":"2019/07/07/review-first-half-2019/","link":"","permalink":"https://taetaetae.github.io/2019/07/07/review-first-half-2019/","excerpt":"누구나 어렸을 땐 빨리 어른이 되고 싶어 하는 것 같다. 시간이 빨리 지나가길 바라고, 빨리 어른이 되고 싶다는 간절함이 있지만 이상하게도 그땐 시간이 천천히 가는 것처럼 느껴졌다. 반면, 시간이 천천히 갔으면 하는 때가 있다. 딱 지금.","text":"누구나 어렸을 땐 빨리 어른이 되고 싶어 하는 것 같다. 시간이 빨리 지나가길 바라고, 빨리 어른이 되고 싶다는 간절함이 있지만 이상하게도 그땐 시간이 천천히 가는 것처럼 느껴졌다. 반면, 시간이 천천히 갔으면 하는 때가 있다. 딱 지금. 남들은 워어어어얼화아아수우우모옥금퇼 이라고 부르며 시간이 느리게 간다고 빨리 주말이 왔으면 좋겠다고 하지만 요즘의 필자는 정 반대다. 방금 출근한 것 같은데 어느샌가 퇴근인사를 주고받고 있다. 무언가에 홀린 것 같다. 벌써 올해도 절반이 지나가고 뜨거운 여름과 함께 후반전이 시작되었다. 그래서 빨리 지나갔나...출처 : https://m.blog.naver.com/kong6482/220584667861그래서 빨리 지나갔나...출처 : https://m.blog.naver.com/kong6482/220584667861 이제까지는 12월 말 즈음에 한 해를 바라보고 리뷰를 했었는데 글또라는 글쓰기 모임에 가입을 하게 되어 상반기 리뷰를 해보려 한다. 글또 모임의 첫 숙제가 상반기 리뷰 포스팅이다. 사실 리뷰를 상반기에 하던 연 말에 한 해 기준으로 하던 정해진 건 없지만 나를 다시 바라보고 다잡는 시간이 많을수록 보다 더 앞으로 가는데 힘이 될 거라는 데에는 이견이 없다. # 회사 속에서의 나회사에서는 회사일이 최우선!출처 : https://m.blog.naver.com/hwee__/221191852972회사에서는 회사일이 최우선!출처 : https://m.blog.naver.com/hwee__/221191852972 최근에 팀장님과 면담 중에 나온 이야기다. 신기하게도 군 시절 장기를 꿈꾸던 필자를 어서 전역하라고 권유하시던 대대장님께 매일같이 들었던 이야기와 비슷하다. “이제는 단순 개발만 하고 기능구현만 하는 것이 아니라 그 이상을 해야 할 시기가 다가온다.” “사람들 관리가 될 수도 있고 어느 한 분야에 전문가가 되어야 할 수도 있고, 선택은 본인의 몫” 사실 기능 구현이야 누구나 다 할 수 있다. 단지 경험에 따른 구현의 속도나 안정성의 차이가 아닐까 생각해본다. 그렇다면 그 이상은 어떻게 해야 할까? 정답은 없겠지만 필자는 그 이상을 해보려 우선 팀에 도움이 되기 위해 여러 가지 자동화 툴 들을 만든 것 같다. 보다 기능 개발에 집중하고 단순 반복적인 업무는 시스템이 할 수 있도록. 그렇게 툴들을 만들어 가며 생각하지 못한 부분들을 배우게 되고 나중에 그걸 또 사용하게 되는, 미래의 나를 위해 강제로 배우고 있는듯한 느낌이랄까. 아, 물론 회사 본연의 업무가 최우선이지만 말이다.어쨌든 시킨 일은 우선 차질 없이 잘 하고 시키지도 않은 일을 찾아서 하려고 노력했던 것 같다. 팀을 위해서, 곧 나를 위해서.적어도 회사에서 있는 시간 속에서는 다른 곳에 한눈 안 팔고 회사 업무에 전념하려고 노력했던 것 같다. # 외부 활동부족한 시간을 쪼개면서 밋업이나 세미나에 참여하곤 했었다. 그리고 마냥 듣고만 오진 않았고 “행사에 참여하면 무조건 질문 하나는 하자”라는 나와의 약속을 지키며 정리한 내용을 블로그에 포스팅하기도 하였다. 올해 첫 발표!올해 첫 발표! 디자이너와 개발자가 함께하는 투게더톤을 진행하기도 했었다. 투게더톤은 약 한 달 동안 진행되는 해커톤으로 하루 또는 무박 2일 동안 하는 기존 해커톤과 다르다. 이 기간 동안 팀 내에서 자유롭게 일정을 조정할 수 있다. 우리 팀은 약 7주에 걸쳐 “동네 마트 할인 정보를 알려주는 앱” 을 만들게 되었다. 필자는 API 전반에 대해 담당을 하였고 작은 부분이었지만 웹사이트도 간단하게 만들어 보았다. 아무것도 없는 백지상태에서 시작하려니 막막했지만 후기에서도 적었듯이 다시 해보라고 하면 머릿속에 전체 아키텍처가 그림으로 그려질 만큼 자신감이 생겼다. 특히 정말 좋은 팀원들과 함께 협업할 수 있어서 너무 좋았다. # 내공 연마한 달에 2개 이상 블로그 글을 작성하는 목표가 있었다. 그런데 지난달에 이사를 하다 보니 (핑계…) 목표를 달성 할 수가 없었다. 하지만 나름 퀄리티가 있는 글을 쓰려고 노력했고 PV도 작년보다 조금씩 오르고 있는 것 같아 내심 기분이 좋다. 그리고 작년 말부터 시작한 필자의 첫 토이프로젝트 인 기술블로그 구독서비스 에 이런저런 기능을 추가하였다. 설마 1000명이 넘게 구독 하겠어?라고 생각했지만 이 글을 작성하고 있는 시점에서 1,569명이나 구독했다. 설마 1년 넘게 내가 이 프로젝트를 운영하겠어?라고 생각했지만 다음 주가 되면 딱 1년째. 신기할 따름이다. 마침 기회가 되어 GDG 주관으로 행사하는 모두의 TOY STORY: SIDE PROJECT 어디까지 가봤니?라는 주제에 첫! 공식 발표자로써 발표를 할 수 있게 되어 너무나도 영광이다. 해당 발표 후기는 나중에 작성하는 것으로~ # 글또 3기 다짐글쓰는 또라이가 세상을 바꾼다 라는 페이스북 모임이 있다. (이번기수가 벌써 3기라고 한다 ㄷㄷ) 글또 라는 모임에 대해 간단히 정리를 해보면 다음과 같다. 일정의 예치금을 먼저 저장한다. 2주 간격으로 블로그 글을 작성 + 2명의 글에 대해 서로 리뷰 PASS권은 2회 (다른사람들의 글에 리뷰를 많이 해주면 PASS 권을 부여) 글을 올리지 않았거나, 리뷰를 하지 않았을 경우 예치금에서 일부 삭감 마지막 날까지 진행하고 예치금을 다시 돌려받는 형식 올해부터 블로그 포스팅을 좀 더 많이 하자고도 했고, 단순 횟수만 늘리는 것이 아닌 글쓰기에 대해서도 연습을 하고자 했는데 마침 딱 원하는 모임이 있어 시작을 하게 되었다.이 모임에 참여하기 위해 무작정 2주마다 한 개의 글을 쓰지 않을 것이다. 배운 것을 기록하고 정리하는 습관을 기르기 위해 글을 쓸 것이다. 또, 양질의 글을 작성하기 위해 글쓰기 책들을 읽어야겠다. # 마치며우선 다음 주에 있을 발표 준비에 최선을 다하고, 여력이 되면 새로운 프로젝트를 시작하고 싶다. 회사에서는 회사일 열심히 하고 회사 밖에서는 나만의 인사이트를 찾기 위한 여정에 지치지 않도록 체력이며 정신력이며 갈고닦아야 할 것 같다. 운동도 다시 시작을 해야 할 텐데 . . . ㅠㅠ","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"},{"name":"글또","slug":"글또","permalink":"https://taetaetae.github.io/tags/글또/"}]},{"title":"Spring에서 Request를 우아하게 로깅하기","slug":"controller-common-logging","date":"2019-06-30T09:39:47.000Z","updated":"2020-04-23T04:41:36.700Z","comments":true,"path":"2019/06/30/controller-common-logging/","link":"","permalink":"https://taetaetae.github.io/2019/06/30/controller-common-logging/","excerpt":"스프링 기반의 웹 어플리케이션을 만들다 보면 요청을 처리하는데 맨 처음에 위치하고 있는 Controller(이하 컨트롤러)라는 레이어를 만들게 된다. 그럴때면 사용자가 어떤 요청(Request)을 하였는지에 대해 확인이 필요할 수 있다.","text":"스프링 기반의 웹 어플리케이션을 만들다 보면 요청을 처리하는데 맨 처음에 위치하고 있는 Controller(이하 컨트롤러)라는 레이어를 만들게 된다. 그럴때면 사용자가 어떤 요청(Request)을 하였는지에 대해 확인이 필요할 수 있다. 물론 확인을 안해도 무방하지만 가급적 로깅은 시스템 로직에 영향을 주지 않는 범위에서 최대한 다양하게 미리 해두는게 나중에 유지보수시 편할 수 있다. (예전 조직장님께서 말씀하신게 아직도 머릿속에 꽉 자리잡고 있다…)아~주 일반적으로, 컨트롤러에서는 다음과 같이 메소드 단위로 파라미터를 직접 로깅하게 된다.12345678910@Slf4j@RestControllerpublic class SampleController &#123; @GetMapping(\"/test1\") public String test1(@RequestParam String id) &#123; log.info(\"id : &#123;&#125;\", id); return \"length : \" + id.length(); &#125;&#125; 이렇게 되면 사용자가 GET /test1 이라는 요청을 보낼때 어떤 파라미터로 호출하였는지에 대해 로깅이 남게 되는데 항상 log.info(&quot;id : {}&quot;, id); 과 같이 수동으로 로깅을 남겨야 하는 불편함이 생긴다. 물론 꼼꼼하게 메소드마다 로깅을 적어주면 전혀 문제될게 없지만 이러한 컨트롤러 ~ 메소드가 한두개가 아닌 수십 또는 수백개일 경우엔 그때마다 로깅을 적어줘야 하는 불편함이 있을 수 있다. 또한 자칫 깜박하고 로깅을 빼먹고 배포를 하게 된 경우 모니터링시 로깅을 하지 않아서 다시 로깅하고 배포를 하는, 별것도 아닌데(?) “정말 불편한” 상황이 있을 수 있다.이번 포스팅에서는 사용자의 요청을 모니터링 하기 위해 컨트롤러마다 코드를 작성해가며 로깅을 하는것이 아니라 HttpServletRequestWrapper 라는 것과 Filter, AOP를 이용하여 Request의 정보를 한곳에서 우아하게 로깅하는 방법에 대해 알아보고자 한다. # 요구사항와 개발하자아!출처 : https://gfycat.com/ko/brightevilaoudad와 개발하자아!출처 : https://gfycat.com/ko/brightevilaoudad 투우사가 흔드는 빨간 천을 보며 돌진하는 황소처럼 (쓰고보니 너무 TMI 같다….) 당장 코딩을 시작하며 개발을 할 수도 있지만 정작 원하는 기능이 무엇인지 천천히 정리하고 넘어갈 필요가 있는 것 같다. (어쩔땐 오히려 후자가 더 빠른 개발을 하게 되는것 같다.) GET, POST 등 다양한 http method 로 구현된 모든 컨트롤러의 파라미터와 기타 Request 정보가 로깅이 되야 한다. 컨트롤러, 메소드가 늘어날때마다 별도의 코드 추가 없이 한곳에서 공통적으로 로깅이 되야 한다. URL 중 특정 패턴으로 들어오는 요청은 다른 방식으로 로깅을 하거나, 로깅에서 제외할 수 있어야 한다. 앞서 말했듯 다른 비지니스 로직에 영향을 주지 않아야 한다. # 구현하기 - Request 의 파라미터 정리Request 의 모든 로깅을 한곳에서 처리하기 위해서 filter(필터)를 활용하였다. 필터는 Dispatcher servlet의 앞단에 위치하고 있기 때문에 모든 정보를 확인할 수 있는데 용이하다. 물론 인터셉터를 활용해서도 방법이 있겠지만 본 포스팅 에서는 필터를 활용해서 구현하는것을 목적으로 한다. (사실 인터셉터로 몇번 시도해보다가 실패해서…유유 ) Spring MVC Request Life Cycle출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/Spring MVC Request Life Cycle출처 : https://justforchangesake.wordpress.com/2014/05/07/spring-mvc-request-life-cycle/ Filter를 만들기 전에 Filter에서 사용할 주요 핵심(?) 클래스가 필요한데 HttpServletRequest 를 Wrapping 해서 사용하기 위해 HttpServletRequestWrapper를 상속받는 클래스를 만들자. Request 에 담겨있는 param 과 body로 요청이 들어올 경우 body에 있는 내용을 param 에 담는 로직이다. 주요 설명은 코드 안에서 주석으로 설명하겠다.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115public class ReadableRequestWrapper extends HttpServletRequestWrapper &#123; // 상속 private final Charset encoding; private byte[] rawData; private Map&lt;String, String[]&gt; params = new HashMap&lt;&gt;(); public ReadableRequestWrapper(HttpServletRequest request) &#123; super(request); this.params.putAll(request.getParameterMap()); // 원래의 파라미터를 저장 String charEncoding = request.getCharacterEncoding(); // 인코딩 설정 this.encoding = StringUtils.isBlank(charEncoding) ? StandardCharsets.UTF_8 : Charset.forName(charEncoding); try &#123; InputStream is = request.getInputStream(); this.rawData = IOUtils.toByteArray(is); // InputStream 을 별도로 저장한 다음 getReader() 에서 새 스트림으로 생성 // body 파싱 String collect = this.getReader().lines().collect(Collectors.joining(System.lineSeparator())); if (StringUtils.isEmpty(collect)) &#123; // body 가 없을경우 로깅 제외 return; &#125; if (request.getContentType() != null &amp;&amp; request.getContentType().contains( ContentType.MULTIPART_FORM_DATA.getMimeType())) &#123; // 파일 업로드시 로깅제외 return; &#125; JSONParser jsonParser = new JSONParser(); Object parse = jsonParser.parse(collect); if (parse instanceof JSONArray) &#123; JSONArray jsonArray = (JSONArray)jsonParser.parse(collect); setParameter(\"requestBody\", jsonArray.toJSONString()); &#125; else &#123; JSONObject jsonObject = (JSONObject)jsonParser.parse(collect); Iterator iterator = jsonObject.keySet().iterator(); while (iterator.hasNext()) &#123; String key = (String)iterator.next(); setParameter(key, jsonObject.get(key).toString().replace(\"\\\"\", \"\\\\\\\"\")); &#125; &#125; &#125; catch (Exception e) &#123; log.error(\"ReadableRequestWrapper init error\", e); &#125; &#125; @Override public String getParameter(String name) &#123; String[] paramArray = getParameterValues(name); if (paramArray != null &amp;&amp; paramArray.length &gt; 0) &#123; return paramArray[0]; &#125; else &#123; return null; &#125; &#125; @Override public Map&lt;String, String[]&gt; getParameterMap() &#123; return Collections.unmodifiableMap(params); &#125; @Override public Enumeration&lt;String&gt; getParameterNames() &#123; return Collections.enumeration(params.keySet()); &#125; @Override public String[] getParameterValues(String name) &#123; String[] result = null; String[] dummyParamValue = params.get(name); if (dummyParamValue != null) &#123; result = new String[dummyParamValue.length]; System.arraycopy(dummyParamValue, 0, result, 0, dummyParamValue.length); &#125; return result; &#125; public void setParameter(String name, String value) &#123; String[] param = &#123;value&#125;; setParameter(name, param); &#125; public void setParameter(String name, String[] values) &#123; params.put(name, values); &#125; @Override public ServletInputStream getInputStream() &#123; final ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(this.rawData); return new ServletInputStream() &#123; @Override public boolean isFinished() &#123; return false; &#125; @Override public boolean isReady() &#123; return false; &#125; @Override public void setReadListener(ReadListener readListener) &#123; // Do nothing &#125; public int read() &#123; return byteArrayInputStream.read(); &#125; &#125;; &#125; @Override public BufferedReader getReader() &#123; return new BufferedReader(new InputStreamReader(this.getInputStream(), this.encoding)); &#125;&#125; 가장 중요한 부분은 IOUtils.toByteArray(is) 요 부분인데, InputStream은 한번밖에 읽을 수 없기 때문에 이 필터에서 스트림을 읽는 대신, 래퍼 구현으로 새 스트림 생성하도록 작업을 하였다. 자칫 잘못하다간 body의 내용이 유실될 수도 있기 때문이다.참조 :http://stackoverflow.com/questions/10210645/http-servlet-request-lose-params-from-post-body-after-read-it-oncehttp://stackoverflow.com/questions/3769259/why-is-the-parameter-value-an-object-hash-code-for-request-getparametermap-ge 위에서 만든 래퍼 클래스를 이제 필터에서 적용해보자.123456789101112131415161718public class ReadableRequestWrapperFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) &#123; // Do nothing &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; ReadableRequestWrapper wrapper = new ReadableRequestWrapper((HttpServletRequest)request); chain.doFilter(wrapper, response); &#125; @Override public void destroy() &#123; // Do nothing &#125;&#125; 이렇게 되면 param으로 넘어온 파라미터나 body로 넘어온 정보들이 파싱되어 param에 담기게 된다.(JSON으로 파싱하는 부분은 조금 지저분해 보일 수 있지만 다양한 테스트를 해보면서 얻은 결과물이다. 더 좋은 방식이 있다면 언제든지 PullRequest 또는 댓글 환영!) # 구현하기 - 컨트롤러에 AOP 셋팅이제 Request 의 params 를 한곳에서 로깅할 차례이다. 물론 위에서 만든 필터에서 로깅을 해도 되지만 필터에 로직이 들어가는 것보다 별도의 로직에서 처리하는게 맞다고 생각되어 분리를 하였다. (관심사의 분리)AOP를 활용하여 다음과 같이 로직을 작성하였다. 아래 로직은 이해하기에 큰 어려움은 없을것 같지만 주석으로 설명을 하겠다.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Component@Aspect@Slf4jpublic class LoggerAspect &#123; @Pointcut(\"execution(* com.taetaetae..*Controller.*(..))\") // 이런 패턴이 실행될 경우 수행 public void loggerPointCut() &#123; &#125; @Around(\"loggerPointCut()\") public Object methodLogger(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; try &#123; Object result = proceedingJoinPoint.proceed(); HttpServletRequest request = ((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest(); // request 정보를 가져온다. String controllerName = proceedingJoinPoint.getSignature().getDeclaringType().getSimpleName(); String methodName = proceedingJoinPoint.getSignature().getName(); Map&lt;String, Object&gt; params = new HashMap&lt;&gt;(); try &#123; params.put(\"controller\", controllerName); params.put(\"method\", methodName); params.put(\"params\", getParams(request)); params.put(\"log_time\", new Date()); params.put(\"request_uri\", request.getRequestURI()); params.put(\"http_method\", request.getMethod()); &#125; catch (Exception e) &#123; log.error(\"LoggerAspect error\", e); &#125; log.info(\"params : &#123;&#125;\", params); // param에 담긴 정보들을 한번에 로깅한다. return result; &#125; catch (Throwable throwable) &#123; throw throwable; &#125; &#125; /** * request 에 담긴 정보를 JSONObject 형태로 반환한다. * @param request * @return */ private static JSONObject getParams(HttpServletRequest request) &#123; JSONObject jsonObject = new JSONObject(); Enumeration&lt;String&gt; params = request.getParameterNames(); while (params.hasMoreElements()) &#123; String param = params.nextElement(); String replaceParam = param.replaceAll(\"\\\\.\", \"-\"); jsonObject.put(replaceParam, request.getParameter(param)); &#125; return jsonObject; &#125;&#125; 위 코드에서는 단순히 console log를 찍었지만 필자가 운영하고 있는 어드민 툴에서는 Request 정보를 Elasticsearch 에 인덱싱하고 이를 키바나에서 보다 쉽게 조회 및 모니터링이 가능하도록 구현하였다. 로깅을 어떤식으로 남길지, 로깅이 아닌 특정 상황에서 알림을 보낸다거나 등등 이 부분은 실제 구현시에 상황에 맞춰서 만들면 될 것 같다.또한 Pointcut 부분을 다르게 활용하여 위에서 이야기한 요구사항의 3번 (URL 중 특정 패턴으로 들어오는 요청은 다른 방식으로 로깅을 하거나, 로깅에서 제외할 수 있어야 한다.) 를 해결할 수 있다.아참, AOP의 개념을 좀더 알고 싶다면 jojoldu 님의 블로그 포스팅을 추천한다. https://jojoldu.tistory.com/71 그래서 아래처럼 테스트를 해보면 GET 이던 POST 이던 로깅이 되는것을 확인할수 있다.더 이쁘게 로깅을 하는건 자유!더 이쁘게 로깅을 하는건 자유! # 마치며사실 이 로직을 만들게 된 계기는 팀원중에 한분이 “GET 의 파라미터는 로깅이 되는데 POST 의 파라미터는 로깅이 안되요” 라는 말에 투우사 앞에 있는 황소마냥 찾아보다 만들게 되었다. 참고로 이 부분은 하나의 무기가 될것 같아 github에 공개를 하였고 위에서 적었던 요구사항보다 더 좋은 내용이 있다면 언제든지 PullRequest 를 날려주기를 바란다. Spring Boot 환경에서 Filter를 추가하며 구성하였다.https://github.com/taetaetae/request_logging필터와 인터셉터, AOP 등 평소 자주 사용하지 않은 (이미 누군가 다 만들어 둔) 부분을 만지면서 다시한번 Spring의 주요 개념을 숙지하는데 도움이 되었던 경험으로 남을 것 같다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"logging","slug":"logging","permalink":"https://taetaetae.github.io/tags/logging/"},{"name":"HttpServletRequestWrapper","slug":"HttpServletRequestWrapper","permalink":"https://taetaetae.github.io/tags/HttpServletRequestWrapper/"},{"name":"Filter","slug":"Filter","permalink":"https://taetaetae.github.io/tags/Filter/"},{"name":"AOP","slug":"AOP","permalink":"https://taetaetae.github.io/tags/AOP/"}]},{"title":"D.light 투게더톤 참가후기","slug":"d-light-togetherthon-2019","date":"2019-05-19T13:46:03.000Z","updated":"2020-04-23T04:41:36.722Z","comments":true,"path":"2019/05/19/d-light-togetherthon-2019/","link":"","permalink":"https://taetaetae.github.io/2019/05/19/d-light-togetherthon-2019/","excerpt":"회사일을 하다 보면 시키는 대로 혹은 팀의 목표에 부합하기 위해 어쩔 수 없이 해야 하는 일을 하게 된다. 그러한 일이 재미있고 결과물에 대한 만족도가 100% 라면 다행이지만 간혹 재미도 없고 시켜서 하는 일은 밤을 꼬박 새 가면서 완성을 해도 썩 그렇게 만족스럽지 못한 경우가 대부분인 것 같다.","text":"회사일을 하다 보면 시키는 대로 혹은 팀의 목표에 부합하기 위해 어쩔 수 없이 해야 하는 일을 하게 된다. 그러한 일이 재미있고 결과물에 대한 만족도가 100% 라면 다행이지만 간혹 재미도 없고 시켜서 하는 일은 밤을 꼬박 새 가면서 완성을 해도 썩 그렇게 만족스럽지 못한 경우가 대부분인 것 같다. (물론 회사일에서 자신만의 인사이트를 찾는다면 금상첨화겠지만… + 매번 회사일이 재미없고 하기 싫은건 아님)언제부터인지 필자도 이러한 부분에 갈증을 느끼며 회사와는 별도로 무언가를 만들어 보고 싶은 마음이 무럭무럭 생겨날 즈음 facebook 타임라인에서 개발자와 디자이너가 약 7주간 프로젝트를 진행하는 D.light 투게더톤 이라는 행사가 있다는 것을 발견하고 나름 정성스레 지원서를 작성 후 합격 메일을 받게 된다. (GDG Facebook 해당 게시글)이번 포스팅에서는 해커톤과는 살짝 성격이 다른 D.light 투게더톤을 진행하면서 느꼈던 부분들과 진행한 결과물에 대해 간략히 리뷰를 해보며 정말 급행처럼 지나간 약 7주간을 돌이켜 보는 시간을 갖고자 한다. # 팀 빌딩눈도 못마주칠 정도로 어색한 첫날Team. 그팽눈도 못마주칠 정도로 어색한 첫날Team. 그팽 총 6개 팀 중에 필자는 여자 디자이너 두 분, 남자 안드로이드 개발자 두 분을 포함한 팀에 속하게 되었다. 5명 중 해커톤 참여 경험이 있다는 이유만으로 여자 디자이너 분께서 팀장이 되시고, 7주라는 시간이 정말 급하게 지나갈 것 같다는 억지(?) 이유를 들먹여 그팽이라는 팀 이름이 정해졌다. 그렇게 “우리가 정말 무엇을 만들 수 있을까?” 하는 의구심 속에 프로젝트가 시작이 되었다. # 프로젝트 진행 전반신기하게도 우리 5명은 각각 사는 지역이 전부 달랐다. (심지어 한 분은 매주 저 멀리 충청남도 천안에서 올라오셔야 하는 수고를 ㅠㅠ) 매 주말마다 오프라인으로 만나서 회의를 진행했다. 그래야 길다면 길고 짧다면 짧은 7주 안에 완성도 높은 결과물을 만들 수 있을 것 같아서였다. 프로젝트의 주제를 정하는 아이디어 회의에서 정해진 우리의 목표는 “동네 마트 할인 정보를 알려주는 앱”을 만들기로 하였다.시간가는줄 몰랐던 아이데이션 회의시간가는줄 몰랐던 아이데이션 회의 팀워크가 중요한 투게더톤 임에도 불구하고 여느 천재 디자이너, 천재 개발자처럼 일당백 스타일로 뚝딱 만드는 그런 프로젝트의 진행 방식은 피하려고 우리 모두가 노력하였다. 되도록이면 이렇게 모인 다섯 명이 한마음 한뜻으로 각자가 생각하는 크기와 양은 다르겠지만 이 프로젝트를 통해 무엇이라도 배울 수 있었으면 했다. 디자이너 분들은 서로 디자인하신 시안에 대해 공유를 하면서 개선해 나가는 모습과, 안드로이드 개발자 두분은 (거의 매일) 밤마다 서로 슬랙에서 개발 방법론에 대해 스터디를 하는 모습이 보기 너무 보기 좋았다. 물론 필자도 아무것도 없는 환경에서 백엔드 서버를 구축하고 API를 만드는 과정 속에서 정말 많은것을 배울 수 있었다.그렇게 시간이 흘러 마지막 발표하는 전날엔 팀원 몇 분과 함께 꼬박 밤을 새우며 프로젝트 결과물의 완성도를 높이는데 노력하였고 필자 개인적으로 아주 성공적으로 프로젝트를 마무리할 수 있었다. # 개발 진행안드로이드 개발자분들은 코틀린 기반으로 개발을 하였다. 여러 디자인 패턴과 다양한 기술들을 사용하였다고 들었는데 필자는 아쉽게도 백엔드 개발을 하다 보니 전부를 이해하지는 못하였다.예전에 토이 프로젝트를 파이썬 기반으로 해본 경험이 있어서 Flask 또는 Django 기반으로 API 서버를 구축해볼까 하고 고민하였다. 하지만 (Spring Boot 기반으로도 해보고 싶었고) 파이썬보다는 자바 기반으로 다양한 어플리케이션의 요구 사항을 개발하는데 조금 더 능숙할 것 같아서 Spring Boot 기반으로 개발 환경을 구성하였다.서버는 AWS 프리티어의 EC2를 발급받고 DB 또한 AWS에서 제공해주는 RDS(mysql)을 발급받아 구성하였다. 그리고 DNS는 예전에 무료 도메인을 찾다가 알게 된 http://mooo.com/ 라는 서비스에서 발급받아 연결하였고, 프로젝트 기능 중에 서버에서 앱으로 푸시를 하는 기능이 있었는데 Firebase를 활용해서 구성할 수 있었다.사용한 기술들사용한 기술들 Entity Relationship Diagram (ERD) 는 무료로 인터넷에서 사용할 수 있는 툴이 있는지 찾다보니 http://aquerytool.com/ 라는 서비스가 있었고 (무려 한글…) 약간 부족하였지만 테이블들을 구성하고 연결관계를 표현하기에는 정말 간단하고 쉽게 사용할 수 있었다.테이블 관계도. PK, FK 등 완벽하진 않다.테이블 관계도. PK, FK 등 완벽하진 않다. 개발환경을 구성하면서 어떤 부분들은 보기만 해보고 사용만 해봤지 실제로 구성을 해보지는 않아서 정말 막막한 부분도 있었고 특히 내가 이것 하나 못하는가 하며 자괴감이 들때도 있었지만 그러면서 많은것을 배울 수 있었고 다시 해보라고 하면 머릿속에 전체 아키텍쳐가 그림으로 그려질 만큼 자신감이 조금이라도 생겨서 다행이라고 생각한다. # 프로젝트 결과http://magarine.mooo.com우리가 만든 서비스는 마트 가격 내린다 는 의미에서 마가린 이라는 이름으로 만들었고 그에 걸맞는 앱 아이콘이 만들어졌다. 필자는 이 아이콘을 볼때마다 심쿵거릴정도로 너무 이쁜것 같다.마가린이 무려 천막을 달고 마트가 되었는데 할인이 되었기에 잘려 나가는 아트웤(?)을 표현해주셨다. 키햐... 무슨말이 필요하는가마가린이 무려 천막을 달고 마트가 되었는데 할인이 되었기에 잘려 나가는 아트웤(?)을 표현해주셨다. 키햐... 무슨말이 필요하는가 정말 간단하게 말하자면 일종의 쇼핑몰을 만들었다. 물건을 사고자 하는 사용자는 손쉽게 위치기반으로 근처 동네 마트의 할인 상품을 한눈에 확인 가능하고, 물건을 파는 마트 주인은 간단하고 편하게 홍보를 하는 서비스이다. 아쉽게도 물건을 파는 마트 주인을 위한 앱은 시간관계상 못 만들었지만 추후 기회가 된다면, 그리고 팀원들과 논의후 만들어 볼 계획이다.프로젝트 소개 페이지에서 적어 놓은 것 처럼 여러가지 기능을 구현하였다. 그것도 완벽하게. 단골마트 : 주변의 마트 또는 슈퍼를 지도에서 찾아 단골마트로 지정하고 마트의 전단지를 보며 행사를 확인할 수 있음 상품검색 : 단골마트에서 팔고있는 상품들을 검색이 가능 장보기 메모 : 상품 상세 페이지에서 구매할 수량을 선택하고 장보기 메모에 담아서 나중에 구매 또는 취소를 할 수 있음 장보기 메모 내역 : 이제까지 구매했던 이력을 확인할 수 있고 마치 가계부처럼 자신의 소비 습관을 해당 메뉴에서 분석할 수 있음 할인 알림 설정 : 원하는 상품이 있다면 알림설정을 할 수 있고 판매자가 상품을 등록하는 순간 푸시메세지로 정보를 알려줌 아래는 발표 자료. 매년마다 어떤 행사에서라도 꼭 발표를 한 번 이상 하자는 나와의 약속을 이번 D.light 투게더톤에서 달성할 수 있어서 좋았다. 물론 이번 발표때도 여유라곤 찾을 수 없었지만… # 마치며결국 여러 심사조건(?)으로 6개 팀중 상위 2팀에게 주는 sketch 라이센스를 받게 되었고 (필자피셜) 만족할만한 마무리가 될 수 있었다. 우리 다섯명 팀원들의 캐릭터(By ZEPETO), 최종발표 장소인 구글캠퍼스, sketch 라이센스우리 다섯명 팀원들의 캐릭터(By ZEPETO), 최종발표 장소인 구글캠퍼스, sketch 라이센스 발표에서도 몇번을 반복하며 이야기 했지만 이런 팀원들을 또 만날 수 있을까 하는 생각과 함께 다시한번 팀원들에게 감사하다는 말을 전하고 싶다. 그리고 아무것도 정해지지 않는 상황과 아무것도 구축되어 있지 않은 개발환경에서 나름 구현하고자 했던 기능을 구현하며 멋지게 마무리 할수 있어 다행이라 생각하고 다양한 트러블슈팅 속에서 나름 많은것을 배울 수 있었다.머릿속에 있는 것과 그것을 말로 설명할 수 있는 것, 나아가 그것을 직접 내 손으로 해보고 문제를 해결해 나가는 것은 엄청난 차이가 있다고 생각한다. 그러한 과정들 속에서, 그리고 이렇게 다양한 사람들과의 협업을 통해 배우는 것 또한 분명히 있고 그것을 오롯히 내것으로 만드는게 마지막으로 남겨진 숙제인것 같다.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://taetaetae.github.io/tags/hackathon/"},{"name":"gdg","slug":"gdg","permalink":"https://taetaetae.github.io/tags/gdg/"}]},{"title":"자바, 성능, 모니터링 테크세미나 정리 및 후기 (by 우아한 형제들)","slug":"got-of-java-seminar","date":"2019-05-12T11:04:01.000Z","updated":"2020-04-23T04:41:36.752Z","comments":true,"path":"2019/05/12/got-of-java-seminar/","link":"","permalink":"https://taetaetae.github.io/2019/05/12/got-of-java-seminar/","excerpt":"실무에서 자바 기반으로 개발을 하고 서비스를 운영을 하다보면 처음엔 아무런 문제가 없다가 사용자가 몰리는 등 이벤트성으로 트래픽이 많아질 경우 꼭 문제가 생기기 마련이다. 그럴때면 뒤늦게 부랴부랴 원인을 찾고 개선하기 바빠지게 된다.","text":"실무에서 자바 기반으로 개발을 하고 서비스를 운영을 하다보면 처음엔 아무런 문제가 없다가 사용자가 몰리는 등 이벤트성으로 트래픽이 많아질 경우 꼭 문제가 생기기 마련이다. 그럴때면 뒤늦게 부랴부랴 원인을 찾고 개선하기 바빠지게 된다. (아마 윗분들에게 혼나면서?ㅠㅠ)평소에 이런 성능문제를 개선하고 미리 모니터링 할수있는 부분에 대해 관심을 갖고 있었던 찰나, 우아한 형제들에서 5월 우아한 테크 세미나를 한다기에 부랴부랴 장문의 글로 신청을 하였고 운이 좋아 당첨이 되었다.한창 회사에서 새로운 서비스 출시, 그리고 잠을 줄여가며 별도로 진행하고 있던 토이프로젝트 등 여러가지로 바쁜 시기였지만 특히 예전부터 뵙고싶던 이상민님께서 직접 강의를 해주신다기에 피곤한 심신을 이끌고 세미나에 참석하였고 그 후기를 적어보고자 한다. 두레이로 만드신 발표자료를 공유해 주셨지만 저작권 문제도 있고 해서 필자기준에서 이해한 부분에 대해서만 공유하고자 한다. 더불어 그냥 듣고 앵무새처럼 발표내용 그대로를 공유하는건 의미가 없다고 생각되어… 포스터만 봐도 벌써부터 가슴이 뛴다(?).포스터만 봐도 벌써부터 가슴이 뛴다(?). # 성능구글에서 작성한 성능이 중요한 이유 라는 아티클을 공유해 주셨다. (시간이 된다면 한번 읽어보길 강추, 무려 한글!) 어플리케이션에서 성능은 사용자의 증가, 이탈율, 응답속도에 영향이 있고 이는 결국 추구하는 가치(이를 테면 수익)에 직면한다고 한다.사용자는 어느 관점에서 바라보는가에 따라 달라지고 각 관점에 따라 성능을 챙겨야 하는 부분이 달라진다. 수강신청을 하는 시점에서의 사용자와 뉴스 페이지를 읽는 시점에서의 사용자는 각 성격이 엄연히 다른것처럼. 시스템 관리자 등록된 / 등록되지 않은 사용자 서버 관점 로그인된 / 로그인 하지 않은 사용자 성능 테스터 관점 Active User 서버에 부하를 주는 사용자 메뉴나 링크를 누르고 결과가 나오기를 기다리는 사용자 성능테스트시 Vuser와 거의 동일 ( Vuser : 가상사용자(virtual user) ) Concurrent user 서버에 부하를 주고 있거나, 줄 가능성이 매우높은 서비스에 접속중인 사용자 웹 페이지를 띄워놓은 사용자 TPS(Transaction Per Seconds)는 초당 얼마나 많은 요청을 처리할수 있는지에 대한 시스템의 절대적인 수치로 볼수있다. (개발자는 어느상황에서든지 대충 감으로 이야기 하지말고 정확한 수치로 이야기 해야한다는 뼈를 때리는 조언과 함께…) TPS는 Scale out/up을 통해 증가시킬수 있지만 Response Time 은 불가능하다. 물론 어플리케이션을 튜닝하면 두 수치 모두 개선이 가능하다. 이러한 TPS와 Response Time의 최대치는 출시전에 반드시 테스트를 통해 알고 있어야 이슈발생시 대응하는데 유용하다.Bottleneck 즉 병목은 장비, 어플리케이션, 저장소, 설정 등 다양한 상황에서 발생할수 있다. 그중에 “아주 일반적”으로 가장 병목이 많이 발생하는 구간은 DB이고 그 다음으로 클라이언트(Web page, App), Network이 있을 수 있다.결론은 Performance engineering is “Composite Art” of IT 라는 하나의 문장으로 정리를 해주셨다. 아무리 이쁜 디자인과 어렵고 복잡한 기능이 있을지라도 성능이 뒷받침 안된다면 대용량 트래픽 상황에서는 무의미해지기 때문이라고 생각한다. # 자바자바의 역사에 대해 설명해 주셨다. ( 역사에 대한 보다 자세한 설명은 https://www.whatap.io/blog/12/ 참고 ) 언제부터인가 JDK 라이센스 이슈가 많았었는데 실무에서 개발하는 입장에서는 java 8 에서는 문제가 안되고 java 11부터 라이센스 문제가 복잡하게 생길수 있다고 한다. 이부분은 공식문서(?)를 찾아보는게 좋을듯 하다. (개인 또는 회사에서 사용할 경우 상황에 따라 법적 이슈가 생길수도, 안생길수도 있는 복잡한 문제가 있어보여서… 필자도 제대로 이해하지는 못했다ㅠ) 그리고 각 자바 버전에서 발표한 새로운 기능에 대해 설명해주셨다. Java 8 lambda, stream, default method, LocalDate / LocalTime 추가 stream 과 foreach 의 성능은 거의 차이 없음 (오히려 가독성이 나빠질수도 있다.) ParallelStream 은 해당 장비의 cpu 개수만큼 스레드 풀을 만들어 사용 (오히려 독이 될수 있으니 잘 알아보고 사용할것) Java 9 Compact Strings : char[] &gt; byte[] G1 default GC : https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html Collections of (불변) : List.of, Set.of, Map.of Java 10 var 의 등장 Application Class-Data Sharing(AppCDS) Java 11 Oracle JDK의 유료화 Http Client. 기본 설정값들을 제대로 알고 써야한다. ( https://golb.hplar.ch/2019/01/java-11-http-client.html ) Java 12 Switch expressions Shenandoah : https://www.youtube.com/watch?v=E1M3hNlhQCg # 모니터링유명한 상용 APM들을 설명해 주셨다. 각각의 장점에 대해 설명해 주셨는데 정말 회사에 요청해 구매할수만 있다면 사서 해보고 싶을정도로 신기한 기능이 많았다. 그중 dynatrace 는 에이전트만 설치해두면 별도의 설정 필요없이 알아서 해준다고… dynatrace (https://www.dynatrace.com/) new relic (https://newrelic.com/) AppDynamics (https://www.appdynamics.com/) WhaTap (https://www.whatap.io/) 오픈소스로는 스카우터와 핀포인트를 설명해 주셨다. 필자는 핀포인트로 회사 서비스를 모니터링 중에 있는데 스카우터에도 좋은 기능이 많아 보여 기회가 된다면 개발서버에 설치해서 핀포인트와 각각 장단점을 비교해 보고 싶어질 정도로 스카우터 자랑을 엄청 해주셨다. (NHN에서는 스카우터로 모니터링 하고 있다고 하니 더욱더 관심이 가게 되었다.) scouter (https://github.com/scouter-project/scouter) pinpoint (https://github.com/naver/pinpoint) APM 즉, Application Performance Management의 핵심은 바로 java.lang.instrument package 와 Java ClassFileTransformer 에 있다고 하셨다. 마치 Spring의 AOP처럼. 이번 세션의 결론은 처음에 이야기 하신 부분과 비슷한 “절대로 단정짓지 마라 ! 데이터로 이야기 하자 !” 라는 문장으로 정리를 해주셨다. 그만큼 테스트를 많이 해보고 평소에 모니터링을 자주 해가며 서비스의 안정성을 높여야 한다는 뜻으로 이해했다.끝으로 Q&amp;A가 있어 평소에 궁금했던 질문을 드렸고 너무 친절하게 화이트보드에 그래프를 그려주시면서 (원래 강의시간보다 30분정도 더 하게 만든 장본인…ㅠㅠ) 답변을 해주셨다. Q. Application의 상태를 확인하기 위해 각종 모니터링 툴을 활용하는데, 오히려 모니터링이 과하다 보면 Application 성능에 영향을 주게 된다. 어떻게 해야하는가? A. 모니터링툴을 홍보하는 쪽에서는 당연히 성능에 영향이 없다고 한다. 하지만 먼저 개발서버에서 테스트를 해봐서 모니터링툴이 있고 없고의 서비 리소스의 차이를 확인해보고 조금씩 적용범위를 늘려가는 식으로 해보는것도 하나의 방법이 될 수 있다. 또한 샘플링을 통해 일부분만 확인하는 방법도 있다. (필자가 이용하는 pinpoint는 request의 20% 이런식으로 샘플링을 하고 있었는데 scouter에서는 response time기준으로 샘플링이 되나보다?ㄷㄷ) 너무 두서없이 적었나...너무 두서없이 적었나... 그리고 필자의 질문 때문이였는지 실무에서 있었던 장애시 그래프 사례를 보여주시며 끔찍한(?) 상황까지 재밌게 표현해 주시며 약 3시간여 진행된 세미나가 마무리 되었다. # 마치며자바로 Application개발을 하면서 성능과 모니터링은 마치 삼겹살엔 소주, 치킨에 맥주처럼 정말 떼려야 뗄 수 없는 사이인것 같다. 우아한 형제들에서 주최한 이번 기술 세미나는 필자에게 정말 많은것을 배우게 해준 좋은 행사였다. 그리고 회사에 가면 짬나는 시간을 활용해서 스카우터로 성능테스트를 해볼 계획이다. (라고 말하면 안되고 점심에 졸려서 성능테스트를 해봤다고 말해야 직장 상사가 좋아하신다 라고 말씀해주셨다 ㅎㅎ) 질문을 해야 내것이 된다는 나와의 약속을 이번에도 지킬수 있었다! (질문하고 받은 배달의 민족 쿠폰!)질문을 해야 내것이 된다는 나와의 약속을 이번에도 지킬수 있었다! (질문하고 받은 배달의 민족 쿠폰!) 1,2 회 모두 탈락해서 못들었지만 다음에도 이런 기술관련 행사가 있으면 꼭 듣고 싶고 마지막으로 필자에 질문에 너무 성실하게 답변해 주시고 재밌고 귀에 쏙쏙 들어오는 강연을 해주신 이상민님께 이 포스팅으로나마 다시한번 감사의 말씀을 전하고 싶다.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"performance","slug":"performance","permalink":"https://taetaetae.github.io/tags/performance/"},{"name":"monitoring","slug":"monitoring","permalink":"https://taetaetae.github.io/tags/monitoring/"}]},{"title":"spring-boot에서 mybatis로 mysql 연동하기","slug":"spring-boot-mybatis-mysql-xml","date":"2019-04-21T13:47:04.000Z","updated":"2020-04-23T04:41:36.873Z","comments":true,"path":"2019/04/21/spring-boot-mybatis-mysql-xml/","link":"","permalink":"https://taetaetae.github.io/2019/04/21/spring-boot-mybatis-mysql-xml/","excerpt":"실무에서 개발을 하다보면 과거 누군가 잘 구성해 놓은 밥상(legacy)에 숟가락만 얹는 느낌으로 로직 구현만 할때가 있다. 그러다보면 각종 레이어가 어떻게 구성(설정)되어있는지도 모르고","text":"실무에서 개발을 하다보면 과거 누군가 잘 구성해 놓은 밥상(legacy)에 숟가락만 얹는 느낌으로 로직 구현만 할때가 있다. 그러다보면 각종 레이어가 어떻게 구성(설정)되어있는지도 모르고 간혹 설정에서 문제가 발생하면 “아 내가 이것도 모르고 이제까지 개발을 해왔나” 하는 자괴감이 들며 몇시간을 삽질하는 경우가 있다. 그게 지금의 필자인것 같다. (눙물…)출처 : http://blog.naver.com/PostView.nhn?blogId=ondo_h&logNo=221437452142출처 : http://blog.naver.com/PostView.nhn?blogId=ondo_h&logNo=221437452142 사이드 프로젝트 초기셋팅을 하며 호기롭게 spring boot 최신버전에서 db를 연동하려 했는데 막상 완전 바닥부터 해본 경험이 적다보니 (spring boot 2 버전에서는 더욱더…) 어디서부터 뭘 설정을 해야할지… 그리고 이럴때 보는 도큐먼트를 봐도 잘 이해가 안되어 삽질을 해가며 당황하기 일쑤였다.이번 포스팅에서는 아래와 같은 구성을 하는데 목표를 두고자 한다. Spring Boot 2 프로젝트를 처음 만들고 mybatis 를 사용해서 mysql 을 연동하는것 (AWS 의 RDS를 사용, 추후 RDS사용법에 대해 블로깅 예정) 위와 같은 상황을 처음 접하는 분들께 도움이 되었으면 하는 바램으로 짧게나마 필자의 삽질기를 여행해보자. # Spring boot 2 프로젝트 만들기필자는 IntelliJ를 사용하고 있어서 새로 프로젝트를 만들려고 할때 클릭 몇번만으로 dependency 설정까지 다 해주기 때문에 편하고 좋았다. 혹 이클립스나 다른 IDE를 사용하고 있다면 https://start.spring.io/ 을 참고하면 도움이 될것같다. 여기서도 클릭 몇번으로 IntelliJ 에서 해주는 것처럼 내가 사용할 모듈을 선택하고 generate 를 누르면 프로젝트가 생성되어 다운로드 받아진다. (참 좋은 세상…)우선 File → New → Project 를 눌러서 아래 창을 열어보자. 그리고 뭔가 다 해줄것 같은 (개발도 해주면 안되나…) Spring Initializr을 선택후 아래와 같은 설정을 적어준 뒤 다음을 눌러준다. 사용할 모듈을 선택해주자. 필자는 이것저것(?)을 도와주는 lombok과 Mybatis, MySQL을 선택하고 프로젝트를 생성하였다. 그러면 이쁜(?) pom.xml 과 함께 당장 개발을 시작할 수 있는 환경이 제공된다.12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 우선 여기까지 잘 되었는제 확인해보기 위해 Controller 에 현재시간을 출력하는걸 만들어 보고12345678@RestControllerpublic class ApiController &#123; @GetMapping(path = \"/helloWorld\") public String helloWorld() &#123; return LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE_TIME); &#125;&#125; 톰켓을 실행해보면 정상적으로 접속과 출력이 되는것을 확인할 수 있다. # MySQL 연동하기필자가 허둥지둥 했던점 중 하나는 MyBatis와 MySQL을 동시에 연동하려고 하다보니 문제가 발생해도 어디서의 문제인지를 제대로 파악하지 못하고 삽질했다는 점이다. 여기서 정확히 짚고 넘어가면 우선 데이터를 연결해주는 ORM인 MyBatis를 셋팅해준 다음 MySQL을 연동해주는 식으로 분리해서 설정을 하면 햇갈리지 않고 (돌아가지 않고) 보다 빠르게 설정이 가능할것 같다. (여기서 순서는 중요하지 않고 별도로 설정해야 한다는 관점이 중요한것 같다.)우선 src/main/resources폴더에 있는 application.properties 에 다음처럼 작성해주자.1234spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.hikari.jdbc-url=jdbc:mysql://&#123;url&#125;:&#123;port&#125;/&#123;db&#125;spring.datasource.hikari.username=&#123;id&#125;spring.datasource.hikari.password=&#123;password&#125; 위의 jdbc-url 항목에서 AWS에서 제공하는 RDS를 사용하는 경우 RDS에서 제공해주는 엔드포인트와 포트를 적어주면 된다. (추후 AWS - RDS에 대해 블로깅 예정이다.)Spring Boot 2.0 이후부터 기본적으로 사용되는 커넥션 풀이 HikariCP로 변경되었다고 한다. (링크) 커넥션 풀 종류중 성능이 좋다고 하는데 링크를 가보면 다른 커넥션 풀 라이브러리와 성능을 비교한 벤치마크 결과를 확인할 수 있다.위처럼 spring.datasource.hikari 가 prefix로 붙고 각종 정보들을 적어주어 config 에서 인식될수 있도록 해주자. 그 다음 DataSource 설정을 해준다.1234567891011121314151617@Slf4j@Configuration@PropertySource(\"classpath:/application.properties\")public class DatabaseConfiguration &#123; @Bean @ConfigurationProperties(prefix = \"spring.datasource.hikari\") public HikariConfig hikariConfig() &#123; return new HikariConfig(); &#125; @Bean public DataSource dataSource() &#123; DataSource dataSource = new HikariDataSource(hikariConfig()); log.info(\"datasource : &#123;&#125;\", dataSource); return dataSource; &#125;&#125; 위 내용은 DataSource 를 hikariConfig에서 설정한 정보로 만들어 준다는 의미이다. 이렇게만 하고 프로젝트를 다시 실행시켜보면 logger 에 의해 datasource 의 정보를 볼수가 있다.12342019-04-22 00:27:35.048 INFO 23040 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-04-22 00:27:36.221 INFO 23040 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.2019-04-22 00:27:36.222 INFO 23040 --- [ main] c.e.m.config.DatabaseConfiguration : datasource : HikariDataSource (HikariPool-1)2019-04-22 00:27:36.527 INFO 23040 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService &apos;applicationTaskExecutor&apos; 여기까지 우선 Datasource 설정이 끝났다.Q : com.mysql.cj.jdbc.Driver 에서 cj가 뭐지?A : 해당 클래스는 더이상 사용하지 않아 com.mysql.jdbc.Driver로 설정하고 실행시켜보면 아래 문구를 볼수가 있다. Loading class `com.mysql.jdbc.Driver’. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver’. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary. 당황하지 말고 클래스를 바꿔주자. # MyBatis 연동하기DB를 연동했으니 이제 쿼리를 작성하고 원하는 결과를 얻기위해 MyBatis를 활용할 차례다. 위에서 작성한 DatabaseConfiguration에 추가로 다음과 같이 작성해주자.1234567891011121314151617public class DatabaseConfiguration &#123; @Autowired private ApplicationContext applicationContext; @Bean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource); sqlSessionFactoryBean.setMapperLocations(applicationContext.getResources(\"classpath:/mapper/**/*.xml\")); return sqlSessionFactoryBean.getObject(); &#125; @Bean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 이 설정은 위에서 설정한 datasource를 사용하고 쿼리가 작성되는 xml위치를 지정해 줌으로써 추후 Mapper or DAO 레벨에서 사용되는 쿼리를 인식해주는 과정이다. 여기서 classpath는 src/main/resourcs이고 해당 쿼리가 있는 xml 위치는 본인의 취향대로 위치키시고 그에 맞도록 설정해주면 된다.이렇게 한뒤 MySQL Workbench 로 DB에 접속후 임의의 데이터를 생성한 다음DAO 를 만들어 주고 이를 호출해보면 정상적으로 데이터를 읽어오는것이 확인된다. DAO 1234567891011121314151617package com.express.magarine.api;import org.apache.ibatis.session.SqlSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;@Repositorypublic class ApiDao &#123; protected static final String NAMESPACE = \"com.express.magarine.api.\"; @Autowired private SqlSession sqlSession; public String selectName()&#123; return sqlSession.selectOne(NAMESPACE + \"selectName\"); &#125;&#125; query xml 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.express.magarine.api\"&gt; &lt;select id=\"selectName\" resultType=\"string\"&gt; SELECT name FROM test LIMIT 1 &lt;/select&gt;&lt;/mapper&gt; Controller 1234567891011@Slf4j@RestControllerpublic class ApiController &#123; @Autowired private ApiDao apiDao; @GetMapping(path = \"/helloWorld\") public String helloWorld() &#123; return String.format(\"%s %s\", apiDao.selectName(), LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)); &#125;&#125; 결과 # 마치며 이 코드를, 그리고 이 포스팅을 작성하기 직전까지만 해도 “그냥 하면 되는거 아니야?”라고 생각했지만 알고있는 지식과 막상 해보는건 정말 하늘과 땅차이 라는걸 다시한번 느끼게 되었다. (자괴감의 연속…) 더불어 Spring Boot 의 간편함에 놀라웠고 이제 회사일이 조금 잠잠해졌으니 (과연?) Spring Boot로 이것저것 만들며 스터디를 해야겠다고 다짐해본다.참고 URL https://spring.io/guides/gs/accessing-data-mysql/ http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://taetaetae.github.io/tags/mybatis/"},{"name":"spring-boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"mysql","slug":"mysql","permalink":"https://taetaetae.github.io/tags/mysql/"}]},{"title":"AWS 프리티어 발급부터 EC2 접속까지","slug":"aws-freetier-create-and-ssh-access","date":"2019-04-14T08:39:03.000Z","updated":"2020-04-23T04:41:36.686Z","comments":true,"path":"2019/04/14/aws-freetier-create-and-ssh-access/","link":"","permalink":"https://taetaetae.github.io/2019/04/14/aws-freetier-create-and-ssh-access/","excerpt":"IT 쪽에 일을 하고 있거나 관심을 가지고 있는 사람이라면 한번쯤을 들어봤을 AWS(Amazon Web Services). 이름에서도 알수있는 것처럼 아마존에서 제공하는 각종 원격 컴퓨팅 웹서비스이다.","text":"IT 쪽에 일을 하고 있거나 관심을 가지고 있는 사람이라면 한번쯤을 들어봤을 AWS(Amazon Web Services). 이름에서도 알수있는 것처럼 아마존에서 제공하는 각종 원격 컴퓨팅 웹서비스이다. 아마존은 이러한 서비스를 누구나 쉽게 접근해볼수 있도록 AWS 프리티어를 제공해 주는데 이 프리티어 만으로도 과금없이 (또는 최소화 하여) 웹서비스를 구성할수 있다. 필자가 운영하고 있는 기술블로그 구독서비스또한 AWS 프리티어로 운영되고 있다.최근 GDG Seoul, P-typer, Sketch Seoul 에서 주최한 D.light 345 투게더톤에 참가하며 사이드 프로젝트를 하고 있는데 마침 AWS를 사용하게 되었다. 예전에 사용했을때는 장님 코끼리 만지듯이 설정을 했었는데 이번기회를 통해 다시한번 정리를 해본다.본 포스팅에서는 AWS 계정을 발급받고 신용카드 확인까지 된 계정에서 EC2 서버를 발급받고 putty를 활용하여 서버에 접근을 해보는것을 목표로 둔다. (사이드 프로젝트를 하면서) 아마도 웹서비스를 개발하면서 AWS를 활용하는 부분에 대해 시리즈물로 포스팅을 하게 될것 같다.사실 너무 간단해서 이런걸 글로 쓰나? 라고 할수도 있지만 눈으로만 보는것과 직접 해보는 것이 다르고, 이걸 다시 글로써 정리를 하는것 또한 완전 다른 부분이기 때문에 포스팅을 해본다. # EC2 생성하기EC2? Amazon Elastic Compute Cloud의 약자로 물리서버가 아닌 클라우드 서버를 제공하고 있다. EC2의 장점은 서버의 스펙을 쉽고 자유롭게 조정할 수 있는점이 가장 매력있게 생각한다. 우선 콘솔에 들어가 EC2를 검색후 접속을 하고 인스턴스 시작을 눌러서 인스턴스 생성 화면으로 들어간다. AMI 즉 생성할 이미지를 선택하는 부분인데 여기서 주의할점은 잘못선택 했다간 계정 만들었을때의 카드로 생각지도 못할 금액이 결제가 되버릴수도 있다. (실제로 필자도 AWS를 처음 만져볼때 아무생각없이 좋아보이는걸로 했다가 한 30달러 정도를 지불했어야만 했다…) 좌측에 보면 프리 티어만이라는 체크박스를 체크하고 자신이 원하는 이미지를 선택하자. 일반적인 리눅스 서버를 발급받고 싶기 때문에 빨간 영역의 이미지를 선택하고 선택한 이미지의 스팩을 다시한번 확인하자. (cpu 1개에 메모리도 1기가… 너무 짜지만 무료니까…) 마지막으로 시작하기 를 누르면 키 페어를 선택 또는 생성하도록 안내가 나오는데 당연히 아무것도 안한 상태라 새 키 페어 생성을 선택해 주고 이름을 지정한뒤 키 파일을 받아준다. 이 부분에서도 조심해야할 점이 키 페어를 한번 다운 받으면 다시 동일한 키 페어를 다운받을수가 없게 된다. (나중에 다시 발급을 받아야 하는 번거로운 문제가…) 다운을 받고 잊어버리지 않도록 잘 보관해두자. 키 페어를 다운 받으면 생성중이라는 메세지와 함께 결과화면이 나온다. 여기서도 중요한 부분! 프리티어라는 달콤한 키워드 때문에 들뜬 마음으로 성급하게 빨리 서버를 받아보고 싶다고 다음다음 신공을 하다보면 자칫 간과할수가 있는데 화면을 보면 결제 알림 생성이라는 다행스러운 기능이 있다. 별 어려운 설정이 아니니 꼭 설정을 해서 필자같이 기부(?)를 하는 일이 발생하지 않았으면 한다… EC2 인스턴스가 생성이 되었다. 인스턴스의 각종 정보를 확인할수가 있는데 public IP, public DNS 까지 제공되는것을 확인할 수 있다. (추후 DNS를 구입하게 되다면 이 IP에 연결을 시켜 도메인으로 해당 서버에 접속을 할수가 있게 된다.) # putty 로 발급받은 EC2 인스턴스에 접속을 해보자.이제 발급받은 EC2 인스턴스에 접속을 해볼 차례이다. 다양한 서버 접속툴이 있지만 필자는 putty를 가장 선호한다. 디자인은 구닥다리처럼 보일지 모르겠지만 개인적으로 직관적인 UI에 가벼운 프로그램이라 생각이 든다. 우선 putty를 다운 받고 putty.exe를 실행시킨뒤에 바로 ssh 접속을 하면 너무 간단하게 서버 접속에 성공을 할수 있지만 위에서 받은 키 페어 파일을 다시 private key 로 전환해야 하는데 putty를 다운받으면 동일한 폴더에 puttygen.exe라는 파일을 실행시켜주자.그다음 pem파일을 불러와서 마우스를 움직여서 게이지(?)를 다 채우고 save private key를 줄러 저장을 하는데 여기서 주의할점은 ppk파일명을 pem파일명과 동일하게 저장해야 한다는 것이다. (안그러면 서버 접속시 실패가 남… 삽질…) putty.exe를 실행시킨뒤 Connection &gt; SSH &gt; Auth 탭에서 방금 만들어 놓은 ppk파일을 불러오고, 다시 Session탭에서 host name 을 입력해주고 적당한 이름으로 저장을 눌러준다. 여기서 host name은 위에서 EC2 생성시 Amazon Linux AMI를 선택했기 때문에 사용자의 이름은 ec2-user가 되고 인스턴스의 정보중 public DNS와 함께 조합하여 다음과 같은 url을 적어준다.12ec2-user@&#123;public DNS&#125;e.g. ec2-user@ec2-###.compute.amazonaws.com 이렇게 하고 해당 세션을 더블클릭 또는 하단에 Open을 누르게 되면 해당 서버로 접속이 되는것을 확인할 수 있다.사실 기술을 배움에 있어 가장 훌륭한 도구는 제공되는 도큐먼트만한게 없다고 생각한다. 그에 필자의 블로그도 좋지만(?) 도큐먼트를 보면서 좀더 자세한 설명을 봐야 한다는 것을 강조하며 이번 포스팅을 마무리 해본다.※ putty로 AWS EC2 접속하기 : https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/putty.html # 마치며다양한 클라우드 서비스들중에 너무나도 신기할정도로 간편하게 클릭 몇번만으로 서버를 띄우고, 서버 접속없이 이또한 클릭 몇번만으로 어플리케이션을 운영할수도 있는 서비스들이 많다. 하지만 필자는 시스템 아키텍쳐를 구성할때엔 버튼 하나로 설치 및 셋팅되는 것보다 직접 설정을 건드려가며 소스로 설치하는 것을 선호한다. 그럼에 AWS의 EC2라는 서비스는 필자의 취향에 너무 알맞는 서비스라며 매력을 느끼고 있는 중이다.사이즈 프로젝트를 진행하면서 보다 다양한 AWS 프리티어 활용기를 포스팅 할 수 있을것 같아 벌써부터 설렌다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"aws","slug":"aws","permalink":"https://taetaetae.github.io/tags/aws/"},{"name":"ec2","slug":"ec2","permalink":"https://taetaetae.github.io/tags/ec2/"},{"name":"putty","slug":"putty","permalink":"https://taetaetae.github.io/tags/putty/"}]},{"title":"KafkaKRU(Kafka 한국사용자 모임) 밋업 후기","slug":"kafka-meetup-2019","date":"2019-03-30T16:49:30.000Z","updated":"2020-04-23T04:41:36.789Z","comments":true,"path":"2019/03/31/kafka-meetup-2019/","link":"","permalink":"https://taetaetae.github.io/2019/03/31/kafka-meetup-2019/","excerpt":"필자는 ElasticStack을 사용하면서 처음 카프카를 접하게 되었다. 메세징 큐 라는 개념도 전혀 모르는 상태에서 설치부터 ElasticStack 연동까지 사용하며 정말 강제로 카프카에 대해 공부를 하게 되었다. 카프카를 자주 다루고 메커니즘에 대해 자세히 살펴보다 잠깐 해이해질 무렵 카프카 한국 사용자 모임에서 밋업을 한다고 하길래 빛의 속도로 신청, 아마도 1등으로 신청했지 않았을까 싶다.","text":"필자는 ElasticStack을 사용하면서 처음 카프카를 접하게 되었다. 메세징 큐 라는 개념도 전혀 모르는 상태에서 설치부터 ElasticStack 연동까지 사용하며 정말 강제로 카프카에 대해 공부를 하게 되었다. 카프카를 자주 다루고 메커니즘에 대해 자세히 살펴보다 잠깐 해이해질 무렵 카프카 한국 사용자 모임에서 밋업을 한다고 하길래 빛의 속도로 신청, 아마도 1등으로 신청했지 않았을까 싶다.사실 작년 카프카 밋업을 못간게 너무 한(?)이 되어 이번엔 회사 업무 등 여러가지로 한창 바쁘지만 “지금이 아니면 안돼” 라는 생각으로 밋업을 다녀왔고, 짧지만 후기를 작성해 보고자 한다. (요즘 왜 이렇게 바쁜지 모르겠지만… 신기하게도 그 바쁜 일정들이 하나도 겹치지 않는게 더 신기하다… ) 삼성 SDS 건물에서 진행된 카프카 밋업삼성 SDS 건물에서 진행된 카프카 밋업 참고로 필자는 카프카에 대해 아주 조금 건드려본 수준이라 발표하시는 분들의 전부를 습득하기엔 다소 그릇이 작아서 일부 세션은 거의 “그런가보다~” 하고 들을 수 밖에 없었다. 후기도 아마 그런 맥락으로 작성할듯 싶다. Kafka 한국 사용자 모임 링크 : https://www.facebook.com/groups/kafka.kru # 카프카를 활용한 캐시 로그 처리 - 김현준(카카오) 이미지 등 캐시서버의 로그를 분석하기 위한 시스템을 구축하는데 ElasticStack 을 활용 Elasticsearch 로 늦게 들어와서 사례를 찾아보니 대용량 로깅 처리시 앞단에 메세징 큐를 둬야 한다고 했고 그게 카프카 카프카 모니터링은 그라파나로 활용 lag이 자꾸 생김 파티션을 쪼개거나, 컨슈머를 늘리는 방법이 있음 auto.commit.interval.ms 와 enable.auto.commit=true 로 조정 interval을 줄이니 lag이 줄어듬 현재는 수백대 캐시서버의 로그를 초당 15만건 이상 처리중 질문을 했다. 필자도 lag이 높아지면 어쩌지 하는 불안감과 높아지면 컨슈머를 늘리면 되겠지 하는 막연함이 있었는데 commit interval을 줄이면 lag이 줄어든다고 해서 무조건 줄이면 좋은가에 답변은 카프카를 관리하는 주키퍼쪽에 무리가 간다고 설명해 주셨다. 역시 만병통치약은 없고 상황에 따라 적절하게 시스템 관리자가 조정해가며 운영해야 하는점을 느꼈다. 참고 URL : https://kafka.apache.org/documentation/#adminclientconfigs # 카프카를 활용한 엘라스틱서치 실무프로젝트 소개 - 이은학(메가존) 카드사의 프로젝트를 약 3개월간 개발하였고 전체 아키텍쳐 중에 일부분을 kakfa를 활용 Elasticsearch 데이터를 hadoop에 백업 형태로 옮기며 관리 filebeat &gt; kafka &gt; spark streaming 을 활용하여 데이터의 검증처리가 가능 (특정 상황에서의 관리자에게 알림 등) logstash 의 ruby 필터를 활용하여 일정의 작업을 해주는 데이터 파이프라인 구성 가능 (개인정보 식별 등) logstash 는 cron형태의 배치로도 가능 또 질문을 하였다. (카프카 밋업과는 무관했지만…) logastsh 를 사용하면서 필터쪽에 로직이 들어가면 성능상 괜찮냐는 질문에 하루에 15억건을 처리하고있고 문제가 없었다고 한다. 필자는 아파치 엑세스 로그를 logstash로 처리하면서 간혹 뻗거나 에러가 발생했는데 아마 파일을 logstash가 직접 바라보고 처리도 하게해서 그런것 같다. (지금은 filebeat가 shipper 역활을 수행하고 있고 큰 무리 없이 운영중) # 카프카를 활용한 rabbitMQ 로그처리 - 정원빈 (카카오) 레빗엠큐는 erlang으로 구현된 AMQP 메시지 브로커이고 TCP기반으로 구성 Kafka 는 게으르지만 메우 효율성이 뛰어남, 반면 RabbitMQ 는 똑똑하지만 보다 느림 Kafka 에서 Elasticsearch 로의 ingset 는 NIFI를 활용 레빗엠큐와 카프카의 차이 Kafka RabbitMQ 컨슈머 추가 여러 컨슈머가 하나의 메세지를 동시에 할수 있어 확장에 용이함 확장할때마다 큐를 추가 생성해야함 메세지 저장 로그기반으로 디스크에 저장, 리텐션 이후 삭제 큐 기반으로 메모리에 저장 컨슈머가 메세지 수신시 즉시 삭제 메세지 처리 발송확인 가능 / 수신확인 불가능 발송확인/수신확인 가능 # 카프카를 마이크로서비스 아키텍쳐에 활용하기 - 이동진 (아파치 소프트웨어 파운데이션) 카프카 스트림즈 소개 (Interactive Query) 카프카를 활용하여 마이크로서비스에서 사용하려면 데이터를 임시 공간에 넣어두고 (redis 같은?) 빼서 사용하는 형태가 아니라 Interactive Query 또는 Queryable Store 로 활용 가능 사실 이부분은 필자가 제대로 못따라간 세션중에 하나이다. 용어나 메커니즘도 다소 생소했고 대략 어떤 부분을 발표해주시는지 느낌은 있었으나 제대로 이해를 못해서 … 부끄럽지만 카프카 스트림즈의 공식링크로 대체한다. https://kafka.apache.org/documentation/streams/ # 카프카 프로듀서 &amp; 컨슈머 - 강한구 (카카오 모빌리티) 프로듀서 메세지를 생산 및 전송 Accumulator : 사용자가 send한 record를 메모리 쌓는 역활 Network thread : 전송 각 옵션 활용법 (도큐먼트 문서로 대체) linger.ms max.request max.in.flight.requests.per.connection 브로커 메세지를 저장 topic name - partition 폴더 구조 세그먼트 단위로 저장 (*.index, *.log, *.timeindex) 컨슈머 Fetcher : 네트워크 스레드와 비슷한 역할 Coordinator : 어떤 토픽의 어떤 파티션을 comsume할지, 브로커의 그룹 코디네이터와 통신 (hearbeat, offset comit, consumer group join) # 마치며발표자 분들과 질문 두번에서 받은 책선물발표자 분들과 질문 두번에서 받은 책선물 확실히 수박 겉핥기 식으로 보다보니 지식에 대한 깊이도 얕아 발표자분이 전달하시고자 하는 내용을 100% 다 수용하기엔 힘들었다. 다음엔 가기전에 미리 밋업 발표에 대한 공부를 조금이라도 하고 들을 준비를 한 뒤에 참여하는것으로… 하지만 카프카를 활용해서 다양한 시스템 구성 방법론에 대해 간접으로라도 배울수 있었고, 현재 필자가 운영하고 있는 카프카의 설정값들을에 대해 잘 설정이 되어있나 (막연히 기본값들로만 설정되어 있지는 않은가) 살펴볼 계기가 만들어진것 같다. 이번에도 다행히 “행사에 참여하면 꼭 질문을 하나이상 하자!” 라는 나와의 약속을 지킬수 있어 다행이었다.","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://taetaetae.github.io/tags/kafka/"}]},{"title":"Write The Docs 서울 밋업 후기 (개발자 강추!)","slug":"write-the-docs-seoul-2019-review","date":"2019-03-24T12:43:14.000Z","updated":"2020-04-23T04:41:36.901Z","comments":true,"path":"2019/03/24/write-the-docs-seoul-2019-review/","link":"","permalink":"https://taetaetae.github.io/2019/03/24/write-the-docs-seoul-2019-review/","excerpt":"필자는 평소 개발자에게 가장 중요한 덕목 중 하나가 글쓰기라고 생각하고 있다. 마침 글쓰기와 기술의 접점을 고민하고 이야기하는 “Write The Docs 서울 밋업”(링크) 이 있다고 하여 쉬고 싶던 주말이지만 만사를 집어치우고 참석하게 되었다.","text":"필자는 평소 개발자에게 가장 중요한 덕목 중 하나가 글쓰기라고 생각하고 있다. 마침 글쓰기와 기술의 접점을 고민하고 이야기하는 “Write The Docs 서울 밋업”(링크) 이 있다고 하여 쉬고 싶던 주말이지만 만사를 집어치우고 참석하게 되었다. 사실 연예인 개발자분들을 직접 만날 수 있다는 기대감도 있었기 때문이다. (발표하시는 바로 앞자리에 앉았는데 정작 한마디도 못 건넸지만…) 밋업 가능길 문득 나를 사로잡았던 문구와 밋업 장소 마루 180밋업 가능길 문득 나를 사로잡았던 문구와 밋업 장소 마루 180 발표에 앞서 “이 발표 자료는 공개할 예정이니 필기하실 필요가 없다”라고 하셨다. 하지만 뒤통수를 (좋은 의미) 몇 대 아니 몇십대 맞은 느낌이라 정리를 하지 않을 수가 없었고 오늘 느끼고 배운 마음을 쭉 유지하고 싶어(내 것으로 만들고 싶어) 후기를 작성해 본다. 더불어 제목에 감히 개발자 강추!라고 적을만큼 최근 밋업 행사 중에 손꼽을 정도로 좋았기 때문이다. 이정도로 쌔게 맞은건 아니다...출처 : https://namu.moe/w/뒤통수이정도로 쌔게 맞은건 아니다...출처 : https://namu.moe/w/뒤통수 # 변성윤(소카) - 글쓰는 개발자 모임, 글또변성윤 님변성윤 님 필자도 가입만 하고 활동은 안 하는 중인 “글 쓰는 개발자 모임 - 글또” 모임에 대해 소개해주셨다. 글을 꾸준히 작성하기 위해 만들었고, 일정에 예치금을 내고 정해진 규칙에 의해 블로그에 글을 올리면 다시 돈을 환급받는 반강제적인 모임이라고 한다. 그뿐만 아니라 다른 분들이 글을 써서 공유를 하면 성윤님이 직접 피드백을 주며 개발 시 리팩토링을 하듯 더 나은 품질의 글을 쓸 수 있도록 도움을 주고 있다고 하신다. 이러한 피드백 문화가 1:N이 아닌 N:N이 되면 또 다른 동기부여가 될 것 같은데 … 하는 아쉬움을 느꼈다.사실 “글을 꾸준히 작성”하는 부분이 필자도 매우 공감이 된다. 바쁘고, 귀찮고, 글을 쓰려면 욕심이 생기고 그러다 미루고… 그 동기부여가 “돈” 일수밖에 없는 현실이 아쉽긴 한데 오히려 그 “돈”만큼 동기부여가 잘 되는 것도 없을것 같다. (헬스장 1년 권 계약하고 돈이 아까워서라도 나가는 느낌으로…)올해 새로운 기수를 모집한다고 하니 그때는 꼭 지원해서 글을 꾸준히 쓰는 습관을 길러보고 싶다. # 김대권(당근마켓) - 기술 블로그 생존 전략 : 구글 시대의 글쓰기김대권 님김대권 님 얼마 전에 한번 쓱 보고 정독할 수밖에 없던 포스팅인 좋은 기술 블로그를 만들어 나가기 위한 8가지 제언 을 작성하시고, 해당 기술블로그 를 운영하시고 계시는 김대권 님께서 글을 왜 쓰는지, 그리고 어떻게 하면 사람들에게 잘 읽힐 수 있을지에 대해 구글 검색엔진 관점에서 정리해주셨다.우리는 보통 읽히기 위해 공개된 글을 쓰기 때문에 좋은 글을 쓰는 게 선행되어야 하지만 반대로 어떻게 하면 잘 읽힐 수 있을지에 대해 고민이 필요한 부분 같다. 요즘은 소셜미디어나 검색을 통해 글이 공유되고 검색되는데 장기적으로 봤을 때는 검색엔진에 노출이 돼야 한다고 하신다. 또한 검색엔진은 백과사전처럼 정답을 알려주는것이 아닌 “거대한 추천 시스템”의 관점으로 접근해야 하며, 글의 양이 너무 크거나 적으면 안 되고 적당한(?) 수준을 지켜야 이를 검색엔진이 알아서 판단한다고 한다.또한 What nobody tells you about documentation (번역본) 이라는 것도 소개해주시며 결국엔 글 내용의 자체가 좋아야 한다고 재차 강조하셨다. (매우 공감, SEO 아무리 잘 설정해봤자 내용이 안 좋으면 말짱 꽝) # 홍연의(LINE) - To. 지식 공유를 시작하려는 개발자, From. 당신의 든든한 서포터 Developer Relations팀홍연의 님홍연의 님 다소 생소한 Developer Relations 팀에 대해 소개를 해주시며 꼭 기술 관점이 아닌 다양한 분야에서 해당 팀이 어떤 지원을 해주고 있는지에 대해 알려주셨다. 기술 블로그 운영, 소셜 페이지 관리, 개발 컨퍼런스, 세미나, 커뮤니티 후원 등등 개발자와 개발 문화를 알리는 모든 일을 하고 있다고 한다.옆 회사(?)이지만 저런 개발자의 문화를 만드는 팀이 있다는 게 부럽기도 하였고, 가끔 세미나가 있는 걸로 아는데 공개적으로 하면 어떨까 하는 아쉬움이 있지만… 점차 private에서 public으로 확대될 꺼라 기대를 해본다.발표를 내가 직접 들으며 이러한 문화를 만들 수도 있겠구나 하는 생각도 해봤다. 작게는 팀 단위부터 시작해서 서버/앱 등 개발자들을 모아두고 관심 있는 사람들끼리 공유하는 자리를 정기적으로 만드는… 중요한 건 “정기적”으로… 일단 나부터라도 시작을 해보자. # 조은별(시큐아이) - 사용자를 외면하지 않는 릴리스 노트조은별 님조은별 님 테크니컬 라이터가 무슨 일을 하고 어떤 부분에서 고민을 하는지에 대해 소개를 해주셨다. 하나의 예로 앱스토어에서 릴리즈 노트를 보면 A라는 앱은 단순 “기능 개선”, “버그 수정” 인데 B라는 앱은 개발과 무관한 일반 사용자가 보더라도 상세히 적힌 걸 볼 수 있다. 이것만 봐도 그 앱에 대한 신뢰가 높아질 수 있는 부분이라고 생각할 수 있다는 점에서 나는 commit message, PR 등 너무 의미 없는 메세지들로 일관한 건 아닐까 하는 반성을 할 수 있었다. (뜬금스럽지만…)프로야구의 더블플레이 룰이 올해부터 개정되는 것을 예로 들어주며 누가 읽고, 어떻게 읽으며 무엇을 읽는가에 대해 관점을 가지고 해당 사용자 시선에서 이해할 수 있도록 하는 게 가장 좋다고 설명해 주셨다. (이 분야 또한 리펙토링의 반복… ) # 이동욱(우아한형제들) - 개발자는 왜 블로그를 해야하나요?이동욱 님이동욱 님 기술블로그를 어떻게 써야 하고 어떤 식으로 관리를 해야 하는지가 아닌 조금 더 강한 느낌의 “개발자는 기술블로그를 해야 한다” 의 이유를 설명해주셨다. 동욱님은 블로그를 통해 이직도 하고 기고&amp;집필 요청도 받으시고 인터뷰 요청도 받고…심지어 광고수입으로 매월 70~100달러가 들어온다고 한다. (필자의 몇 배인지 가늠도 안 간다…)다양한 분야에서 얻은 이득이 많기 때문에 기술블로그를 해야 한다고 말하고 있고, 연봉/회사/직위/재산을 빼고 나를 표현할 수 있는 것이라고는 기술블로그밖에 없다고 한다. (극 공감) 필자도 서두에 말했던 것처럼 개발자는 글을 써야 한다고 하는 사람 중에 한 명이다 보니 동욱님의 발표 하나하나가 너무 몸 쪽 깊숙이 들어와서 글을 좀더 자주 + 잘 써야겠다고 다짐을 하게 되었다. 그리고 마지막에 말씀하신 중국 속담 하나가 아직까지 필자의 뒤통수를 계속 때리고 있다.아무리 흐린 잉크라도 좋은 기억력보다 낫다 # 변정훈(BlockchainOS) - 개발 관련 기술 블로그 운영하기변정훈 님변정훈 님 국내에 몇 안되는, 오랫동안 기술블로그를 운영해오시는 개발자 중에 한분인 아웃사이더 변정훈님께서 어떤 식으로 기술블로그를 운영해야 하는가에 대해 발표해주셨다. 필자와는 다르게 (워낙 많이 쓰셔서 일것 같지만) 퇴고는 잘 안 하시고 항상 글을 작성할 것을 생각하며 개인 노트에 메모하고 글을 쓴다고 하신다. (필자도 얼마 전부터 노션이라는 것을 활용해서 관리하고 있는데… 잘 따라 하고 있는 것 같아 나름 뿌듯함을 느꼈다.)이 세션에서도 뒤통수를 때리는 멘트가 많았는데… 괜히 유명하신 분이 아니구나 싶을 정도였다. (심지어 멘트마저…) 공부할 시간도 적은데 블로그는 또 언제 쓰는가 &gt; 공부할게 많으니까 블로그를 쓴다. (캬~ 1) 글을 지속적으로 쓰려면 어떻게 해야 하는가 &gt; 꾸준히 쓰다 보니 이제는 근육처럼 되었다. (캬~ 2) 문제가 생겨 검색해보고 해결한다고 해서 내 것이 되는 것은 아님 &gt; 내가 직접 재현을 해보고 테스트를 해봐야 내것이 됨. (캬~ 3) 나름의 철학으로 글을 작성할 때 일관된 흐름을 유지하려고 노력 중이시고 그게 구글에서 검색하면 아웃사이더님의 글이 처음으로 나오는 이유가 아닐까 싶다. (그만큼 사이트의 신뢰도가 높아져서?) # 마치며무슨 말이 필요하겠는가. 필자의 메모장에도 블로깅을 하려고 적어놓은 것들만 있지 실제로 실행에 옮기지 못하고 있는데 꾸준히, 그리고 체계적으로, 읽는 사람의 위치에서 글을 잘 써보겠다고 다짐할 수 있었던 좋은 행사였다. 한 가지, 밋업이 끝나고 네트워킹 행사나 뒷풀이가 있었으면 좋았을 텐데 하는 아쉬움이 있었지만 다른 행사에서 자주 찾아뵈고 하다 보면 인연이 생길 꺼라 감히 소망해본다. #wtdseoul #WritetheDocs","categories":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/categories/review/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/tags/blog/"},{"name":"write","slug":"write","permalink":"https://taetaetae.github.io/tags/write/"},{"name":"write the docs","slug":"write-the-docs","permalink":"https://taetaetae.github.io/tags/write-the-docs/"}]},{"title":"Jenkins 업그레이드 및 Master-Slave 구성","slug":"jenkins-upgrade-master-slave","date":"2019-03-17T09:23:03.000Z","updated":"2020-04-23T04:41:36.782Z","comments":true,"path":"2019/03/17/jenkins-upgrade-master-slave/","link":"","permalink":"https://taetaetae.github.io/2019/03/17/jenkins-upgrade-master-slave/","excerpt":"어떠한 작업(Job)이 있다고 가정해보자. 이를 “정해진 시간에 주기적” 이나 “필요할때” 작업을 수행하고 싶다면 어떤 툴(Tool)이 떠오르는가?","text":"어떠한 작업(Job)이 있다고 가정해보자. 이를 “정해진 시간에 주기적” 이나 “필요할때” 작업을 수행하고 싶다면 어떤 툴(Tool)이 떠오르는가? 그리고 이 작업(Job)들의 실행이력 등 전체적으로 관리하고 필요에 따라 다양한 플러그인을 활용하여 입맛에 맞는 작업(Job)으로 구성하고 싶을때 가장 첫번째로 떠오르는 툴은 바로 “Jenkins” 다. (극히 필자 개인적인 생각일수도 있지만… ) 물론 리눅스 기반의 crontab 이나 다른 스케쥴러를 활용할수도 있다. 다만 필자 개인적인 느낌으로 나만의 Jarvis(?)처럼 내가 원하는데로 설정만 해두면 정해진 시간에 수행하고 그 결과를 로그로 남겨놓고 문제가 발생했을때 알림도 받을수 있으니 너무 좋은 툴이라 생각이 든다. 실제로 Jarvis가 있다면 얼마나 편할까출처 : https://gfycat.com/ko/colossalsociablebuffalo실제로 Jarvis가 있다면 얼마나 편할까출처 : https://gfycat.com/ko/colossalsociablebuffalo 지난 포스팅에서는 Jenkins 를 설치하는 방법에 대해 알아보았다. (정확히 말하면 치트키 수준의… ) 이번 포스팅에서는 Jenkins에 노드를 추가하여 master-slave 분산환경으로 구성하는 방법과 Jenkins 버전을 업그레이드 하는 방법에 대해 정리해보고자 한다. 마침 필자의 팀에서 젠킨스를 분산환경으로 운영하고 있었는데 버전은 1.x … 간헐적으로 Jenkins 버전 이슈로 에러가 발생해서 업그레이드를 해야하는 상황이 생긴것이다. 시키지도 않은 일을 하면서 팀에 도움도 될겸, 포스팅도 할겸, 1석 2조 효과. 서버 환경은 CentOS 7.4 64Bit 에서 테스트 하였다. # Jenkins 버전 업그레이드 하기Jenkins를 업그레이드 하게되면 기존에 있었던 Jenkins의 환경설정은 어떻게 마이그레이션 할까? Job 실행기록들은 그냥 날려버려야 하나? 걱정을 하며 구글링을 해본다. 그러면 “안해본것에 대한 두려움” 을 갖는 필자의 마음이 무색할 정도로 너무 간단하게도 그냥 기존에 있던 war 파일을 최신버전으로 교체하고 재시작 하라고 나온다. 읭? 뭐이리 간단해? 대부분의 문제들은 지레 겁부터 먹고 실행에 옮기지 못해서 않아서 해결을 하지 못하는게 절반 이상같다. 자, 바로 실행에 옮겨보자.우선 버전 업그레이드를 테스트 하기 위해 일부러 낮은버전으로 설치를 해둔다. (필자는 1.609.1로 설치해봤다.) 그리고 버전 업그레이드 후 설정이 그대로 옮겨지는지를 확인하기위해 Security 설정을 해서 Jenkins 접근시 로그인 여부를 물어보록 설정해둔다. 우측 하단에 빨간영역으로 낮은버전이 설치된것을 확인할수 있다.우측 하단에 빨간영역으로 낮은버전이 설치된것을 확인할수 있다. 설정이 완료되었으면 최신버전의 war를 다운받아 교체하고 재시작을 해준다. 그러면 너무나도 간단하게 버전이 업그레이드가 된것을 확인할수 있다. 그리고 처음에 설정한 Security 설정까지 그대로 유지되는것 또한 확인이 가능하다. 물론 구 버전에서 설치되었던 플러그인들이 버전업이 되며 그에 따라 지원하지 않는 문제들이 생길 수 있는데 이 부분은 플러그인을 업그레이드를 해준다거나 각 상황에 맞는 대응을 해줘야 한다. 이렇게 해서 생각보다(?) 너무 간단하게 버전업이 완료되었다. 업그레이드 후 플러그인 업그레이드도 동일하게 맞춰주는게 중요하다.업그레이드 후 플러그인 업그레이드도 동일하게 맞춰주는게 중요하다. # Jenkins 분산환경 구성하기 (노드 추가하기)이번엔 Jenkins를 분산환경으로 구성해보고자 한다. 이렇게 노드를 추가하며 분산환경을 구성하는 이유는 마스터-슬레이브(Master-Slave) 패턴의 장점을 얻고자 함이다. 마스터는 작업을 쪼개고 슬레이브로 구성된 노드에게 분배를 하게되면 슬레이브 서버는 마스터의 요청을 처리하고 리턴하게 된다. 마치 스타크래프트에서 일꾼을 늘려서 미네랄과 가스를 더 빨리 얻는것처럼 말이다. 여기서 필자가 가장 많이 삽질한 부분. 슬레이브 서버를 추가하는데 슬레이브 서버가 되는 서버에 동일하게 젠킨스를 설치하고 그들을 모두 연결하려 했던것… 마치 클러스터링 하는것처럼… 당연히 Jenkins 들의 묶음형태(?) 가 되야 할것같은 생각으로 시도하였지만 엄청난 삽질의 연속이 되어버렸다. 알고보니 마스터 Jenkins에서 슬레이브 서버에 작업을 전달할수 있도록 연동만 시켜주면 자동으로 Agent를 마스터 Jenkins가 슬레이브 서버에 설치/실행을 하고 작업을 분할하는것을 확인할 수 있었다. 자, 그럼 시작해보자. 마스터 서버에서 공개키와 개인키 생성먼저 마스터 서버와 슬레이브 서버를 SSH로 통신할수 있도록 SSH 키 설정을 해준다. 통상 홈 디렉토리 하위 .ssh 폴더에서 생성한다. 1234567891011121314151617181920212223242526ssh 키 생성$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/~/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /~/.ssh/id_rsa.Your public key has been saved in /~/.ssh/id_rsa.pub.The key fingerprint is:SHA256:~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ user@hostnameThe key's randomart image is:+---[RSA 2048]----+|oo. . ||o... o + ||. .o o+.o ||.++++. +o+o.. ||o.+*=.o.SEoo= || . o+.*...+ + || .. + +. + || + . . || ... |+----[SHA256]-----+공개키 확인$ cat id_rsa.pubssh-rsa AAAAB3Nza~~~~~~~~eQKcx8B6uAflRm1J8In1 user@hostname 슬레이브 서버에서 마스터 서버에서 만든 공개키를 등록슬레이브 서버에서는 마스터 서버에서 SSH 접속을 허용해야 하기때문에 마스터 서버에서 생성한 공개키를 등록해준다. 슬레이브 서버의 홈 디렉토리 하위 .ssh 폴더아래 파일을 만들고 위 공개키를 넣어주자. 12$ vi authorized_keysssh-rsa AAAAB3Nza~~~~~~~~eQKcx8B6uAflRm1J8In1 user@hostname Jenkins 에서 Credentials 을 만들때 Private Key 설정을 “From the Jenkins master ~/.ssh”으로 설정한다. 나중에 이 정보로 인증을 처리한다. 노드를 추가하고 조금 있으면 마스터 노드가 슬레이브 서버에 에이전트를 설치/실행하고 연동이 된것을 확인할수 있다. 실제로 슬레이브 서버에서 프로세스를 확인하면 아래처럼 에이전트( slave.jar )가 설치/실행되고 있는것을 확인할수 있다.123$ ps -ef | grep javauser 105431 105288 0 01:03 ? 00:00:00 bash -c cd &quot;/home&quot; &amp;&amp; java -jar slave.jaruser 105463 105431 3 01:03 ? 00:00:08 java -jar slave.jar 위와 같은 방법으로 슬레이브 서버를 총 두개를 구성하고 job을 여러개 실행하게 하면 자동으로(랜덤으로) 분배되어 실행하는것을 확인할 수 있다. 특정 job은 특정 슬레이브 서버에서 실행하고 싶은 경우도 있다. 예로들어 특정 슬레이브 서버가 성능이 더 좋다거나 네트워크 ACL이 특정 슬레이브 서버만 오픈되었다거나… 그럴 경우에는 아래처럼 job 실행설정에서 슬레이브를 강제로 지정할수도 있다. (짱…) # Jenkins 분산환경에서 버전 업그레이드 하기이제 위에서 했던것들의 종합 세트인 “Master-Slave로 되어있는 구성에서의 Jenkins 업그레이드” 를 해보자. 우선 위에서 했던것처럼 구버전으로 Master-Slave 를 구성한다.이제부터가 중요한데 필자는 당연히 위에서 했던 업그레이드 방법처럼 (이렇게 노드가 연결되어있는 상황에서) 기존의 war을 교체하면 되겠거니 했다. 하지만 업그레이드는 되었지만 노드가 연결이 안되면서 너무나도 다양한(?)에러를 만나야만 했다. 에러 내용을 찾아보니 필자처럼 버전 업그레이드를 하며 예외상황이 발생해 에러가 나는 경우가 많았고 삽질을 거듭해본 결과 다음과 같은 방법으로 하면 업그레이드도 되고 노드도 연결이 가능한것을 확인할 수 있었다. (다른 더 좋은 방법이 있다면 알고싶다… ) 우선 기존에 추가해둔 노드들을 제거한다. 그 다음 위에서 했던것처럼 war를 교체하며 업그레이드를 진행한다. Credentials 항목에 보면 개인키가 있는것을 볼수 있다. (기존에는 “From the Jenkins master ~/.ssh” 항목이 있었는데 없어졌다. ) 위에서 했던것처럼 노드를 추가해준다. 그럼 다음과 같은 에러를 만날수 있는데 에러 내용을 보면 known_hosts 파일이 없다고 나온다. 뭔가 해결할수 있을것만 같은 느낌이 든다. master 서버에서 ssh 슬레이브서버주소 명령어를 실행해서 known_hosts 파일을 생성하도록 해준다.123456789101112131415161718$ pwd/home/.ssh$ lsid_rsa id_rsa.pub$ ssh slave-hostThe authenticity of host 'slave-host (0.0.0.0)' can't be established.ECDSA key fingerprint is SHA256:0zb~~~~~B1A.ECDSA key fingerprint is MD5:~~~~:87.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'slave-host,0.0.0.0' (ECDSA) to the list of known hosts.Last login: Thu Mar 14 13:30:04 2019 from 10.113.219.197[user@slave-host ~]$ exitlogoutConnection to slave-host closed.$ lsid_rsa id_rsa.pub known_hosts$ cat known_hostsslave-host,0.0.0.0 ~~~ AAAA~~~~~~~8= 업그레이드 후 노드 구성한 화면업그레이드 후 노드 구성한 화면 # 마치며“Master-Slave로 되어있는 구성에서의 Jenkins 업그레이드”를 하며 정말 많은 시간을 할애할 수 밖에 없었고 (관련 지식도 없고 경험도 없었으니… ) 너무 안되어 포기할까도 싶었다. 하지만 경험하지 않은 모든 일들은 다 그만큼의 고통이 필요하고, 그 고통이 있어야지만 비로소 내것이 된다는 생각을 하고 있다. 이것도 나만의 무기가 되어 나중에 jenkins 를 업그레이드 한다거나 노드구성을 할때 보다 쉽고 빠르게 할수있지 않을까 기대를 해본다. 더불어 어려운 이야기이지만 삽질도 올바른 삽질을 할수 있도록 소망해본다…이런 삽질은 그만... 출처 : https://gfycat.com/ko/illiterateonlyicelandgull이런 삽질은 그만... 출처 : https://gfycat.com/ko/illiterateonlyicelandgull","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"기술블로그 구독서비스 개발 후기 - 3부","slug":"daily-dev-blog-3","date":"2019-02-16T16:17:27.000Z","updated":"2020-04-23T04:41:36.711Z","comments":true,"path":"2019/02/17/daily-dev-blog-3/","link":"","permalink":"https://taetaetae.github.io/2019/02/17/daily-dev-blog-3/","excerpt":"작년 7월 12일부터 시작한 필자의 첫 토이프로젝트인 기술블로그 구독서비스. 오픈할 때까지만 해도 “AWS 프리티어를 사용하고 있는 1년 안에 구독자가 설마 1,000명이 넘겠어?” 라고 생각을 했었는데","text":"작년 7월 12일부터 시작한 필자의 첫 토이프로젝트인 기술블로그 구독서비스. 오픈할 때까지만 해도 “AWS 프리티어를 사용하고 있는 1년 안에 구독자가 설마 1,000명이 넘겠어?” 라고 생각을 했었는데 오픈을 하고 220일째 되는 바로 어제 어느덧 벌써 구독자가 1,000명을 달성하게 되었다. 그 기념으로 그동안 미뤄두었던 기술블로그 구독서비스 개발 후기 시리즈의 3부를 쓰고자 한다.오예~ 1,000명이다! 땡큐! 출처 : https://gfycat.com/ko/leafytorngroundbeetle오예~ 1,000명이다! 땡큐! 출처 : https://gfycat.com/ko/leafytorngroundbeetle 혹시 전에 내용을 보고자 하면 아래 링크에서 확인할 수 있다. 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 # 그간 어떤 식으로 서비스를 운영했는가?(한마디로 정리할 순 없는 지난 220일이었지만…) 딱 한마디로 정리하자면 엄청나게 많은 것을 배우고 경험할 수 있었으나 그만큼 힘들었던 시간들이라고 말할 수 있을 것 같다. 2부에서 이야기한 문제 발생에 따른 Trouble Shooting들도 있었지만 운영을 해오다 보니 사전에 생각하지도 못한 부분에서 문제가 생기는 정말 다양한 경험을 할 수 있었기 때문이다. 블로그 포스팅을 수집하는 과정에서의 문제일부 블로그 RSS url에 접근을 할 때 요청에 대한 응답이 무한대로 멈춰버리는 현상이 간헐적으로 있었다. 이는 별도의 타임아웃을 설정하지 않았기 때문이다. 그래서 어느 정도의 타임아웃을 두고 시간 내에 응답이 없을 경우 다음 포스팅으로 넘어가도록 하였다. (타임아웃은 아주 기본적인 부분인데…) 1requests.get(rss_url, timeout=10.0) 메일 발송하는 과정에서의 문제가끔 메일이 오지 않는다고 친절하게 필자 개인 메일로 연락이 오는 경우가 있었다. 그때마다 서버의 상태를 보면 서버에 직접 접속조차 안 될 정도로 메모리 사용량이 너무 많아서 그때마다 AWS 웹 콘솔에서 강제로 서버를 재부팅을 하곤 했었다. 예전에도 이야기한 것처럼 AWS 프리티어를 사용하고 있다 보니 서버의 메모리가 1기가밖에 되지 않아서 … 제한된 시스템에서 서비스 운영을 할 수밖에 없는 상황이었다.그래서 수집/발송 상태를 로깅으로 쉽게 볼 수 있고 스케줄링을 하기 위해 띄워둔 Jenkins(tomcat)를 중단하고 crontab으로 스케줄링을 하도록 하였고, 로깅은 별도의 파일로 로깅하도록 변경하였다. 1/usr/bin/python3.6 /home/~~~/email_send.py &gt; /home/~~~/logs/job/email_send_`date +\\%Y\\%m\\%d\\%H\\%M\\%S`.log 2&gt;&amp;1 또한 기존에는 빠르게 발송하기 위해 냅다 스레드로 돌렸는데 구독자 수가 많아지다 보니 RuntimeError: can&#39;t start new thread 라고 스레드를 만들 수 없다는 에러가 발생하기도 했다. 그래서 Pool을 사용하는 방식의 multiprocessing 을 도입하여 스레드로 발송할 때보다는 엄청나게 빠른 속도는 아닐지라도 효율적인 메모리 사용으로 2분 안에 1,000명에게 안정된 메일을 보낼 수 있게 되었다. (여담이지만 메일이 안 온다고 알려주셨던 분들께 이 자리를 빌려 감사의 인사를 전하고 싶다.) from multiprocessing import Pool ... pool = Pool(20) pool.map(sendMail, email_list) Heroku 나 Netlify 같이 서버를 직접 들어가지 않고 앱 형태로 배포하는 식으로 할 수도 있다. 하지만 초기에 이 토이프로젝트를 시작할 때 실 서비스와 최대한 동일한 시스템으로 운영해보고 싶었기 때문에 라즈베리파이에 설치하는 것까지 알아보다 결국 AWS를 사용하기로 하게 되었다.그렇다면 AWS 프리티어를 사용하지 않고 별도의 서버를 구매하면 안 될까? 하는 생각도 해봤지만 최소한의 인프라로 최대한의 성능을 내보고 싶은 욕심(?) 때문에 1년간은 프리티어로 운영하고 그다음엔 (혹은 소프트웨어적으로 한계까지 도달한다면) 서버를 구매해서 운영하게 될 것 같다. (적어도 이후에도 이 서비스를 유지한다는 가정하에…) 농부의 마음으로... 출처 : http://www.iwithjesus.com/news/articleView.html?idxno=2511농부의 마음으로... 출처 : http://www.iwithjesus.com/news/articleView.html?idxno=2511 아침 10시가 되면 자동으로 메일이 잘 발송되었는지, 혹 어제 수집된 것이 아니라 예전에 수집된 내용이 중복 발송된 건 아닌지, 발송은 구독한 사람 전부에게 잘 보내졌는지… 거의 매일같이 Daily-DevBlog 서비스를 살피며 지낸 것 같다. (하루라도 문제가 생기면 밤을 새워서라도 원인을 파악하고 다음 발송에는 정상적으로 발송되도록 수정하기도 했다.) # 앞으로의 계획과 방향성여력이 되는 데까지 이 서비스를 운영할 계획이다. AWS 프리티어 기간이 끝나도 라즈베리파이나 안 쓰는 노트북을 활용해서 서버를 구성하던지 (한 달에 얼마를 지불할지는 모르겠지만) AWS에서 서버를 발급받아서라도 운영하고 싶다. 그 이유는 이 토이프로젝트를 진행하면서 얻게 된 인사이트도 상당히 많았고, python과 apache 등 기존에 알고 있던 부분 이외로 알게 되는 것 또한 많았기 때문이다. 그리고 가장 중요한 공유, 사실 이 서비스를 만들면서 필자 또한 많은 좋은 글들을 볼수있었고 그에 큰 도움도 많이 받을 수 있었다.만들고 싶은 기능도 많다. 포스팅의 내용을 분석하여 자동으로 기술과 관련되지 않는 글을 제외하는 기능도 만들고 싶고, 자동으로 주요 키워드 (태그)를 만들어 이후에도 태그 기준으로 검색을 통해 보고싶은 글을 뉴스처럼 볼수 있는 기능도 만들고 싶고… 운영을 하다 보니 만들고 싶은 기능은 많지만 기술적인 접근이 어려운 상황이다.하지만 가장 중요한 건 새로운 기능 추가보다 안정적으로 매일 아침 10시마다 바로 어제의 글들을 수집하여 구독자들에게 발송하는 것이 가장 중요한 게 아닐까 싶다. 구독자수 증가 그래프구독자수 증가 그래프 # 구독자 1,000명 기념 추가 기능 공개!예전부터 1,000명이 되는 시점에 뭔가 이벤트 성으로 새로운 기능을 공개하고 싶어서 준비를 해보았다. 아카이브위에서 이야기했듯이 기술과 관련되지 않는 글들에 대한 필터링을 기술적으로 하고 싶었으나 예로 들어 “00역 맛집리스트 자동으로 가져오기” 나 “코딩하면서 먹기에 좋은 음식” 이라는 제목이 있을 경우 과연 어떤 글이 기술에 관련된 글이고 어떤 글이 기술과는 거리가 있는 글인지 기술적으로 분석할 방법이 아직까지는 떠오르지 않는다. (물론 머신러닝이나 다른 방법이 있겠지만…)그래서 기존에 수집한 글들을 한 곳에서 보여주면서 기술과는 거리가 있어 보이는 글들에 대해서 제외하고 볼 수 있도록 아카이빙 페이지를 만들었다. 그리고 날짜를 넘겨가며 조회할 수 있고 정렬 순서는 랜덤으로 만들었다. 크롬 익스텐션 기술블로그 라고 검색해도 나온다.기술블로그 라고 검색해도 나온다. 위에서 만들었던 아카이빙 페이지를 단순하게 익스텐션 클릭 한 번으로 접속이 되도록 만들어보았다. 점심시간 또는 여유시간에 공유된 기술 블로그 포스팅을 쉬운 접근성을 통해 읽어보자는 조금이라도 챙겨보자는 느낌으로 만들게 되었고, 크롬 알림 기능을 활용하여 PC 크롬이 켜져 있는 상황에서 아침 10시가 되면 메일이 발송되는 것처럼 아래 화면과 같이 알람을 주도록 하였다. 아침 10시엔 우리 모두 Daily-DevBlog를~아침 10시엔 우리 모두 Daily-DevBlog를~ 주간 인기글구독자들이 어떤 글에 더 관심이 갖는지 궁금하였고 많이 본 글에 대해서는 한 번 더 정리하여 메일로 발송해주는 것이 좋을 것 같다는 생각이 들었다. 그래서 메일로 발송된 글에 대해 클릭수를 기준으로 매주 월요일마다 “주간 인기글”을 발행하는 기능을 추가하였다. 단체 블로그 추가 수집지금은 어썸 데브블로그에서 제공해주는 개인 블로거들의 피드를 수집하고 있는데 단체 블로그들 또한 추가로 수집하여 메일의 상단에 배치한다. (단체 블로그는 아무래도 검증이 된 글일 거라 생각이 든다.) # 마치며혹시 이 서비스에 대한 아이디어가 있는 분들은 아래 댓글이나 개인 메일로 알려주시면 최대한 반영해보고자 한다. 또한 나중에는 github에 공개하여 오픈소스화한다면 필자보다 더 뛰어난 python 개발자들이 보다 좋은 코드를 만들어주어 점점 해당 서비스가 좋아지지 않을까 하는 기대를 해보며 기술블로그 구독서비스 개발후기를 마친다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[]},{"title":"누구나 할 수 있는 엑세스 로그 분석 따라 해보기 (by Elastic Stack)","slug":"access-log-to-elastic-stack","date":"2019-02-10T05:37:31.000Z","updated":"2020-04-23T04:41:36.641Z","comments":true,"path":"2019/02/10/access-log-to-elastic-stack/","link":"","permalink":"https://taetaetae.github.io/2019/02/10/access-log-to-elastic-stack/","excerpt":"필자가 Elastic Stack을 알게된건 2017년 어느 여름 동기형이 공부하고 있는것을 보고 호기심에 따라하며 시작하게 되었다. 그때까지만 해도 버전이 2.x 였는데 지금 글을 쓰고있는 2019년 2월초 최신버전이 6.6이니 정말 빠르게 변화하는것 같다.","text":"필자가 Elastic Stack을 알게된건 2017년 어느 여름 동기형이 공부하고 있는것을 보고 호기심에 따라하며 시작하게 되었다. 그때까지만 해도 버전이 2.x 였는데 지금 글을 쓰고있는 2019년 2월초 최신버전이 6.6이니 정말 빠르게 변화하는것 같다. 빠르게 변화하는 버전만큼 사람들의 관심도 (드라마틱하게는 아니지만) 꾸준히 늘어나 개인적으로, 그리고 실무에서도 활용하는 범위가 많아지고 있는것 같다. trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"elasticsearch\",\"geo\":\"KR\",\"time\":\"today 5-y\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&geo=KR&q=elasticsearch\",\"guestPath\":\"https://trends.google.co.kr:443/trends/embed/\"}); 그래서 그런지 최근들어 (아주 코딱지만큼 조금이라도 더 해본) 필자에게 Elastic Stack 사용방법에 대해 물어보는 주변 지인들이 늘어나고 있다. 그리고 예전에 한창 공부했을때의 버전보다 많이 바꼈기에 이 기회에 “그대로 따라만 하면 Elastic Stack을 구성할 수 있을만한 글”을 써보고자 한다. 사실 필자가 예전에 “도큐먼트를 보기엔 너무 어려워 보이는 느낌적인 느낌” 때문에 삽질하며 구성한 힘들었던 기억을 되살려 최대한 심플하고 처음 해보는 사람도 따라하기만 하면 “아~ 이게 Elastic Stack 이구나!”, “이런식으로 돌아가는 거구나!” 하는 도움을 주고 싶다. + 그러면서 최신버전도 살펴보고… 1석2조, 이런게 바로 블로그를 하는 이유이지 않을까?다시한번 말하지만 도큐먼트가 최고 지침서이긴 하다… Elastic 공식 홈페이지에 가면 각 제품군들에 대해 그림으로 된 자세한 설명과 도큐먼트가 있지만 이들을 어떤식으로 조합하여 사용하는지에 대한 전체적인 흐름을 볼 수 있는 곳은 없어 보인다. (지금 보면 도큐먼트가 그 어디보다 설명이 잘되어 있다고 생각되지만 사전 지식이 전혀없는 상태에서는 봐도봐도 어려워 보였다.)이번 포스팅에서는 Apache access log를 Elasticsearch에 인덱싱 하는 방법에 대해 설명해보고자 한다. # 전체적인 흐름필자는 글보다는 그림을 좋아하는 편이라 전체적인 흐름을 그림으로 먼저 보자. 외부에서의 접근이 발생하면 apache 웹서버에서 설정한 경로에 access log가 파일로 생성이 되거나 있는 파일에 추가가 된다. 해당 파일에는 한줄당 하나의 엑세스 정보가 남게 된다. fileBeat에서 해당 파일을 트래킹 하고 있다가 라인이 추가되면 이 정보를 logstash 에게 전달해준다. logastsh 는 filebeat에서 전달한 정보를 특정 port로 input 받는다. 받은 정보를 filter 과정을 통해 각 정보를 분할 및 정제한다. (ip, uri, time 등) 정리된 정보를 elasticsearch 에 ouput 으로 보낸다. (정확히 말하면 인덱싱을 한다.) elasticsearch 에 인덱싱 된 정보를 키바나를 통해 손쉽게 분석을 한다. 한번의 설치고 일련의 과정이 뚝딱 된다면 너무 편하겠지만, 각각의 레이어가 나뉘어져있는 이유는 하는 역활이 전문적으로(?) 나뉘어져 있고 각 레이어에서는 세부 설정을 통해 보다 효율적으로 데이터를 관리할 수 있기 때문이다. beats라는 레이어가 나오기 전에는 logstash에서 직접 file을 바라보곤 했었는데 beats가 logstash 보다 가벼운 shipper 목적으로 나온 agent 이다보니 통상 logstash 앞단에 filebeat를 위치시키곤 한다고 한다. 전체적인 그림은 위와 같고, 이제 이 글을 보고있는 여러분들이 따라할 차례이다. 각 레이어별로 하나씩 설치를 해보며 구성을 해보자. 설치순서는 데이터 흐름의 순서에 맞춰 다음과 같은 순서로 설치를 해야 효율적으로 볼수가 있다. (아래순서대로 하지 않을경우 설치/시작/종료 를 각각의 타이밍에 맞추어 해줘야 할것 같아 복잡할것같다.) 1elasticsearch → logstash → kibana → filebeat 이 포스팅은 CentOS 7.4에서 Java 1.8, apache 2.2가 설치되어있다는 가정하에 보면 될듯하다. 또한 각 레이어별 설명은 구글링을 하거나 Elastic 공식 홈페이지에 가보면 자세히 나와있으니 기본 설명은 안하는것으로 하고, 각 레이어의 세부 설정은 하지 않는것으로 한다. # Elasticsearch공식 홈페이지1234567891011121314151617다운받고 압축풀고 심볼릭 경로 만들고 (심볼릭 경로는 선택사항)$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.tar.gz$ tar zxvf elasticsearch-6.6.0.tar.gz$ ln -s elasticsearch-6.6.0 elasticsearch설정 파일을 열고 추가해준다.$ cd elasticsearch/conf$ vi elasticsearch.ymlpath.data: /~~~/data/elasticsearch (기본경로에서 변경할때추가)path.logs: /~~~/logs/elasticsearchnetwork.host: 0.0.0.0 # 외부에서 접근이 가능하도록 (실제 ip를 적어줘도 됨)elasticsearch 의 시작과 종료를 조금이나마 편하게 하기위해 스크립트를 작성해줌 (이것또한 선택사항)$ cd ../bin$ echo &apos;./elasticsearch -d -p es.pid&apos; &gt; start.sh$ echo &apos;kill `cat es.pid`&apos; &gt; stop.sh$ chmod 755 start.sh stop.sh 혹시 아래와 같은 에러가 발생할경우 공식문서 대로 진행해준다.1234ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]&gt; sudo /sbin/sysctl -w vm.max_map_count=262144 이렇게 하고 시작을 한뒤 브라우저에서 http://{ip}:9200 로 접속하면 다음과 같이 설치된 elasticsearch에 기본 정보가 나오게 되고 이렇게 elasticsearch의 설치 및 실행이 완료되었다. 1234567891011121314151617&#123; &quot;name&quot;: &quot;@@@&quot;, &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;@@@&quot;, &quot;version&quot;: &#123; &quot;number&quot;: &quot;6.6.0&quot;, &quot;build_flavor&quot;: &quot;default&quot;, &quot;build_type&quot;: &quot;tar&quot;, &quot;build_hash&quot;: &quot;@@@&quot;, &quot;build_date&quot;: &quot;2019-01-24T11:27:09.439740Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;7.6.0&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;5.0.0&quot; &#125;, &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; # Logstash공식 홈페이지 1234567891011121314151617181920212223242526272829303132다운을 받고 압축풀고 심볼릭 링크 설정$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.6.0.tar.gz$ tar -zxvf logstash-6.6.0.tar.gz$ ln -s logstash-6.6.0 logstashlogstash가 실행될때 설정값 파일을 만들어준다.$ cd logstash/config$ vi access_log.conf# beats 에서 5044 port 로 데이터를 input 받겠다는 의미input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;# grok 필터를 활용하여 엑세스로그 한줄을 아래처럼 파싱하겠다는 의미# 해당 필터는 apache의 로깅 설정에 의해 만들어지는 파일의 포멧에 맞추어 설정해야한다.filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [&quot;%&#123;IPORHOST:clientip&#125; (?:-|%&#123;USER:ident&#125;) (?:-|%&#123;USER:auth&#125;) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;NOTSPACE:uri&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER:bytes&#125;) (?:-|%&#123;NUMBER:bytes2&#125;)( \\&quot;%&#123;DATA:referrer&#125;\\&quot;)?( \\&quot;%&#123;DATA:user-agent&#125;\\&quot;)?&quot;] &#125; remove_field =&gt; [&quot;timestamp&quot;,&quot;@version&quot;,&quot;path&quot;,&quot;tags&quot;,&quot;httpversion&quot;,&quot;bytes2&quot;] &#125;&#125;# 정제된 데이터를 elasticsearch 에 인덱싱 하겟다는 의미# index 이름에 날짜 형태로 적어주면 인덱싱 하는 시점의 시간에 따라 인덱싱 이름이 자동으로 변경이 된다. (아래는 월별로 인덱스를 만들경우)output &#123; elasticsearch &#123; hosts =&gt; [ &quot;&#123;elasticsearch ip&#125;:9200&quot; ] index =&gt; &quot;index-%&#123;+YYYY.MM&#125;&quot; &#125;&#125; 실행은 다음과 같이 &amp;연산자를 활용하여 background로 실행하게 한다. 1$ bin/logstash -f config/access_log.conf &amp; 이렇게 해서 실행을 하고 에러없이 정상적으로 실행이 된뒤 프로세스가 올라와 있으면 (ps -ef | grep logstash) 성공된 상태라 볼수있다. # Kibana공식 홈페이지 12345678910역시 다운받고 압축풀고 심볼릭 링크 설정$ https://artifacts.elastic.co/downloads/kibana/kibana-6.6.0-linux-x86_64.tar.gz$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz$ ln -s kibana-6.6.0-linux-x86_64 kibana외부에서 접근을 하기위해 ip를 적어주고, 연결할 elasticsearch 주소또한 적어준다.$ cd kibana/config$ vi kibana.ymlserver.host: &quot;@.@.@.@&quot;elasticsearch.hosts: [&quot;http://@.@.@.@:9200&quot;] 실행은 bin 폴더로 이동후에 다음과 같이 실행시켜준다. 별다른 에러가 없으면 외부에서 접근이 가능한지 확인해보자.1234$ cd bin/$ nohup ./kibana &amp;접속 : http://@.@.@.@/5601 # Filebeat공식 홈페이지 1234567891011121314151617181920212223242526다운 → 압축해제 → 심볼릭링크$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.6.0-linux-x86_64.tar.gz$ tar -zxvf filebeat-6.6.0-linux-x86_64.tar.gz$ ln -s filebeat-6.6.0-linux-x86_64 filebeatfilebeat가 실행될때의 설정파일을 작성해준다.$ cd filebeat$ vi access_log.ymlfilebeat.prospectors:- type: log enabled: true paths: - /~~~/logs/apache/access.log.* # 실제 엑세스 파일의 경로 tail_files: true # filebeat 시작시점 기준 파일 끝에서부터 로깅을 읽기 시작 ignore_older: 1m # filebeat 시작시점 기준 1분전의 내용은 무시 close_inactive: 2m clean_inactive: 15mlogging.level: infologging.to_files: truelogging.files: # filebeat가 실행되면서 남기는 로깅파일 정보. 도큐먼트를 읽어보는것을 추천한다. path: /~~~/logs/filebeat name: test-filebeat-log keepfiles: 7 rotateeverybytes: 524288000output.logstash: # 최종적으로 output 할 logstash의 정보를 입력해준다. hosts: [&quot;@.@.@.@:5044&quot;] 위와 같이 설정파일을 작성한 다음 아래처럼 실행을 하면 엑세스 파일의 내용이 filebeat를 거치고 logstash를 거쳐 최종적으로 elasticsearch 에 도달하게 된다. 기존에 엑세스 로그가 양이 많다면 그 정보를 다 읽는 시간이 걸리므로 주의한다. (filebeat 자체적으로 해당 파일의 offset을 관리하기 때문) 1$ ./filebeat -c access_log.yml -d publish &amp; # 최종 확인왼쪽에는 해당 서버를 호출하고 오른쪽에는 키바나를 띄워논뒤 테스트를 해보면 아래처럼 access log를 확인이 가능하다. (apache만 띄워놓은 상태라 404상태로 나오긴 한다..) 불필요한 필드가 있다면 logstash 의 filter에서 remove 하면되고 키바나에서 각 정보를 가지고 다양한 유의미한 데이터를 만들어볼 수 있게 되었다. # 마치며막상 해보면 (해보기전에 느끼는 두려움보다는) 엄청나게 미친듯이 어렵지는 않는데… 맨땅에 해딩이든 뭐든 시작해보고 만들어보는게 중요하다고 다시한번 생각해본다. 필자는 elasticsearch 2.4버전에 대해 영어로된 문서를 보며 설치하고 구성하며 (왜 한글로 된 문서가 한명도 없을까…) 하는 아쉬움에 있었는데 이 글이 필자처럼 설치하는데 비슷한(?) 고충을 느낀 사람들에게 도움이 되었으면 한다.마지막으로 세부 설정값들로 인해 성능이나 기능이 다양하게 바뀔수 있으니 공식 도큐먼트를 보는것을 강력 추천하고 싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"https://taetaetae.github.io/tags/logstash/"},{"name":"kibana","slug":"kibana","permalink":"https://taetaetae.github.io/tags/kibana/"},{"name":"filebeat","slug":"filebeat","permalink":"https://taetaetae.github.io/tags/filebeat/"}]},{"title":"Spring MVC Redirect 처리중에 발생한 Out Of Memory 원인 분석하기","slug":"spring-redirect-oom","date":"2019-01-10T09:39:47.000Z","updated":"2020-04-23T04:41:36.883Z","comments":true,"path":"2019/01/10/spring-redirect-oom/","link":"","permalink":"https://taetaetae.github.io/2019/01/10/spring-redirect-oom/","excerpt":"초창기 신입시절에 배우거나 사용했던 기술적인 방법들이 있다. 시간이 지날수록 왠만해선 다른방법은 사용하지 않으려 하고 습관처럼 기존에 사용했던 방법을 고수하는 버릇이 있다. 그 이유는 과거에 사용했을때 아무 탈 없이 잘 되었기 때문에, 그리고 빠른 구현 때문이라는 핑계일 것 같다.","text":"초창기 신입시절에 배우거나 사용했던 기술적인 방법들이 있다. 시간이 지날수록 왠만해선 다른방법은 사용하지 않으려 하고 습관처럼 기존에 사용했던 방법을 고수하는 버릇이 있다. 그 이유는 과거에 사용했을때 아무 탈 없이 잘 되었기 때문에, 그리고 빠른 구현 때문이라는 핑계일 것 같다. 이러한 버릇은 비단 이 글을 적고있는 필자 뿐만이 아니라 대부분의 개발자들이 가지고 있을꺼라 조심스레 추측해본다. (아니라면…더욱 분발 해야겠다…ㅠ)최근 운영하고 있는 서비스에서 장애 상황까지 갈수있는 위험한 상황이 있었는데 팀내 코드리뷰를 통해 문제점을 파악할 수 있었다. 그 원인은 Spring MVC Controller 레벨에서 redirect 처리를 할때 return값의 Cardinality가 높을경우 다음과 같이 사용하면 안된다고… 12345@RequestMapping(value = \"/test\", method = RequestMethod.GET)public String test() &#123; String url = \"어떠한 로직에 의해 생성되는 url\"; return \"redirect:\" + url; // &lt;- 위험 포인트!&#125; 이 코드가 왜? 어디가 어때서?이제까지 Controller 레벨에서 redirect 처리를 할때 아무생각없이 위에 있는 코드 형태로 구현을 했는데 저러한 코드 때문에 OOM이 발생하여 fullGC 가 여러번 발생하고, 일시적으로 서비스가 지연되는 현상이 발생했다고 한다. 자주 사용하던 방법이였는데 장애를 유발할수 있는 위험한 방법이였다니…이번 포스팅에서는 이러한 방법이 왜 잘못되었는지 실제로 테스트를 통해 몸소(?) 체감을 해보고, 그럼 어떤 방법으로 redirect 처리를 해야 하는가와 개선을 함으로써 기존방식에 비해 어떤점이 좋아졌는지에 대해서 정리해보고자 한다. 뭔가 내것으로 만들기 시리즈물이 나올것만 같은 느낌이다… # 기존방식의 문제점 재현 및 다양한 원인분석기존방식으로 했을때 왜 OOM이 발생했을까? 우리는 개발자이기 때문에 이런저런 글들만 보고 추측 할것이 아니라 직접 재현을 해보고 다양한 시각에서 원인분석을 해보자.먼저 기본적인 Spring MVC 뼈대를 만들고 redirect 하는 return 값의 Cardinality가 높도록 random string 을 만들어 주도록 한다. 즉, /random을 호출하면 /result/ETmHfowFkU처럼 random string 이 만들어 지며 redirect 처리가 되는 매우 심플한 구조이다. 123456789101112131415// Spring 버전은 4.0.6.RELEASE@Controller@RequestMapping(\"/\")public class TestController &#123; @RequestMapping(value = \"random\", method = RequestMethod.GET) public String random() &#123; return \"redirect:result/\" + UUID.randomUUID(); &#125; @RequestMapping(value = \"result/&#123;message&#125;\", method = RequestMethod.GET) public String result(ModelMap model, @PathVariable String message) &#123; model.addAttribute(\"message\", message); return \"result\"; &#125;&#125; 또한 해당 프로젝트에서는 AOP를 사용하고 있었기 때문에 그때와 동일한 상황으로 재현을 하기 위해 AOP관련 설정도 추가해준다. 12345678910111213141516@Configuration@EnableWebMvc@EnableAspectJAutoProxy@ComponentScanpublic class HelloWorldConfiguration &#123; @Bean(name=\"HelloWorld\") public ViewResolver viewResolver() &#123; InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setViewClass(JstlView.class); viewResolver.setPrefix(\"/WEB-INF/views/\"); viewResolver.setSuffix(\".jsp\"); return viewResolver; &#125;&#125; 이렇게 한뒤 tomcat으로 최대/최소 메모리를 256m으로 설정후 해당 모듈을 띄워준다. 그다음 메모리 상태를 보기 위해 tomcat에 pinpoint를 연동하고 마지막으로 호출테스트를 위해 nGrinder을 설정해준다. 특별한 설정은 없고 위 컨트롤러의 url (/random) 을 여러번 호출하도록 하였다. nGrinder을 설정하는대에는 이 블로그 포스팅을 참고해서 설정하였다. 자, 이제 테스트를 시작해보자. (마치 수술 집도하는것 같은 기분으로…간호사~ 칼!) nGrindernGrinder의 기본 스크립트에서 url만 해당 서버로 호출되도록 바꿔주고 총 가상 사용자는 2,000으로 시간은 5분으로 설정후에 테스트 시작을 하였더니 다음과 같은 그래프를 볼수 있었다. TPS가 불안정해지다가 어느시점부터 낮아지는것을 확인할 수 있다. 이게 서비스 였다면 사용자가 접속하는데 불편을 느꼈을꺼라 추측을 해본다. 또한 아주 간단한 random string 을 리턴하는 페이지 임에도 불구하고 에러 응답이 적지 않은것을 확인할 수 있었다. pinpoint메모리 상태는 어떤지 확인하기 위해 pinpoint를 확인해보면 다음과 같은 그래프를 볼수 있었다. 보기만해도 심장이 벌렁벌렁(?) 뛸 정도로 무서운 그림이다. 실제로 서비스에 (이정도까진 아니였지만) 비슷한 상황이 발생했었다. 메모리가 테스트를 점점 하면 할수록 올라가다가 fullGC가 발생하더니 대나무 숲에 있는 대나무마냥 fullGC가 빼곡히 발생하였다. (이러니… 페이지 접근에 지연이 생긴것 같다.) Heap dump그럼 실제로 메모리는 어떤 상태였고 어디서 메모리를 많이 사용하고(점유하고) 있는지를 확인하기 위해 Heap dump를 생성해 보았다. 힙덤프 분석하는데 잘 알려진 Memory Analyzer (MAT)를 다운받고 해당 프로세스의 힙덤프를 생성한다음 분석을 해봤더니 아래와 같은 화면을 볼 수 있었다. 힙덤프 파일을 열자마자 (저 문제 있어요~ 도와주세요 하듯) 뭔가 많이 점유하고 있는것처럼 보이는 파이그래프가 Overview에 보였다. Reports 영역에 있는 Leak Suspects를 확인해보니 아래 경로에서 많이 사용하는 것을 확인할 수 있었다. java.util.concurrent.ConcurrentHashMap$Node 이 툴에서는 OQL이라고 힙덤프에 있는 데이터를 일반 SQL처럼 쿼리처럼 볼수 있었다. 그래서 아래처럼 쿼리를 작성해서 봤더니 결과만 봐도 어디서 메모리를 점유하고 있는지 한눈에 볼수 있었다 123SELECT o.key.toString() FROM java.util.concurrent.ConcurrentHashMap$Node o WHERE ((o.key != null) and (o.key.toString().indexOf(\"org.springframework.web.servlet.view.RedirectView_redirect\") = 0)) 무작위로 만들어진 url에 대한 정보를 캐시하고 있는 듯한 결과였다. # 방식 개선 및 변화 비교결국 return &quot;redirect:&quot; + url; 와 같은 처리가 문제를 야기했던 것이였다. 그럼 redirect 처리를 어떻게 하는게 좋을까?조금 검색을 해보면 RedirectView 나 ModelAndView를 사용하라고 권장하고 있다. 물론 redirect 되는 url의 Cardinality가 높지않고 고정적이라면 지금의 return &quot;redirect:&quot; + url; 이 방식을 사용해도 무방할수 있다. 하지만 컨트롤러 메소드가 String타입을 return 하게 되면 View 클래스로 변환작업을 진행하게 된다고 한다.이 작업중에 org.springframework.beans.factory.config.BeanPostProcessor구현체들도 같이 진행되고, 이중에 하나가 AnnotationAwareAspectJAutoProxyCreator 라고 있는데 해당 클래스 내부적으로 ConcurrentHashMap&lt;Object, Boolean&gt; 타입 객체에 key : viewName, value : 필요 여부(boolean) 형태로 갯수 제한 없이 저장하고 있다. 그러다보니 url의 종류가 많아질수록 메모리가 많이 사용될 수밖에 없었던 것 같다.즉, 동일한 url에 대해 View 객체를 캐싱하고 있으니 위와 같이 url의 종류가 다양할경우 (특히 로그인 같은 처리를 할때 고유값을 파라미터로 넘기는 경우) 캐싱 객체 숫자가 많아지기 마련이다. 실제로 코드를 보면 캐싱을 하고있는것을 볼수 있다. Spring-project github123456789101112131415// ... 생략protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; // ... 생략 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); // &lt;- 여기 return bean; &#125; // ... 생략 this.advisedBeans.put(cacheKey, Boolean.FALSE); // &lt;- 여기 return bean;&#125;// ... 생략 자, 그럼 개선방법을 알아봤으니 한번 비교를 해보자. 아래처럼 ModelAndView를 사용해서 redirect처리를 할수있도록 코드를 변경하고 1234567891011121314151617181920@Controller@RequestMapping(\"/\")public class TestController &#123; @RequestMapping(value = \"random\", method = RequestMethod.GET) public ModelAndView random() &#123; ModelAndView modelAndView = new ModelAndView(); RedirectView redirectView = new RedirectView(); redirectView.setUrl(\"result/\" + UUID.randomUUID()); modelAndView.setView(redirectView); return modelAndView; &#125; @RequestMapping(value = \"/result/&#123;message&#125;\", method = RequestMethod.GET) public String result(ModelMap model, @PathVariable String message) &#123; model.addAttribute(\"message\", message); return \"result\"; &#125;&#125; 기존과 동일한 방법과 환경에서 테스트를 해보자. nGrinder위해서 했던 방법과 동일한 가상 사용자 수와 동일한 시간으로 테스트를 해보니 다음과 같은 그래프를 볼수 있었다. 위에서 봤던 들쭉날쭉 그래프보다 훨씬 더 안정적인것을 볼 수 있었고, 에러도 단 한건도 없이 훨씬 높은 TPS를 끝까지 일정하게 유지하는 모습을 볼수 있었다. (로직상 에러가 나는게 이상한… 아니 안나야 정상이다.) pinpointTPS가 안정적이였기 때문에 메모리의 상태를 안봐도 되겠지만 비교의 목적이 있기 때문에 pinpoint 의 그래프를 한번 보자. nGrinder로 테스트하는 시점만 잠깐 메모리가 올라가다가 다시 내려오고 전에 있었던 fullGC도 없고 위에서 테스트 했던 그래프 보다는 안정적인 그래프라고 볼수 있었다. Heap dump메모리가 안정적이였지만 혹시 pinpoint에서 잘못 집계하거나 그래프만 보고 맹신할수 없었기 때문에 이번에도 Heap dump를 생성해 보았다. 일단 점유하고 있는 메모리의 크기가 약 10분의 1정도로 줄어든것을 확인할수 있었고, 위에서 했던 OQL을 이용한 메모리 점유를 확인해봐도 기존에 있던 RedirectView_redirect관련 데이터가 아예 없음을 확인할 수 있었다. 코드 몇줄 변경한것밖에 없는데 같은 테스트 환경에서 확연히 좋아진것을 확인할 수 있다. (뿌듯) 전체적으로 다시 비교를 해보면 아래와 같이 이쁜(?) 변화를 볼수가 있다. # 마치며평소에 자주 사용하던 방식인데 성능적으로 자칫 치명적인 결과를 가져올수 있다고 하여 재현을 안해볼수가 없었다. 만약 악의적으로 url뒤에 무작위 문자열을 더해서 ddos공격을 했더라면? 얼마 안가서 서버가 터졌을지도 모른다.지금이라도 알아서 다행이라는 생각과 재현을 안해보고 그냥 그런가보다 하며 넘어갔다면 실제 Spring 내부 코드까지 볼일이 있었을까 하는 생각을 해본다.이번에 재현을 해보면서 nGrinder 로 성능테스트에 pinpoint 모니터링, 마지막으로 힙덤프 분석까지. 꼭 이번 url redirect 문제만이 아니라 다른 성능적인 이슈가 생길때 마치 치트키처럼 활용할수 있을 나만의 무기를 얻은것 같아 다시한번 뿌듯함을 느낀다. 마지막으로, 이렇게 재현까지 하도록 자극을 주신 팀 동료분께 감사드린다고 전하고 싶다. (보실지 안보실지 모르겠지만 ^^;) 관련 참고글https://www.baeldung.com/spring-redirect-and-forwardhttps://www.slideshare.net/benelog/ss-35627826","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"redirect","slug":"redirect","permalink":"https://taetaetae.github.io/tags/redirect/"},{"name":"out of memory","slug":"out-of-memory","permalink":"https://taetaetae.github.io/tags/out-of-memory/"},{"name":"heap dump","slug":"heap-dump","permalink":"https://taetaetae.github.io/tags/heap-dump/"}]},{"title":"천만 명의 사용자에게 1분 내로 알림 보내기 (병렬프로세스의 최적화)","slug":"faster-parallel-processes","date":"2019-01-02T03:43:44.000Z","updated":"2020-04-23T04:41:36.734Z","comments":true,"path":"2019/01/02/faster-parallel-processes/","link":"","permalink":"https://taetaetae.github.io/2019/01/02/faster-parallel-processes/","excerpt":"만약 1번부터 10번까지 번호표가 있는 사람들 총 열명에게 혼자서 동일한 내용의 메일을 보낸다고 가정해보자. 그리고 메일 발송시 한번에 한명에게만 보내야 하는 제한사항이 있을때 과연 당신은 어떤식으로 보내겠는가? 이어서 읽지말고 한번 생각해보자.","text":"만약 1번부터 10번까지 번호표가 있는 사람들 총 열명에게 혼자서 동일한 내용의 메일을 보낸다고 가정해보자. 그리고 메일 발송시 한번에 한명에게만 보내야 하는 제한사항이 있을때 과연 당신은 어떤식으로 보내겠는가? 이어서 읽지말고 한번 생각해보자.아무것도 고려하지 않고 단순하게 생각한다면 1번 보내고 &gt; 2번 보내고 … 9번 보내고 &gt; 10번 보내는 방법이 먼저 떠오르게 된다. (for loop 1 to 10 … ) 하지만 보내야 할 사람들이 많아져서 백명, 천명 많게는 천만명에게 보내야 할 경우 방금과 같은 순차적인 방법을 사용하면 너무 늦게 발송된다는건 코드를 작성하지 않아도 알 수있는 문제… 그렇다면 어떤 방법으로 보내야 보다 빨리 보낼수 있을까?이번 포스팅에서는 필자가 운영하고 있는 서비스에서 기존에 있던 병렬프로세스를 어떤식으로 최적화 했는지, 그래서 결국 얼마나 빨라졌는지에 대한 과정을 정리해 보고자 한다. 비단 메일 발송이나 앱 푸시 등 특정 도메인에 국한되지는 않고 전반적인 프로세스에 대해 이해를 한다면 다른 곳에서도 비슷한 방법으로 활용할 수 있을꺼라 기대 해본다. # 상황파악 및 목표(원할한 이해를 돕기 위하여) 먼저 필자가 운영하고있는 서비스를 간략히 소개부터 해야겠다. (그렇다고 필자 혼자 다 하는건 아님^^;…)셀럽의 방송이 시작되면 구독한 사용자에게 각 모바일 기기에 설치되어있는 앱으로 알림을 보내어 예정에 없던 깜짝 라이브 방송이나 VOD 영상 오픈을 보다 빠르게 확인할 수 있도록 제공하고 있다.여기서, 알림이 늦게 발송되면 셀럽은 방송을 시작하고 팬들이 들어오기까지 기다려야 한다거나 반대로 팬들은 방송 시작하고 뒤늦게 방송을 보게되는 불편함이 생기게 된다. 그리고 중복으로 알림이 발송되거나 특정 사용자들에게 발송이 누락되면 안 되는 등 “알림” 이란 기능은 서비스에 있어서 중요한 기능 중에 하나라고 할수 있다. 여기서 “발송 시간”은 처음 발송작업 시작부터 마지막 사용자에 대해 사내 발송 플랫폼으로 발송 요청을 하기까지의 시간을 의미 그리고 “채널” 이라는 샐럽단위의 그룹이 있는데 영상과 채널의 관계는 1:N이다. 즉, 하나의 영상을 여러 채널에 연결시킬수 있어서 하나의 영상에 대해 여러 채널들에게 연결을 시켜놓으면 채널을 구독하고있는 각각의 사용자에게 모두 알림을 발송 할수가 있게 된다. 우선, 알람이 사용자에게 전달되기까지의 큰 흐름은 다음과 같다. 알림 프로세스알림 프로세스 서비스에서 보낼 대상과 보낼 정보를 조합하여 사내 푸시 발송 플랫폼인 사내 발송 플랫폼에게 전달을 하면 플랫폼에 따라 발송이 되고 최종적으로는 사용자의 모바일 기기에 노출이 됨 간단하게 “병렬로 발송하면 되지 않을까?”라는 필자의 생각이 부끄러워질 정도로 이미 redis, rabbitMQ 를 활용해서 아래 그림처럼 병렬 프로세스로 구성되어 있었다. 기존 구조기존 구조 라이브가 시작되거나 VOD가 오픈될 경우 api가 호출이 되고 다시 배치 서버에게 영상의 고유번호를 전달 전달받은 영상의 고유번호를 rabbitMQ의 수신자 조회 Queue에 produce 수신자 조회 Queue의 consumer인 수신자 조회 모듈에서 영상의 고유번호를 consume 후 아래 작업을 진행3-1. 영상:채널 은 1:N 구조이기 때문에 여러 채널의 사용자들에게 알림을 발송할 수 있고, 영상에 연결된 채널들의 user를 db에서 가져온다.3-2. 가져온 user를 (중복으로 알림이 발송되지 않기 위해) java set에 담고 모든 채널을 조회했다면 redis에 sorted set으로 담는다.3-3. 적당한 크기로 분할하고 이 분할정보를 발송 Queue에 produce 발송 모듈에서 분할 정보를 consume 하고 아래 작업을 진행 (병렬처리)4-1. redis 에서 user 모음을 가져오고4-2. 조회한 user에 해당하는 deviceId를 db에서 가져옴 deviceId와 컨텐츠 정보를 활용하여 적절한 payload를 구성 후 사내 발송 플랫폼 에게 전달 기존 구조에서 발송 시간은 서비스에서 구독자 수가 가장 많은 채널 기준으로 약 1.1천만 명에게 최종 11분 정도 소요되고 있었다. (맨 처음에 이야기 한 순차적인 방법이였다면… 훨씬더 오래 걸렸을꺼라 예상해본다…) 기존에 구성하셨던 분들도 수많은 시행착오와 고민을 하시며 구성하셨을 텐데 더 이상 어떻게 더 빠르게 보낼 수 있을까 하는 부담감과 자칫 알림이 잘못 발송되기라도 한다면(장애가 발생한다면) 그 수많은 사용자들의 불만 화살 과녁이 필자가 되어야 한다는 압박감이 개선 시작 전부터 머릿속을 휘감고 있었던 찰나에 답정너답정너 라는 불가능할 것만 같은 목표가 (답정너 마냥) 정해지며 그렇게 푸시 개선 프로젝트가 시작되었다. 결국 사내 발송 플랫폼에게 얼마나 더 빨리 보낼수 있는가 가 개선 포인트 라고 할수 있겠다. # 1차 개선 : AsyncRestTemplate 적용사내 발송 플랫폼에 요청을 한 뒤 응답의 종류(성공/실패)에 따라 발송 시간 로깅만 하기 때문에 응답을 기다리지 않고 AsyncRestTemplate를 사용해서 비동기 호출로 변경하였다. 사내 발송 플랫폼에 요청에 따른 응답은 1~2초 내외였지만 발송 대상이 많을수록 기다리는 시간을 모아보면 무시 못 할 시간이었기 때문이다. 구조가 크게 변경된 건 없었고 발송하는 부분에서 약간의 로직만 변경하였는데 나름의 큰 효과를 볼 수 있었다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 첫발송 약 6분 약 6분 마지막 발송 약 11분 약 7분 # 2차 개선 : 발송대상 구하는 즉시 병렬 발송처리, 불필요 프로세스 제거 발송대상 구하는 즉시 병렬 발송처리기존 구조에서는 발송 대상을 전부다 구한 뒤에 발송이 시작되었다. 왜냐하면 영상에 연결된 채널이 여러 개가 될 수 있다 보니 중복 사용자 제거를 해야 하기 때문이었다. 예로 들어 영상 하나에 A, B, C 채널이 연결되어있고 어느 사용자가 A, B 채널을 구독하고 있는 상황에서 중복제거를 하지 않고 보낸다면 해당 사용자는 같은 내용의 알림을 두 번 받는 상황이 된다.영상에 연결된 채널이 한 개라면 문제가 없지만 두 개 이상일 경우부터 중복알림 문제가 발생했기 때문에, 그리고 이 중복제거 프로세스가 다 되어야지만 첫 번째 발송이 되는 구조였기 때문에 어떻게든 다른 방법으로 중복 발송을 해결해야만 했다.그래서 여러 시행착오 끝에 결정된 방법은 “이 사용자는 발송이 되었다”라는 정보를 redis에 담는 식으로 중복체크하는 방법을 바꾸는 것이었다. 또한 첫 번째 채널(구독자 수가 가장 많은 채널)은 중복체크를 할 필요가 없기 때문에 db에서 조회하는 즉시 발송해서 방송 시작 1초 내에 사용자에게 알림을 발송할 수가 있었다. 불필요 프로세스 제거발송 triggering 을 배치(jenkins)에서 하고 있었다. api에서 jenkins remote api로 호출이 되면 기본적으로 약 20~30초가량의 인스턴스 구동시간이 존재하게 되는데 이 시간 또한 불필요한 프로세스라고 생각되어 api가 바로 수신자 조회 Queue에 produce 하는 식으로 구조를 변경하였다. 2차 개선2차 개선 api에서 바로 수신자 조회 분할 Queue로 produce 수신자 조회 분할 모듈에서 consume을 하고 적당한 크기로 start index, end index를 구분하여 다시 수신자 조회 Queue로 produce 수신자 조회 모듈이 병렬로 consume을 하며 아래 작업을 수행합니다.3-1. 발송 대상 user를 db에서 가져옴3-2. 첫 번째 채널일 경우(구독자 수가 가장 많은 채널) 중복제거키에 담고 발송대상 key에 담은 뒤 발송 Queue에 produce3-3. 첫 번째 채널이 아닐 경우 중복제거를 해야 하기 때문에 중복제거키에서 redis의 zscore 연산 (시간 복잡도 O(1) )을 활용하여 발송되지 않은 user만 간추려서 발송 대상 key에 담은 뒤 발송 Queue에 produce 기존과 동일 이렇게 개선한 결과 사용자들이 방송이 시작되자마자 알림을 받기 시작할 수 있었고, 발송 대상을 구하자마자 발송하기 때문에 발송 속도도 개선이 됬음을 확인할 수 있었다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 약 11,120,000명 첫발송 약 6분 약 6분 약 1초 마지막 발송 약 11분 약 7분 약 5분 30초 # 3차 개선 : 발송대상 병렬x병렬조회, redis 파티셔닝, 채널간의 발송 타이밍 해소 발송대상 병렬x병렬조회몇 차례 속도 개선을 하는 필자를 보고 팀원 분들이 짠하게(?) 느끼셨는지 아이디어를 하나 건네주셨다. 그건 바로 db에서 user를 조회할 때 병렬 조회하는 것을 다시 병렬 조회하는 것.db에 채널별 구독자 테이블에는 user가 오름차순으로 정렬되어 있다 보니 큰 단위로 나눌수가 있고, 다시 이를 작은 단위로 분할하여 조회가 가능했던 것이었다. 대신, 나누는 단위가 적당해야 하고(테스트를 통해서 찾아내야…) user가 꽉 찬(?) 그룹이 있는가 반면 비어있는 그룹이 있을 수가 있다. 그림으로 그려보면 다음과 같다. 1단계 : 첫 번째 user가 1, 마지막 user가 300만이라고 가정할 때 큰 단위(10만)로 분할합니다.예 ) 0~100,000 / 100,000~200,000 / … / 2,900,000~3,000,000 2단계 (병렬) : 1단계에서 나눈 단위를 다시 작은 단위(1,000)로 분할하여 db에서 조회를 하고 그다음 단계를 진행합니다.예) 0~1,000 / 1,000~2,000 / … / 99,000~100,000 이렇게 하고서 반영을 해보니 속도가 빨라진 대신 redis 가 부하를 많이 받게 되어 다른 모듈에서 redis 를 사용하는 곳에서 지연이 발생하게 되었습니다. 모니터링 툴인 pinpoint에 롯데타워가 뙇.. 결국 알림 속도를 빠르게 한답시고 서비스 전체가 사용하는 공용 redis에 지연이 발생하게 되어버린 것이었다. 개선을 함에 있어 서비스 영향도를 리스트업 하고, 조금이라도 문제가 생길것 같은 부분을 고려해야 하는 교훈을 얻을수 있었다. redis 파티셔닝알림 발송만을 위한 별도 redis 클러스터를 구축하기에는 장비 발급부터 간단한 작업이 아니었기에 어떻게든 로직에서 해결점을 찾아야 했다. 고민의 고민을 한 결과 redis 는 Single thread 방식으로 처리하기 때문에 key 하나에 연산이 끝날 때까지 해당 key가 속한 redis는 다른 연산을 처리할 수가 없게 되는 부분을 인지하고 중복 발송을 막기 위한 redis 키를 기존에는 하나를 사용하고 있었는데 이를 user 값 기준으로 여러 개의 키로 파티셔닝 하게 되었다.즉, user가 천만 개라고 가정했을 때 기존에는 한 개의 키에 천만 개가 들어가던 구조에서 user 값을 10,000으로 나누어 결과적으로는 하나의 키에 1,000개씩 총 10,000개의 키에 파티셔닝되어 들어가게 되는 구조로 변경하게 되었다. 그랬더니 발송 속도도 더 빨라지고 pinpoint에 응답 그래프도 전혀 문제가 없는 수치인 것을 확인할 수 있었다. 채널간의 발송 타이밍 해소여러 채널을 동시에 보내다 보니 아주 간헐적으로 중복 알림이 발생하게 되었다. 이유는 지금까지 프로세스를 보면 여러 단계의 병렬 프로세스가 있는데 각 프로세스별 순서 보장이 안되고 각자 진행되기 때문에 중복체크 키에 들어가기 전에 다른 프로세스에서 먼저 중복체크를 하고 발송을 해버리면 중복으로 발송이 되어버리던 것이었다. 간단히 그림으로 설명해보면… 위 그림에서 1,2,3,4,5 가 동시에 발송을 시작한다고 가정했을 때 그 다음은 2번이 먼저 진행될 수도 있고 5번이 먼저 진행될 수도 있게 된니다. 그렇기 때문에 매번 중복 알림이 발생되는 건 아니었지만 아주아주 간헐적으로 발생하게 되었다.이 문제는 간단히 채널별로 병렬 조회 하는 부분에서, 1초마다 발송 대상수(redis 를 활용하여 로깅 목적으로 발송 대상수를 트래킹하고 있다.)가 변하지 않을 경우 한 채널에 대해 발송이 완료되었다고 간주하고 그 다음 채널을 발송하는 방법으로 해결하였다. 이렇게 거듭된 개선을 거쳐 정리된 최종 구조는 다음과 같다. 3차 개선. a.k.a. 최종 구조3차 개선. a.k.a. 최종 구조 채널별로 큰 단위로 index를 파티셔닝 하여 병렬 조회할 수 있도록 한다.1-1. 첫 consumer는 채널을 채널 간의 알림 발송 진행을 담당해주고,1-2. redis에 발송 대상수가 변함이 없을 경우 다음 채널을 발송하도록 한다. 1에서보다 더 작은 단위로 파티셔닝하여 아래 작업을 수행한다.2-1. db에서 user를 조회하고2-2. user를 중복제거 key에 10만단위로 파티셔닝 하여 담는다. ex ) user가 105872 인경우 push:overlapCheck:100000 에, user가 3409572 인 경우 push:overlapCheck:34000002-3. 2-1에서 가져온 user를 임의 redis key에 담는다. 중복체크 작업을 수행합니다.3-1. 첫 번째 채널일 경우 중복체크를 하지 않고 바로 발송을 한다.3-2. 첫 번째 채널이 아닐 경우 2-3에서 저장한 redis key의 값을 조회하여 2-2에서 저장한 중복제거 key에 있는지 확인 후 발송 여부를 결정한다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 약 11,120,000명 약 11,240,000명 첫발송 약 6분 약 6분 약 1초 약 1초 마지막 발송 약 11분 약 7분 약 5분 30초 51초 # 마치며결국 처음에 개선 프로젝트 시작 시 정해졌던 목표에 도달할 수 있었다.(발송 대상이 약 100만 명이 더 늘었지만 1분내로 발송 성공)또한 무조건 좋다고 사용하다간 오히려 독이 될 수 있고, 반대로 돌아가는 원리를 잘 알아보고 사용한다면 본인이 원하는 가장 이상적인 결과를 만들 수 있다는 좋은 경험을 얻을 수 있었다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"parallel precess","slug":"parallel-precess","permalink":"https://taetaetae.github.io/tags/parallel-precess/"},{"name":"redis","slug":"redis","permalink":"https://taetaetae.github.io/tags/redis/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"https://taetaetae.github.io/tags/rabbitMQ/"}]},{"title":"2018 회고 - Coder가 아닌 Programmer로","slug":"review-2018","date":"2018-12-31T12:33:29.000Z","updated":"2020-04-23T04:41:36.836Z","comments":true,"path":"2018/12/31/review-2018/","link":"","permalink":"https://taetaetae.github.io/2018/12/31/review-2018/","excerpt":"매사에 행동하는 모든것들의 끝자락에서는 그동안 잘한것과 못한것을 다시 생각하며 잘한것은 보다 더 잘할수 있도록 하고 못한것은 왜 못했는지 그리고 어떻게 하면 못한 부분을 고칠수 있을지에 대한 시간을 갖으려고 노력해왔다. 그게 개발이 되었든 게임이 되었든 연인과의 데이트가 되었든 뭐든지.","text":"매사에 행동하는 모든것들의 끝자락에서는 그동안 잘한것과 못한것을 다시 생각하며 잘한것은 보다 더 잘할수 있도록 하고 못한것은 왜 못했는지 그리고 어떻게 하면 못한 부분을 고칠수 있을지에 대한 시간을 갖으려고 노력해왔다. 그게 개발이 되었든 게임이 되었든 연인과의 데이트가 되었든 뭐든지. 이러한 시간들은 필자에게 큰 인사이트를 얻을 수 있게 되었고 지난 한해를 돌이켜 보자면 개인적으로 계획한 전부를 다 이뤄내지는 못했지만 나름의 많은 경험과 성과를 달성했다고 생각해본다.이제 몇시간 뒤면 올해가 끝나고 새로운 한 해가 시작되는 이 시점에 개발자로써의 회고를 해보며 2018년 정리 및 2019년 목표를 다짐해보자. # 글쓰는 개발자가 되자. 개인 블로그 운영아주 오래전, 동기 형을 통해 개발자가 글을 써야하는 중요성에 대해 절실하게 배우게 되었고 그때부터 블로그를 운영하기 시작하였다. 그 동기형의 말에 조금 더 내 생각을 첨가하자면 글을 쓰다보면 누군가 내 글을 본다는 마음에 내가 알고있는 지식을 보다 더 깊게 공부하게 되고 그것들이 모여 내 개발 히스토리가 만들어 지며 포트폴리오 등 다양하게 활용할 수 있기에 블로그를 운영하는건 정말 좋은 선택지 였던것 같다. 실제로 그냥 구글링 해서 알게된 것과는 또 다른 배움이 있었기 때문이다.회사 일 그리고 개인 공부를 하면서 적어도 한달에 한가지 이상은 배우게 되기 때문에 올해 초 한달에 한개 이상의 글을 쓰기로 결심하였다.(그 달의 글이 없다면 뭔가 놀았거나(?) 미친듯이 바빴거나 아니면 게을렀거나…) 블로그에 글을 쓴 내역을 그래프로 시각화 해보면 아래처럼 총 23개의 글을 작성하였고 월 평균 1.9개의 글을 작성하게 된것을 볼수 있다. 9월달엔 팀 옮기자마자 엄청 바빴고, 11월엔 그 바쁜게 결실을 맺는 시간… 이라 핑계를… (나중에 블로깅 예정, 병렬 프로그래밍 관련) 월별 글 작성수월별 글 작성수 위 결과만을 두고 봤을땐 많으면 많고 적으면 적다고 할 수 있는 결과지만 개인적으로는 자투리 시간을 활용해서 그간 배웠던것, 그리고 경험했지만 내것으로 만들지 못하고 보기만 하며 넘어간것들에 대해 귀찮지만 시간을 투자하고 정리했더라면 더 많은 글을 썼을것 같다는 조금 아쉬운 결과라고 생각이 든다. 주 단위 PV, 누군가 내 글을 보고 있다는것에 뿌듯함주 단위 PV, 누군가 내 글을 보고 있다는것에 뿌듯함 나름 열심히 글을 쓴 결과일까, GA를 통해 본 필자의 블로그에 유입량이 점점 늘어나는것을 보며 하나를 쓰더라도 좀더 자세히 독자의 입장에서 써야겠다고 다시한번 다짐하게 된다. 다만 글을 “많이” 쓰는것보다 하나를 작성하더라도 원인과 근거를 들어가며 문제를 정확히 파악하는데 집중을 해야하고, 단순 사용법 나열이 아닌 실제로 경험을 해가면서 “내것”으로 만드는 과정이 필요하겠다. # 회사 팀 변경 그리고 토이 프로젝트기존에 아무것도 없던 환경에서 서버 발급부터 이런 저런 서비스에 도움이 되는 다양한 모니터링 툴을 개발하며 무사히 서비스를 오픈을 하였고, 약간의 매너리즘이 생겨날 즈음 좋은 기회가 생겨 성격이 전혀 다른 서비스를 하는 팀을 옮기게 되었다. 약간 이직과도 비슷한 느낌으로 팀을 옮기게 되었는데 처음엔 새로운 지식을 습득해야 하는 두려움도 있었고 기존 서비스에 애정이 많아서 고민이 많았지만 벌써 옮긴지 5개월이 지나고 돌이켜보면 올해 가장 잘한 일 중 하나가 아닐까하는 생각이 든다. 전 팀에선 서비스를 운영하는데 그쳤지만 지금 내가 있는 곳은 대용량 서비스를 성능측면에서, 그리고 아키텍쳐 측면에서 보다 효율적으로 개발하는데 집중을 하려는 모습들이 보이기 때문이다. 더불어 팀에 투입되자마자 필자 홀로 기존에 있던 병렬 프로세스를 개선하여 서비스적으로 약 90%의 개선효과를 볼수있었는데 이 부분은 추후 포스팅 할 예정이다.그리고 팀을 옮기기 한두달 전 개인적인 여유시간이 많이 있었고, 다른사람들의 블로그를 보며 챙겨보고 싶은 마음에 토이 프로젝트를 만들게 되었다. 7월 중순부터 시작했으니 이것도 어느덧 반년이 지나고 있는데 운영을 해가면서 기능을 추가하기 위해 종종 밤을 새는 등 올 한해있어 꽤 많은것을 얻을수 있었던 시간이였다. 간혹 버그가 생겨 메일이 발송 안되면 지인 또는 모르는 분들이 메일로 제보도 해주시고 … 색다른 경험이였다. 자세한 내용 및 후기는 개발후기-1 과 개발후기-2에서 확인 가능하다. (어서 3편을 쓰고 마무리를 지어야 할텐데…) 그리고 최근에는 아카이빙 기능을 만들어 과거 글을 조회할수 있도록 만들었는데 2% 부족한 느낌이다… (맘같아서는 형태소 분석을 해서 자동 필터링도 해보고 싶은데…) 점점 늘어가는 구독자수, AWS 프리티어가 끝나기 전에 뭔가 방법을 찾아야 하는데 ...점점 늘어가는 구독자수, AWS 프리티어가 끝나기 전에 뭔가 방법을 찾아야 하는데 ... # 공유 및 발표3월 즈음 POP it 관리자분께서 회사까지 직접 찾아와 주셔서 만남을 갖고 POP it 저자활동을 시작하게 된다. 그리고 비슷한 시점 D2 Hello World 담당자의 제안으로 이전 팀에서 활용했었던 기술에 대해 기고를 하는 영광을 얻게되고 (내 서버에는 누가 들어오는 걸까 - Apache 액세스 로그를 Elastic Stack으로 분석하기, 여러차례 각종 개발 관련 행사에 참여하며 “난 언제쯤 저런 발표를 할수 있을까?” 하는 부러움이 무엇때문인지 “나도 할수있다”는 자신감으로 변화되어 2018 Pycon 행사에서 짧은 5분이였지만 급작스럽게 필자가 만들었던 토이 프로젝트에 대해 약 100~200여명 앞에서 간단히 소개하는 발표를 하게된다. (Pycon 라이트닝토크) 작년까지만 해도 전혀 생각하지도 못할 외부활동을 정말 다양하게, 그리고 두려움을 떨쳐내고 회사라는 울타리를 벗어나 바깥세상을 바라볼수 있는 눈을 얻는 좋은 기회였다고 생각이 든다. # 결론, 그래서 내년엔?숨가쁘게 달려온 2018년. 올해는 무엇보다 외부활동을 많이 하면서 기존에 갖고있던 주니어로써의 개발 마인드를 조금이나마 벗어나며 주니어와 시니어 사이의 포지션으로 한발자국 올라선 기분이다. 예전에는 회사내에 팀장님이나 선배 개발자분들이 시킨일을 하며 감을 받아 먹었다면, 지금은 그 감을 어떻게 따먹는지, 어떤 감이 더 맛있고 어떻게 따먹어야 더 효율적인지 스스로 일어서는 방법에 첫 단추를 낀것 같아 한편으로는 마음이 무겁지만 한편으로는 새로움을 경험하고 배운다는 것에 벅차오르기까지 한다. 또한 기존에는 일반적인 Web Framework인 Spring 만을 가지고 CRUD에 고심했다면 ElasticStack, kafka, RabbitMQ, Redis 등 새로운 기술들을 배우기 시작하면서 새로운것에 대한 두려움 보다는 호기심이 더 커서 스펀지마냥 습득할수 있었던것 같다.새해계획이라면 거창하게 들릴지 모르겠지만 다가오는 2019년엔 Coder 가 아닌 Programmer 가 되고 싶다. (관련 좋은 글) 막연하게 들릴지 모르겠지만 회사원이 아닌 개발자로써 나를 발전시키고 공유하며 서로 성장해 가는, 골을 직접 넣진 않지만 그 과정을 빌드업 하는 미드필더같은 역활을 할수있는 개발자가 되고 싶다. 이러한 계획을 달성하기 위해서는 내년에도 올 2018년을 계속 회고해가면서 잘못된 점을 고쳐나가고 잘한점을 상기하며 개발에 임해야 하지 않을까 싶다.2018년, 고생했다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"}]},{"title":"엘라스틱서치 12월 서울 밋업 후기","slug":"elastic-meetup-201812","date":"2018-12-13T13:27:02.000Z","updated":"2020-04-23T04:41:36.726Z","comments":true,"path":"2018/12/13/elastic-meetup-201812/","link":"","permalink":"https://taetaetae.github.io/2018/12/13/elastic-meetup-201812/","excerpt":"엘라스틱을 처음 접하게 된 건 2017년 여름 facebook 피드에 “Elastic Stack을 이용한 서울시 지하철 대시보드” 라는 링크를 보게 된 것부터인 것 같다. 그 당시 데이터 분석 및 자동화에 관심이 커지고 있던 찰나였는데","text":"엘라스틱을 처음 접하게 된 건 2017년 여름 facebook 피드에 “Elastic Stack을 이용한 서울시 지하철 대시보드” 라는 링크를 보게 된 것부터인 것 같다. 그 당시 데이터 분석 및 자동화에 관심이 커지고 있던 찰나였는데 키바나로 간단하면서도 아주 멋진 대시보드를 그릴 수 있다는 게 너무 흥미롭게 다가왔고 거기다 실시간으로 볼수 있다는 점에 공부를 시작하지 않을 수 없었다. 그렇게 이것저것 만들어 보기도 하고 한국 엘라스틱서치 커뮤니티 활동을 해오던 찰나 (최근들어 눈팅만 하고 있지만…) 올해 마지막 밋업을 한다고 하여 참여하게 되었다. # 여기어때 본사 방문강남에 위치한 여기어때 본사에서 밋업을 하게 되어 덕분에 다른 회사 구경을 할 수 있게 되었다. 예전 다른 IT 스타트업 밋업 행사에서도 느꼈던 부분인데 엄청나게 큰 시설은 아니지만 아기자기하게 회사의 색깔과 특징을 잘 살려놓은 인테리어가 인상적이었다. 그런데 생각보다 사람이 너무~ 많이 와서 약간 집중이 안 될것 같았지만 다행히도 자리를 잘 잡아서 세션을 듣는 데는 무리가 없었다. (정확하진 않지만 참석하신 분들 중의 절반 정도만 강의장에 들어오고 나머지는 밖에서 듣는 걸 보고 이런 IT 행사의 인기를 다시 한번 실감할 수 있었다.)여기어때 본사건물에서 엘라스틱 밋업을!여기어때 본사건물에서 엘라스틱 밋업을! # 엘라스틱서치 6.5 최신버전 소개 및 커뮤니티 회고행사 처음 세션으로 김종민 커뮤니티 엔지니어 분께서 엘라스틱의 최근 업데이트 정보와 커뮤니티 활동에 대해서 회고해주셨다. 내가 처음 엘라스틱서치를 접한 버전이 2.4였는데 벌써 6.5라니… 빨라도 너무 빠르다. 이번 버전에서는 한 클러스터에서 다른 클러스터로의 인덱스를 복제하는 방법인 Cross-cluster replication (클러스터 복제) 기능이 추가되었고 ODBC Client 추가, 자바 11지원 등 여러 가지 기능이 추가되었다고 한다.특히 키바나에서는 파일을 업로드하면 자동으로 분석해서 인덱싱을 해주는 기능도 생겼고 (파일 크기가 100메가 제한이라는게 살짝 아쉽긴 했다.) 캔버스, 스페이스 등 역시 키바나 라는 생각이 들 정도로 비주얼라이징을 한번더 업그레이드 한듯 하다. (다 사용할 수 있을까 하는 정도로… 엘라스틱 스택을 들어보기만 하던 함께 참석한 동기 녀석도 당장 해보겠다고 할 정도로…)다른 자세한 내용은 여기서 확인이 가능하다.너무나 빠른 버전업과 너무나 발빠르게 움직이는 사람들너무나 빠른 버전업과 너무나 발빠르게 움직이는 사람들 # 엘라스틱서치 활용사례스마일게이트 및 여기어때 에서 엘라스틱 서치를 활용한 사례를 발표해 주셨다. 하지만 아쉽게도 필자는 5.6 버전까지밖에 사용한 게 전부여서인지(그것도 일부 기능만) 전체 발표 내용을 다 이해를 하진 못했지만 구축하면서 생긴 문제나 삽질 경험담을 공유해주셔서 간접적으로라도 그때의 현장감(?)을 느낄 수 있어 좋았고, 한편으로 여태까지 나름 엘라스틱서치를 만져봤다고 약간의 자신감 반 자만심 반으로 생각했었는데 역시 세상엔 고수가 많구나 하며 다시 분발해야겠다고 다짐했다.스마일게이트 + 여기어때스마일게이트 + 여기어때 # 마치며커뮤니티 활동 회고 시간에 누가 페이스북 커뮤니티에서 “공유”라는 단어를 사용해서 게시글을 작성했는지 키바나로 보여주고 밋업에 온 사람이 있다면 5만원 여기어때 쿠폰을 준다고 했었다. 마침 키바나 대시보드 한쪽 구석에 필자의 이름이 보였지만 (예전에 나름 활발하게 질문도 하고 공유도 했던 적이 있어서…) 쿠폰을 받는구나 하며 기대를 하고 있었지만 아쉽게도 최근에 작성한 몇 분에게만 선물이 돌아갔다… 하지만 그 아쉬움도 잠시, 무작위로 추첨하여 또 쿠폰을 준다고 했는데 당첨이 되어서ㅎㅎ 감사하게도 쿠폰을 받는 기쁨을 누릴 수 있었다!! 역시 밋업의 마무리는 굿즈모음이지(?)역시 밋업의 마무리는 굿즈모음이지(?) 매번 이런 IT밋업에 참가 신청을 하고 참석하기 전에는 “아 귀찮다. 취소할까. 날도 추운데. 피곤한데” 하며 가기 싫었지만 막상 와보면 생각보다 많은 것을 배워가고 얻어 간다. (쿠폰을 받아서가 아니라…) 세션에 발표하시는 분들, 그리고 그 발표를 듣는 참석하신 분들의 눈동자에서 배움에는 끝이 없고 배워야 살아남는다는 걸 (특히 IT직군은 더…) 다시 한번 느끼고 생각할 수 있었던 좋은 시간이었다. 내년엔 엘라스틱으로 뭘 만들어 볼까! 새로워진 기능들 + 삽질 경험담을 내 것으로 만들어 보자!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"}]},{"title":"Jenkins 설치 치트키","slug":"jenkins-install","date":"2018-12-01T19:37:59.000Z","updated":"2020-04-23T04:41:36.774Z","comments":true,"path":"2018/12/02/jenkins-install/","link":"","permalink":"https://taetaetae.github.io/2018/12/02/jenkins-install/","excerpt":"“show me the money”, “black sheep wall”.어렸을적 스타크래프트라는 게임이 나오고서 입에 달고 살았던 치트키. 게임이 시작되고 해당 치트키를 입력하면 돈이 들어오거나 맵이 훤하게 보여 컴퓨터를 이기는데 도움을 주곤 했었다.","text":"“show me the money”, “black sheep wall”.어렸을적 스타크래프트라는 게임이 나오고서 입에 달고 살았던 치트키. 게임이 시작되고 해당 치트키를 입력하면 돈이 들어오거나 맵이 훤하게 보여 컴퓨터를 이기는데 도움을 주곤 했었다. 개발을 하면서 Jenkins는 나 대신 어떤 업무를 수행하는데 강력한 툴 중에 하나이다. (물론 만능이라는 소리는 아니지만…) 새로운 프로젝트가 시작되거나 개발도중 무언가 자동화를 하고 싶을 경우엔 Jenkins를 찾게 되는데 그럴때마다 설치를 하고 이런저런 설정이 필요하다.눈치를 챘을수도 있지만 이 포스트는 오로지 젠킨스 설치하는 방법을 아주 간단하고 핵심만 정리하고자 한다. 마치 치트키처럼.나중에 다시 보기위해 + 누군가 해당 포스트를 보고 도움이 되었으면 하는 바람으로. (물론 이 방법밖에 있는건 아니지만 필자는 아래와 방법을 사용하고 있다.) 우선 CentOS 환경에 Java가 설치되어 있는 상황이라 가정한다. 적당한 위치에 tomcat 다운 ( https://tomcat.apache.org/download-80.cgi ) 1wget &#123;압축파일 다운경로, 필자는 apache-tomcat-8.5.35 &#125; 압축 해제후 하위 폴더중 webapps로 이동 12tar -zxvf apache-tomcat-8.5.35.tar.gzcd apache-tomcat-8.5.35/webapps Jenkins 다운 ( https://jenkins.io/download/ ) 1wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war tomcat 하위폴더중 conf 폴더로 이동 1cd ../conf server.xml 수정 및 http port 확인 1234567vi server.xml&lt;Host&gt; 하위에 추가&lt;Context path=&quot;/jenkins&quot; debug=&quot;0&quot; privileged=&quot;true&quot; docBase=&quot;jenkins.war&quot; /&gt;port 확인&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;/&gt; 해당 서버의 ip와 위 port에 맞춰 url 입력후 jenkins 설치 1http://ip:8080/jenkins","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"Jenkins에서 파이썬 출력을 실시간으로 보고싶다면?","slug":"python-buffer","date":"2018-12-01T16:40:11.000Z","updated":"2020-04-23T04:41:36.825Z","comments":true,"path":"2018/12/02/python-buffer/","link":"","permalink":"https://taetaetae.github.io/2018/12/02/python-buffer/","excerpt":"필자가 운영하고 있는 Daily Dev Blog 라는 서비스는 매일 동일한 시간에 주기적으로 데이터를 크롤링 하고 사용자에게 메일을 발송하는 일련의 작업을 수행하고 있다. 헌데 예상하지 못한 부분에서 예외가 발생하게 되면 어떤경우는 메일 발송을 못한다거나 기존에 발송했던 데이터를 다시 보내는 등 정상적이지 못한 상황을 맞이하게 된다.","text":"필자가 운영하고 있는 Daily Dev Blog 라는 서비스는 매일 동일한 시간에 주기적으로 데이터를 크롤링 하고 사용자에게 메일을 발송하는 일련의 작업을 수행하고 있다. 헌데 예상하지 못한 부분에서 예외가 발생하게 되면 어떤경우는 메일 발송을 못한다거나 기존에 발송했던 데이터를 다시 보내는 등 정상적이지 못한 상황을 맞이하게 된다.메일이 하루라도 잘못오면 여기저기서 연락이 온다. 감사한 분들...메일이 하루라도 잘못오면 여기저기서 연락이 온다. 감사한 분들...이런저런 바쁜일들로 차일피일 미루다 마침 여유가 생겨 기존에는 Crontab 스케쥴로 파이썬 스크립트를 실행하던 것에서 Jenkins로 옮기는 작업을 했다. 젠킨스가 스케쥴링을 해주고 실행이력을 보여주며, 실시간으로 스크립트가 돌아가는걸 볼수 있을것 같다는 기대감에서이다. 위에서 이야기 했던 예외상황을 보다 빠르고 편하게 실시간으로 디버깅을 하기 위해서가 가장 컸다. # 당연히 될거라고 생각했으나…작업은 간단할꺼라 생각했다. 우선 Jenkins를 설치하고 기존에 스크립트 파일을 Jenkins Job으로 옮긴후에 적당한 코드 중간중간에 디버깅이 용이하도록 로그를 출력하게 해둔다음 스케쥴링만 걸어두면 끝이라고 생각했다. 하지만, 이렇게 간단하게 끝날것만 같았던 작업이 은근 귀찮은 작업이 될줄이야. 디버깅을 위해 로그를 출력하도록 해놨는데 모든 스크립트가 끝이 나서야 해당 로그가 출력되는 것이였다. 로그를 실시간으로 볼수 없다면 Crontab에서 Jenkins로 옮기는 이유가 크게 없게 된다. 실제로 아래처럼 코드를 작성하고 Jenkins Job을 실행시켜보면 다 끝나고서야 출력이 되는걸 볼수 있었다. (1초에 한번씩 5초동안 로그를 찍는 간단한 코드다.) 123456789import timeprint('start')for second in range(0,5) : print(second) time.sleep(second)print('end') 스크립트가 다 끝나서야 출력을 볼수 있다ㅠ 실시간으로 디버깅이 어렵다.스크립트가 다 끝나서야 출력을 볼수 있다ㅠ 실시간으로 디버깅이 어렵다. # 그럼 어떻게 해야할까?개발을 하면서 만나는 대부분의 문제들은 누군가 과거에 경험했던 문제였고, 이미 해결된 문제일 확률이 상당히 높은것들이 많다. 이번에도 역시, 갓 스택 오버플로우 : https://stackoverflow.com/questions/107705/disable-output-buffering 위 링크에서 알려준것처럼 해보면 다음과 같이 로그가 출력되는대로 젠킨스에서 볼수 있게 된다.콘솔환경에서의 디버깅은 로깅이 최고!콘솔환경에서의 디버깅은 로깅이 최고! 정리해보면 다음과 같은 방법이 있겠다. Execute Python script 을 활용하여 Jenkins 에 직접 코드를 작성하는 경우 print의 flush옵션을 활용 ( https://docs.python.org/3/library/functions.html?highlight=print#print ) 1print('hello', flush=True) 매번 print 가 될때마다 flush가 되도록 재정의 123456789101112131415import sysclass Unbuffered(object): def __init__(self, stream): self.stream = stream def write(self, data): self.stream.write(data) self.stream.flush() def writelines(self, datas): self.stream.writelines(datas) self.stream.flush() def __getattr__(self, attr): return getattr(self.stream, attr)sys.stdout=Unbuffered(sys.stdout) Execute shell을 활용하여 특정경로의 Python 파일을 실행할 경우 -u 옵션을 줘서 실행시킨다. ( python -u python_module.py ) 이렇게 두고보면 너무 간단한 작업인데 이런 방법을 모르는 상황에서는 작성된 Python Script를 Shell Script로 다시 감싸보거나 Python 코드를 쓰지 말까 까지 생각했었다… 삽질의 연속들… (Shell Script로 작성하면 바로바로 보였기 때문…) 다시한번 모르면 몸이 고생한다(?)라는걸 몸소 체험한 좋은…시간이였다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"Deview 2018 리뷰 (Day 1, Day2)","slug":"deview-2018","date":"2018-10-14T09:26:26.000Z","updated":"2020-04-23T04:41:36.719Z","comments":true,"path":"2018/10/14/deview-2018/","link":"","permalink":"https://taetaetae.github.io/2018/10/14/deview-2018/","excerpt":"회사 내에서도 대학시절 수강신청마냥 1분도 안되서 마감될 정도로 관심이 많았던 DEVIEW 2018. 다행히 클릭신공으로 운좋게 신청에 성공하였고 팀에서도 바쁜 시기였지만 감사하게도 보내주셔서 올해는 이틀 모두 다녀올수 있게 되었다.","text":"회사 내에서도 대학시절 수강신청마냥 1분도 안되서 마감될 정도로 관심이 많았던 DEVIEW 2018. 다행히 클릭신공으로 운좋게 신청에 성공하였고 팀에서도 바쁜 시기였지만 감사하게도 보내주셔서 올해는 이틀 모두 다녀올수 있게 되었다. 예전에는 연차가 올라가면 DEVIEW행사는 참여 안하겠지~라는 생각이 있었는데 그때는 단순 호기심에 참석을 하고 싶었다면 이번에는 뭐라도 배워오자라는 마음으로 신입 시절보다 조금더 성숙한 마음가짐과 자세를 가지고 참석을 하게 되었다. 다시 생각해보면 호기심만으로 세션들을 듣고 부스에서 나눠주는 굿즈를 조금이라도 더 받아와야지 하고 생각했던 신입시절의 생각이 틀린건 아니였지만, 말 그대로 기술행사이니만큼 가급적이면 세션에서 발표하는 내용을 내것으로 만들고 실무에서 또는 다른곳에서 활용할수는 없을까 하는 생각을 갖는게 보다 성장하려는 개발자로서의 자세가 아닐까 생각이 든다. (라고 멋드러지게 말하지만 세션내용의 절반이라도 이해하면 다행이겠지…) # 행사 시작 그리고 키노트10초만에 마감되었다는 소리가 있을정도로 올해도 여전히 관심이 많았던 DEVIEW 2018. 코엑스에 도착하고 등록을 한뒤 이곳저곳 부스들을 구경하기 바빴다. 이번에는 지난번과 달리 거의 네이버 서비스가 60~70%를 자리잡고 있었고(파파고, 지도, 클로바, 글로벌 광고 등등) 일반 기업에서는 얼마 오지 않았다.(내 기억으로 5~6개?) 개인적으로 여러 다양한 회사들이 함께하는 기술행사가 되었으면 하는 바램이 있었지만 회사를 선정하는데, 그리고 기타 사정들이 있을꺼라는 아쉬움을 뒤로하고 CTO님이 발표하시는 키노트를 들으러 메인강의장에 들어갔다. (자칫… 이것도 네이버 독과점(?) 이러면 할말이 없는데…ㅠㅠ)송창현 네이버 CTO님의 keynote송창현 네이버 CTO님의 keynote작년에는 거의 로봇잔치로 느껴졌는데 올해는 그 기술들의 융합(?)잔치 로 받아들여졌다. Ambient Intelligence 를 강조하시며 기술의 진정한 가치는 기술이 생활속으로 사라졌을 때 나온다라는 명언같은 말씀도 해주셨다. 연결 : 사물, 상황, 위치인식, 이해 발견 : 적시에 답, 추천, 액션제공 그리고 그와 관련된 네이버 서비스를 공개 하셨는데, 네이버 지도 Map API를 무제한/무료로 사용할수 있게 된다고 한다. (박수 유도하심 ㅎㅎ) 또한 이번에 가장 크게 바뀌는 네이버 모바일 홈 페이지인 그린닷, 지도 기술들의 종합 플랫폼인 xDM Platform(측위, 지도, 내비), 그리고 자율주행과 로봇에 대해 연구결과 그리고 앞으로의 방향성에 대해 정리해주셨다. 집에 돌아와서 검색좀 하다보니 테크수다에서 벌써(?) 영상을 하나 올린게 있어 공유해본다. 키노트를 다 듣고 작년에는 그런가보다 하고 별생각이 안들었는데 올해는 저런 기술들이 서비스 레벨까지 가는데 이렇다할 허들없이 사용자들에게 보여질수만 있다면 개발자로서 보다 더 큰 자부심을 가지고 기술개발에 정진할텐데… 하는 씁슬한 생각을 해보게 되었다. (물론 이런 부분들도 다 사정이 있을꺼라 생각이 들지만 안타까운건 감출수가 없을것 같다.) 이틀에 걸쳐 이런저런 다양한 세션들을 들을수 있어 좋았는데 몇몇 세션들은 기본지식이 없어 (AI, 머신러닝 등…ㅠ) 이해하기 힘들었다. 내년엔 이해할수 있도록 준비를 해서 오자며 또다짐을 하고… 그나마 조금이라도 이해할수 있었던 세션들 몇개만 정리해본다. # React Native: 웹 개발자가 한 달 만에 앱 출시하기React Native: 웹 개발자가 한 달 만에 앱 출시하기React Native: 웹 개발자가 한 달 만에 앱 출시하기 지난팀에서 아주 잠깐 React를 경험해보긴 했지만 거의 hello world 수준이였기 때문에 이 세션 역시 이해가 잘 되지 못했다. 하지만 필자처럼 이해를 잘 못하는 사람도 발표자가 전달하려는 목적이 무엇인지 알수 있을 정도로 전체적인 흐름은 조금이나마 이해를 할수 있었고 특히 개발하면서 좋았던 것이나 경험담을 알려주며 삽질공유를 해주는게 듣기 좋았다. React Native 는 빠른개발을 할수있고 코드공유가 쉬우며 개선이 쉽다는 장점이 있다고 한다. 또한 단기간에 크로스 플랫폼을 만들어야 할때 사용한다고 하니 나중에 참고해봐도 좋을듯 싶다. 발표자료 [121]React Native: 웹 개발자가 한 달 만에 앱 출시하기 from NAVER D2 # LINE x NAVER 개발 보안 취약점 이야기LINE x NAVER 개발 보안 취약점 이야기LINE x NAVER 개발 보안 취약점 이야기 버그바운티라는 신기한(?)프로그램에 대한 소개와 운영에 대한 내용을 발표해 주셨다. 가끔 사내에서도 버그를 잡으면 포상을 드려요 라는 글이 올라왔었는데 그때마다 손안데고 코풀려나 하는 비뚤어진(?)생각을 갖곤 했었다. 하지만 듣고보니 해커를 고용하는 가장 좋은 방법이면서도 회사의 보안을 지키는 가장 좋은 방법이라고 한다. 또한 잘 알려진 왠만한 기업들은 버그바운티 프로그램을 운영하고 있다고 한다. 비뚤어진 생각을 다시 고쳐 생각해보면, 해커에게는 취약점을 찾으며 해커 본연의 업무를 더욱더 발전시킬수 있고, 회사로써는 수만가지의 취약점을 관리하는 별도의 팀을 운영하는 비용보다 이러한 버그바운티라는 프로그램을 운영하면 선택과 집중을 하며 보다 효율적일것 같다는 생각이 들었다. 그러면서 어떠한 내용들로 보상을 해줬는가에 대해 소개를 해줬는데 가장 접하기 쉬운 XSS 공격이나 서버설정 미스로 인한 정보노출 등 아주 다양한 사례를 공유해 주셨다. 발표자료 [113]LINExNAVER 개발 보안 취약점 이야기 from NAVER D2 # 쿠팡 서비스 Cloud Migration을 통해 배운 것들쿠팡 서비스 Cloud Migration을 통해 배운 것들쿠팡 서비스 Cloud Migration을 통해 배운 것들 사내에서도 발표가 있었는데 그때 제대로 못들어서 다시 듣게 되었다. 지난 2년동안 서비스를 클라우드로의 이전을 하면서 마주쳤던 문제들과 해결책, 그리고 클라우드의 마이크로서비스가 만나면서 마주친 새로운 문제들과 정리했던 생각들에 대해 공유하는 발표였다. 클라우드 이전원칙 : 확장성 확보하기 위함, 무중단 이전, 고객에게 만족도에 영향이 없어야 함 공통배포 파이프라인 유지, 만든사람이 운영하는 문화, 장애관리 문화 특히 안정상태 찾기라는 제목으로 서비스의 건강도를 측정하는 대시보드 형태의 페이지를 만들었다는 부분에서 내가 하고있는 서비스에서도 API Status 페이지처럼 서비스 전반에 대한 건강도(?)를 대시보드 형태로 충분히 만들수 있지 않을까 하는 생각을 해보았다. 발표자료 [115]쿠팡 서비스 클라우드 마이그레이션 통해 배운것들 from NAVER D2 # Druid로 쉽고 빠르게 빅데이터 분석하기Druid로 쉽고 빠르게 빅데이터 분석하기Druid로 쉽고 빠르게 빅데이터 분석하기 여태 참석한 DEVIEW 세션들 중에 시작전에 줄이 가장 길었던 세션(행사장 전체 반바퀴를 줄서야 했던 ;;) 그만큼 사람들도 빅데이터 분석에 대해 관심이 많이 있다는걸 증명하였고 나역시 Day1, Day2 전 세션들 중에 이 세션이 가장 기대가 되었다. Druid는 분석용도로 만든 플랫폼이고 아파치 인큐베이터에 들어가 있을정도로 각광받고 있는 플랫폼 이라고 한다. 일반적으로 빅데이터를 분석하기 위해서는 Druid와 Elasticsearch, Apache kudu 가 비교대상이 되는데 발표자분은 각 플랫폼을 아주 다양한 각도에서 비교 분석하며 현 상황에서 가장 적합한 플랫폼을 찾기위한 노력한 부분을 보여주셨다. 필자는 기존에 Elastic Stack을 백지부터 홀로 터득한 경험이 있어 무슨차이가 있는지 궁금했는데 가장 큰 다른점은 join과 group by가 된다는 장점이 있다고 한다. (join기능은 자체 제공하지는 않지만 spark와 연동해서 해결했다고 한다.)또한 가장 관심갖고 봤던 비쥬얼라이징툴에 대해 소개해주셨는데 Imply UI소개를 듣고 감탄사가 절로 나왔지만 유료… 그라파나는 플러그인을 통한 다양한 그래프를 쉽게 표현이 가능하다고 하였고 Superset 은 표현할수있는 그래프가 가장 많고 권한관리도 입맛에 맞게 가능하다고 한다. 다만 너무 많아서 복잡할수도 있다고 … Metabase는 어플리케이션 방식이고 깔끔한 UI를 제공하지만 세부적인 설정은 불가능 하다는 단점이 있다고 한다.솔루션 선택을 할때 마냥 좋다고 사용하는것이 아니라 실제로 다양한 관점에서의 테스트를 해보고 서비스에 적절한 솔루션을 선택하는 좋은 발표를 들을수 있어서 너무 좋았다. 발표자료 [215] Druid로 쉽고 빠르게 데이터 분석하기 from NAVER D2 # 마치며나도 언젠간 스피커가 될수 있겠지...?나도 언젠간 스피커가 될수 있겠지...? 이번 Deview에서는 Facebook에서 알게된 POPit 저자분을 실제로 만나뵙기도 했고 지금은 S사에 계시는 우연히 만나게된 반가운 예전팀 형, 그리고 군 장교시절 필자의 소대원이 AI 스타트업 소속으로 부스를 운영을 하며 서비스 소개를 하고 있는 모습도 보고… 참 다양한 이벤트들이 많았던 행사였다. 사실 올해 Deview 발표자를 모집할때 주니어 개발자가 회사밖에서 성장하는 방법 같은 내용으로 발표를 해볼까도 생각했지만 지금생각해보면 좀더 준비를 철저히 그리고 깊게 해야겠다고 느꼈다.그리고 내가 하고있는 업무 즉, 서비스 운영/개발은 아무리 바쁘고 힘들어도 기본으로 해야하고 회사를 벗어나 신기술 또는 기존기술의 고도화 방법을 찾아서 공부하고 내것으로 만들어 나가야 하는건 예전이나 지금이나 변함이 없다는걸 다시한번 느낄수 있었던 좋은 행사였다고 생각한다. 이번에도 뒤통수 세게 맞고 간다…","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"deview","slug":"deview","permalink":"https://taetaetae.github.io/tags/deview/"}]},{"title":"2018 Pycon. 그리고 첫 발표를 하다.","slug":"pycon-2018","date":"2018-08-28T14:43:16.000Z","updated":"2020-04-23T04:41:36.818Z","comments":true,"path":"2018/08/28/pycon-2018/","link":"","permalink":"https://taetaetae.github.io/2018/08/28/pycon-2018/","excerpt":"IT관련 행사에 참여하면 여러가지 정보를 얻을수 있다. 개인적으로는 사실 정보를 얻기 위함보다 그곳의 분위기를 현장에서 몸소 느끼고 참여한 사람들의 눈빛을 보며 해이해진 마음가짐을 다시 다잡을수 있음이 가장 큰 목적이다. 그에 올해 Pycon도 하나의 전환점이 되길 바라는 마음으로 신청을 하게 되었다.","text":"IT관련 행사에 참여하면 여러가지 정보를 얻을수 있다. 개인적으로는 사실 정보를 얻기 위함보다 그곳의 분위기를 현장에서 몸소 느끼고 참여한 사람들의 눈빛을 보며 해이해진 마음가짐을 다시 다잡을수 있음이 가장 큰 목적이다. 그에 올해 Pycon도 하나의 전환점이 되길 바라는 마음으로 신청을 하게 되었다. # 등록https://www.pycon.kr얼리버드 등록을 한다고 Facebook에서 홍보를 하길래 그런가보다 했는데 잠깐 회사일에 집중하고 다시 보니 이미 매진이 되어있었다. 사실 Pycon 은 올해가 처음 가보는거라 인기를 실감할수 없었는데 이정도일줄은 상상도 못했다. (나중에 알게 된 사실이지만 올해가 가장 인원이 많았다고…) 그래서 나중에 진행되었던 일반표 등록은 휴대폰 알람까지 걸어두며 늦지않게 등록할수 있었다. 세부 일정들이 업데이트가 되고 어떤 세션을 들을까 고민하면서 간략 소개를 하나둘씩 보게 되었는데 Python을 만지며 평소에 궁금했던거나 재밌어 보이는 세션들이 너무많아 고민을 많이 했다. 한가지 아쉬운건 로그인 기반이 아니다 보니 (임시 로그인기반?) 내 시간표 설정하는게 없었다. 나는 별도로 적어서 갔지만 나중엔 그런 기능이 생겼으면 좋겠다. 2019년 Pycon엔 크롬 익스텐션으로 기능을 만들어 로그인 여부와 상관없이 몇시에 내가 어떤 세션을 들을건지에 대한 설정을 하고 이를 이미지로 캡쳐해서 출력/다운 받을수 있는 걸 만들어 보고 싶다. (그전에 미뤄뒀던 크롬 익스텐션 개발하는 방법부터 공부하자…) # 첫째날개인적으로 아침잠이 너무 많은데 알림이 울리기도 전에 눈이 떠졌고 행사장에 도착해보니 후원사 부스는 아직 텅텅 비어있었고, 밤새가면서 준비를 하셨는지 자원봉사자 분들은 여기저기 빈백에 누워(쓰러져) 자고 있었다. 그만큼 Pycon에 대한 기대가 컸나보다. 시간이 지나니 하나둘씩 사람들이 등록을 하며 오기 시작하였고 역시나 행사에 꽃중에 꽃인 후원사 부스에서 나눠주는 이벤트 상품들을 받기 바빴다.DIVE INTO DIVERSITY !!DIVE INTO DIVERSITY !!키노트를 시작으로 사람들은 각자 듣고싶은 세션에 참가하며 행사는 시작이 되었다. 전체적으로 기술의 난이도는 초급 수준의 발표였던걸로 느껴졌다. (물론 나는 초초초급도 안되는 꼬꼬마 수준이지만…) 대부분 Python으로 어떤걸 해봤고, 어떤 어려움이 있었고, 이러저러한 상황들을 만났으며, 요런 경우에서는 어떻게 하며 해결을 하였다는 등 기술을 활용한 “경험기”에 대한 내용들을 들을수 있었다.Pycon의 슬로건인 DIVE INTO DIVERSITY에 걸맞게 아주 다양한 주제로 흥미있는 발표내용들이였다. 기억나는 것들중에 인상깊었던 부분들을 정리해본다. 파이썬 문화(?)중의 하나는 몰라서 물어보는 사람에게 구글링을 하라기보다 직접 알려주라는 것이다. 배우고 싶다면 다른사람들을 가르치는것부터(알려주는것부터) 시작하라. 여성 개발자, 여성 발표자들도 점점 늘어나고 있다. 파이썬을 개발 현장(?)이 아닌 다른곳에서 사용한다면 작업 속도도 빠르고 얻어내는 가치또한 훨씬 더 방대하다. 엑셀로 할수 있는 작업을 파이썬으로 할수 있다. 파이썬의 다양한 라이브러리는 일상의 도움을 준다. 행사를 들으며 꼭 질문을 해야지 하는 마음을 갖고 있었는데 (그래야 오래 기억에 남으니) 마침 어떤 세션에서 궁금한게 있어 질문을 할수 있었다. (질문을 하니 파이썬 관련 책 선물도 받았다.^^)그리고 마지막 라이트닝 톡이라는 세션이 있었는데 여러 발표자들이 짤막하게 5분동안 하고싶은 이야기를 하는 세션이였다. 5분이라는 제한이 있기에 다들 쉽고 편하게 발표하는듯 보였으나 발표 자료나 발표내용을 보면 꼭 그렇게 간단하게 발표하는건 아니였다. 본 세션에서 말하기엔 다소 분량이 작은 알차고 깨알같은 발표도 있었고, 매년 Pycon 라이트닝톡에 발표하는게 목표이신 분도 있었다.발표를 들으면서 난 언제 저런자리에 가서 발표를 할수 있을까 하는 마음이 스쳐 지나갈때 쯤. “왜못하지? 나 파이썬으로 만든거 있잖아?” 라고 혼잣말로 궁시렁거리며 둘째날에 있는 라이트닝톡에서 발표하기로 마음을 먹고 서둘러서 참가 신청을 보냈다. 그러고서는 저녁을 먹고 집에 늦게 돌아와 새벽 3시넘어서야 발표자료를 완성하였지만 “발표” 라는 부담감때문에 어렵게 잠에 들었다. # 둘째날어제와는 달리 오늘은 잠을 많이 못자서 인지 늦게 일어나 첫 세션이 시작하고서 거의 끝날 즈음에 행사장에 도착하게 되었다. “괜히 발표 한다고 한걸까” 라는 생각이 들며 진행위 본부에 가서 발표 순서를 확인해보니 첫번째… 슬슬 머리가 아파오기 시작했다. 그래도 듣기로한 세션은 들어보고 싶어서 집중해서 세션들을 돌아가며 들었지만 머릿속에는 온통 “발표 발표 발표”라는 생각때문에 오히려 다른분께서 하시는 발표를 집중해서 듣지 못하였다. # 첫 발표라이트닝톡이 시작이 되고, 첫 타자로 단상위에 올라섰다. 어림잡아 200여명 정도 였던것 같다. (기분탓으로 많아 보였을수도…) 두근두근. 드디어 발표 시작. 마치 잡스 마냥 멋지게 발표를 하고 싶었지만 막상 올라가서 스피커를 통해 들려오는 내 목소리를 듣고 있자니 너무 떨렸다. (이럴줄 알았으면 청심환이라도 먹고 올라가는건데…) 잠깐 말을 안하고 있었는데 적막이 흐르고… 빨리 발표를 진행해야겠다 싶어 준비한 스크립트를 그대로 보고 읽기 시작한다… 마치 국어책 읽는것마냥…저땐 왜그렇게 떨렸는지...저땐 왜그렇게 떨렸는지...발표의 주제는 “파이썬으로 토이프로젝트 만들기” (a.k.a 기술블로그 구독서비스 홍보) 였다. 필자도 파이썬을 공식적(?)으로 배워본적도 없고 주어 듣고 구글링 한게 전부인 상태에서 왜 만들게 되었고 어떤식으로 만들었는지에 대해 발표를 했는데 사실 이 발표의 주된 목적중에 하나는 기술블로그 구독서비스 홍보였다. 조금이라도 구독자수를 늘리고 싶어서… 적어도 한 100명은 가입 하겠지 라는 생각으로…하지만 발표가 끝나고 구독자수를 보기위해 홈페이지에 접속해보니 서버가 다운;;; ssh 접속도 안되는 상황이였다. 부랴부랴 AWS콘솔에서 서버 재시작을 하고 겨우 살렸는데 아뿔싸! 동접에 대한 테스트 즉, TPS 측정같은 성능테스트를 전혀 하지 않은것이다. 발표도 너무 못하고, 홍보도 제대로 못하고ㅠㅠ 이렇게 발표가 끝나서 너무 아쉽다. (물론 준비 시간이 부족했던게 핑계지만 가장 크다.)발표자료유튜브 녹화영상 # 번외 (nGrinder를 성능테스트편)필자는 기술에 있어서 “개념만 아는 사람”과 “그 개념을 토대로 실제로 해본사람”은 하늘과 땅차이라고 생각한다. 예전 포스팅 한것중에 Apache 의 mpm 방식에 대해 정리한게 있었는데 (링크) 정리할때 한번 각 방식에 대해서 프로토타이핑이라도 해봤으면… 이런 사태(?)는 면했을것 같다. Apache 설치시 Default방식인 Prefork방식으로 설치되어 있었고 AWS Free tier 의 메모리는 1기가. 사용중인 메모리는 약 450메가였고 mpm방식에 대해 아무 설정도 하지 않았으니 당연히 사용자가 늘어나면 늘어날수록 메모리는 올라가고 KeepAlive 또한 기본값인 on으로 되어있어서… 예정된 서버 다운이였다. (이것도 포스팅을 했었다…ㅠㅠ링크)Apache 를 다시 Worker 방식으로 바꿔 설치하고 KeepAlive 또한 Off로 설정하고 nGrinder를 통해 성능테스트를 해보니 다음과 같은 결과를 얻을수 있었다.Prefork방식일때 테스트를 해보니 다운이 되었다.Prefork방식일때 테스트를 해보니 다운이 되었다.만약, 이걸 예상하고 Worker방식에 KeepAlive Off 가 되어있었다면? 홍보가 더 잘되었을것 같다는 생각을 뒤늦게 땅을 쳐가며 후회해본다. (이래서 개발 마지막 단계는 다양한 QA가 필요하다며…) # 마치며이틀간의 걸쳐 진행된 Pycon은 필자에게 많은 선물을 안겨 주었다. (이벤트 상품들을 많이 받아서가 아니라..ㅎㅎ;;)이번 파이콘에서 받은 선물들이번 파이콘에서 받은 선물들물론 주된 목적은 파이썬이라는 언어의 사용법과 가치, 확장성 등 다양한 세션을 통해 얻은 정보였지만 아무래도 첫 발표를 했던지라 필자에겐 라이트닝톡이 가장 기억이 남는다. 발표를 막상 해보니 별거 없다는 생각과 자신감이 생겼다. (물론 청중이 많아지고 시간이 길어진다면 또 다른 문제일수 있겠지만)내년엔 좀더 준비를 탄탄히 하고 발표도 여유롭고 부드럽게 해서 꼭 본 세션에 참가해 보고 싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"pycon","slug":"pycon","permalink":"https://taetaetae.github.io/tags/pycon/"}]},{"title":"자바 객체 복사하기 ( feat. how to use CloneUtils? )","slug":"how-to-use-cloneUtils","date":"2018-08-21T09:02:47.000Z","updated":"2020-04-23T04:41:36.767Z","comments":true,"path":"2018/08/21/how-to-use-cloneUtils/","link":"","permalink":"https://taetaetae.github.io/2018/08/21/how-to-use-cloneUtils/","excerpt":"자바(Java)로 개발을 하다보면 한번쯤 객체를 복사하는 로직을 작성할때가 있다. 그때마다 나오는 이야기인 Shalldow Copy 와 Deep Copy. 한국어로 표현하면 얕은 복사와 깊은 복사라고 이야기를 하는데 이 두 개념의 차이는 아주 간단하다. 객체의 주소값을 복사하는지, 아니면 객체의 실제 값(value)를 복사하는지.","text":"자바(Java)로 개발을 하다보면 한번쯤 객체를 복사하는 로직을 작성할때가 있다. 그때마다 나오는 이야기인 Shalldow Copy 와 Deep Copy. 한국어로 표현하면 얕은 복사와 깊은 복사라고 이야기를 하는데 이 두 개념의 차이는 아주 간단하다. 객체의 주소값을 복사하는지, 아니면 객체의 실제 값(value)를 복사하는지. 이 둘의 차이점을 소개하는 글들은 워낙 많으니 패스하도록 하고 이번 포스팅에서는 Deep Copy를 할때 org.apache.http.client.utils 하위에 있는 CloneUtils 사용법에 대해 정리 하고자 한다. 그냥 쓰면 되는거 아닌가? 라고 생각했지만 (별거 아니라고 생각했지만) 해보고 안해보고의 차이는 엄청컸고 사용할때 주의점이 몇가지 있어 정리 하려고 한다. 예제에 앞서 본 포스팅에서 사용할 객체를 간단히 정리하면 다음과 같다. (학교에서 학생 신상정보를 관리한다고 가정해보자.) 12345678910111213141516public class Student &#123; String name; // 이름 int age; // 나이 Family family; // 가족&#125;public class Family &#123; String name; // 이름 int age; // 나이 boolean isOfficeWorkers; // 직장인 여부&#125;public class PhysicalInformation &#123; int height; // 키 int weight; // 몸무게&#125; # 객체는 Cloneable interface 를 implement 해야하고 clone 메소드를 public 으로 override 해야한다.당연한 이야기가 될수도 있으나 CloneUtils를 사용하기 위해서는 해당 객체는 Cloneable interface 를 implement 해야한다. 그리고 나서 clone 메소드를 override 해야되는데 여기서 가장 중요한점은 외부에서도 호출이 가능해야하기 때문에 public 으로 override를 해야한다. (기본은 protected 로 되어있다.) 우선 간단히 객체를 생성하고 출력부터 해보자. (출력을 이쁘게 하기 위해 ToStringBuilder.reflectionToString을 사용하였다.) 12345PhysicalInformation physicalInformation = new PhysicalInformation();physicalInformation.height = 180;physicalInformation.weight = 70;System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE)); 결과는 당연히 1PhysicalInformation@5d6f64b1[height=180,weight=70] 이제 Cloneable interface 를 implement 하고 clone 메소드를 public 으로 override 한뒤, CloneUtils를 사용해서 객체를 복사해보자. 테스트를 하면서 Shalldow Copy도 해보자. 1234567891011121314151617181920212223242526272829303132333435363738394041// class settingpublic class PhysicalInformation implements Cloneable&#123; int height; int weight; @Override public Object clone() throws CloneNotSupportedException &#123; // public 으로 바꿔주자. return super.clone(); &#125;&#125;// test codePhysicalInformation physicalInformation = new PhysicalInformation();physicalInformation.height = 180;physicalInformation.weight = 70;PhysicalInformation physicalInformationShalldowCopy = physicalInformation;PhysicalInformation physicalInformationDeepCopy = null;try &#123; physicalInformationDeepCopy = (PhysicalInformation)CloneUtils.clone(physicalInformation);&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;// 원본System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE));// 얕은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationShalldowCopy, ToStringStyle.DEFAULT_STYLE));// 깊은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationDeepCopy, ToStringStyle.DEFAULT_STYLE));// 값 변경physicalInformation.weight = 80;physicalInformation.height = 170;// 원본System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE));// 얕은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationShalldowCopy, ToStringStyle.DEFAULT_STYLE));// 깊은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationDeepCopy, ToStringStyle.DEFAULT_STYLE)); 결과는 원본과 얕은 복사를 한것은 메모리 주소(?)가 같으나 깊은 복사를 한것은 데이터는 같지만 주소가 다르고 값을 변경해도 영향을 주지 않는다. (완전히 서로다른 객체인것을 증명) 1234567PhysicalInformation@1376c05c[height=180,weight=70]PhysicalInformation@1376c05c[height=180,weight=70]PhysicalInformation@1b4fb997[height=180,weight=70]PhysicalInformation@1376c05c[height=170,weight=80]PhysicalInformation@1376c05c[height=170,weight=80]PhysicalInformation@1b4fb997[height=180,weight=70] 만약 위에서 clone을 기본값인 protected로 override를 하게 되면 어떤 결과를 가져올까? 1234Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.PhysicalInformation.clone() at org.apache.http.client.utils.CloneUtils.cloneObject(CloneUtils.java:55) at org.apache.http.client.utils.CloneUtils.clone(CloneUtils.java:77) at com.Test.main(Test.java:16) 접근제한자에서 눈치를 챌수도 있었겠지만 접근을 할수없어 CloneUtils 이 리플렉션을 하는 과정에서 Exception을 발생한다. 꼭! public 으로 override를 해주자. # 객체 내에 clone이 안되는 변수는 별도 처리가 필요하다.객체 내에 있는 멤버 변수는 원시 변수(int, char, float 등) , Immutable Class (String, Boolean, Integer 등) 또는 Enum 형식일 때는 원본의 값을 바로 대입해도 되지만, 그렇지 않을 때는 멤버변수의 clone을 호출하여 복사해야 한다. 말로만 보면 무슨이야기 인지 모르니 예제를 보자. 12345678910public class Student implements Cloneable &#123; String name; int age; Family family; @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; Student 클래스에서 Cloneable 를 implements 하고 clone 메소드를 override 하였다. (여기서 구멍이 있다!!) 그다음 Family 클래스는 초기 그대로 두고 CloneUtils을 사용해서 객체를 복사하는 코드를 작성해보자. 12345678910111213141516171819202122Student student = new Student();student.name = \"taetaetae\";student.age = 20;Family family = new Family();family.age = 25;family.isOfficeWorkers = true;family.name = \"son\";student.family = family;Student studentDeepCopy = null;try &#123; studentDeepCopy = (Student)CloneUtils.clone(student);&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;// student 객체 복사여부 확인System.out.println(ToStringBuilder.reflectionToString(student, ToStringStyle.DEFAULT_STYLE));System.out.println(ToStringBuilder.reflectionToString(studentDeepCopy, ToStringStyle.DEFAULT_STYLE));// student 내 family 객체 복사 여부 확인System.out.println(ToStringBuilder.reflectionToString(student.family, ToStringStyle.DEFAULT_STYLE));System.out.println(ToStringBuilder.reflectionToString(studentDeepCopy.family, ToStringStyle.DEFAULT_STYLE)); Student 객체 안에 int, String, 그리고 별도로 만든 객체인 Family 가 있는 상황에서 복사를 해보자. 결과는 어떻게 나왔을까? 12345com.Student@1376c05c[name=taetaetae,age=20,family=com.Family@51521cc1]com.Student@deb6432[name=taetaetae,age=20,family=com.Family@51521cc1]com.Family@51521cc1[name=son,age=25,isOfficeWorkers=true]com.Family@51521cc1[name=son,age=25,isOfficeWorkers=true] 위에서 말했던 구멍의 결과를 볼수 있다. Student 객체는 주소값이 다른걸 보니 깊은 복사가 되었지만 그 안에 있는 Family 형 변수는 얕은 복사가 된것을 확인할수 있다. 위에서 말한것과 같이 clone이 안되는 경우는 (다시 말하면 원시변수나 Immutable Class, enum 등 clone을 지원하는 객체가 아닐경우) 별도로 clone이 되도록 설정이 필요하다. 그래서 Family 도 Cloneable 를 implements 하고 clone 메소드를 override 해준다음 최상위 객체였던 Student의 clone 메소드를 아래처럼 조금 수정해주고 나서 테스트를 해보면 이쁘게 깊은 복사가 된것을 확인할수 있다. 12345678910111213141516171819202122232425// 복사가 될수있도록 설정public class Family implements Cloneable&#123; String name; int age; boolean isOfficeWorkers; @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125;// 일단 복사를 하고, 멤버 객체 자체를 복사한 다음 대입 public class Student implements Cloneable &#123; String name; int age; Family family; @Override public Object clone() throws CloneNotSupportedException &#123; Student student = (Student)super.clone(); student.family = (Family)CloneUtils.clone(student.family); return student; &#125;&#125; 그러고 테스트를 해보면 내부 멤버 변수도 복사가 된것을 확인할수 있다. 1234com.Student@51521cc1[name=taetaetae,age=20,family=com.Family@1b4fb997]com.Student@28ba21f3[name=taetaetae,age=20,family=com.Family@694f9431]com.Family@1b4fb997[name=son,age=25,isOfficeWorkers=true]com.Family@694f9431[name=son,age=25,isOfficeWorkers=true] # 마치며멤버변수가 List, Map, Set 등 여러 유형으로 복잡하게 만들어졌을경우 각각 복사를 해주는 등 다양한 케이스에 유연하게 대응할수 있어야 하겠다.객체복사? 그거 그냥 하면 되는거 아냐? 라고 볼수도 있으나 입개발 하는 사람과 직접 구현해보고 내부 코드까지 들여다보는 수고를 하는 사람의 차이는 언젠간 분명히 드러날꺼라 생각한다. (필자가 그랬으니ㅠㅠ)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"cloneUtils","slug":"cloneUtils","permalink":"https://taetaetae.github.io/tags/cloneUtils/"},{"name":"java deep copy","slug":"java-deep-copy","permalink":"https://taetaetae.github.io/tags/java-deep-copy/"}]},{"title":"기술블로그 구독서비스 개발 후기 - 2부","slug":"daily-dev-blog-2","date":"2018-08-09T11:37:26.000Z","updated":"2020-04-23T04:41:36.708Z","comments":true,"path":"2018/08/09/daily-dev-blog-2/","link":"","permalink":"https://taetaetae.github.io/2018/08/09/daily-dev-blog-2/","excerpt":"1부에서는 기술블로그 구독서비스(이하 서비스)를 왜 만들게 되었고 어떤구조로 만들가에 대해 이야기를 해보았다면, 이번 포스팅에서는 만들면서 만나게 된 각종 트러블슈팅 종합세트(?)를 하나씩 풀어보고자 한다.","text":"1부에서는 기술블로그 구독서비스(이하 서비스)를 왜 만들게 되었고 어떤구조로 만들가에 대해 이야기를 해보았다면, 이번 포스팅에서는 만들면서 만나게 된 각종 트러블슈팅 종합세트(?)를 하나씩 풀어보고자 한다. 물론 개발을 하면서 아무 문제 없이 잘 되면 당연히 좋겠으나 잘되도 이상한게 개발이라는 세계가 아니던가.잘 안되면 문제, 잘 되도 문제 ㅠㅠ출처 : https://www.clien.net/service/board/park/9111495잘 안되면 문제, 잘 되도 문제 ㅠㅠ출처 : https://www.clien.net/service/board/park/9111495 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 지난 1부에서 이야기 했던것처럼 문제 - 해결, 문제 - 해결 식으로 나열해보고자 한다. 다소 글의 전개가 뒤죽박죽일수도 있겠지만 말 그대로 트러블슈팅 종합세트이니 독자들의 양해를 미리 구한다. - 트러블 슈팅 리스트 10시에 로직이 실행되었지만 메일을 11시 넘어서 받게 된다. 제목없는 글? 블로그 RSS파싱 오류? 간헐적으로 오류가 생긴다. 메일 내에 class를 적용하여 CSS 처리가 불가능하다. 메일을 보냈으나 스팸으로 처리된다. Elastic Stack을 사용할수 없다. 메일 보내는 발송속도가 너무 느리다. 구독해제가 아닌 자체 수신거부는 어찌 처리할까? # 10시에 로직이 실행되었지만 메일을 11시 넘어서 받게 된다.👉 해결방안 : Divide and Conquer본 서비스의 요구사항중 하나는 매일 오전 10시, 구독자들에게 어제 등록된 글을 수집하여 메일로 보내주는게 있다. 우선 로직은 다음과 같은 순서로 진행되게 개발하였고, jenkins 등 별도의 스케쥴러 관리 어플리케이션에 의해 할수도 있었으나 이 또한 심플하게 crontab 에 등록하여 매일 오전 10시에 실행되도록 하였다.12341. awesome-devblog 에서 블로거들의 RSS 피드를 조회한다.2. 어제 등록된 글이 있다면 리스트에 담는다.3. 조회가 끝나면 메일형식에 맞추어 html 문자열을 만든다.4. 만들어진 문자열을 가지고 등록된 구독자들에게 메일을 보낸다. 로직은 아주 간단했다. 데이터를 파싱하는 방법이나 메일형식에 맞추어 html문자열을 만드는 등 별도의 라이브러리를 사용하는 다소 복잡한 부분만 빼면 단순히 for문과 if문을 조합해서 로직을 구성할수 있었다. 헌데, 10시에 해당로직이 실행되었지만 최대 1시간이 지나고서야 메일을 받는 경우도 있었다. 이게 무슨일일까!?눈치를 챘을수도 있지만 RSS 피드를 조회하는 곳에서 오래걸린 것이다. 티스토리나 네이버등 다른 블로그들은 RSS를 읽고 파싱하는 속도가 그렇게 오래 걸리지 않았는데 (1초 이내) 유독 이글루스 블로그의 RSS파싱이 오래걸리는건 1분까지도 걸리던 것이였다. ( 참고로 RSS 파싱모듈, yaml 파싱모듈 을 사용했다. ) 아마 RSS의 형식이 약간 달라서 그런것 같긴 한데 그렇다고 이글루스 일 경우에 파싱을 다르게 하는건 좀 그렇고… 추후 이글루스가 아닌 또다른 파싱속도가 느린 블로그의 RSS를 만날수도 있기에 RSS 타입별로 예외처리를 하는건 좀 아닌것 같았다.이런저런 고민끝에 아주 간단하게도 임무(?)를 나누는식으로 해결 하였다. 즉, RSS를 읽고 메일에 보낼 데이터를 만드는 job 하나와 만들어진 데이터를 가지고 이메일을 보내는 job 으로 나눈뒤 RSS를 분석하는 job은 9시에, 메일보내는 job은 10시에 보내도록 해서 생각보다 아주 심플하게 문제를 해결할수 있었다.복잡하고 어려운 문제를 꼭 복잡하고 어렵게만 해결해야하는 법은 없는것 같다. 모로 가도 서울만 가면 된다라는 속담이 있지 않는가. # 제목없는 글? 블로그 RSS파싱 오류? 간헐적으로 오류가 생긴다.👉 해결방안 : 언제나 신경써야 하는 예외처리(try-catch)내가 만든 코드는 언제나 내 생각대로만 돌아갔으면 하는건 모든 개발자의 마음과 같다.흔한 IT 종사자들.deploy출처 : https://9gag.com/gag/a0Yxw4B/operations-team-before-leaving-for-holidays흔한 IT 종사자들.deploy출처 : https://9gag.com/gag/a0Yxw4B/operations-team-before-leaving-for-holidays하지만 그생각도 잠시 언제나 예외는 발생하기 마련. ( 물론 전혀 예외가 발생 안할수도 있으나 만약 발생하지 않았다 할지라도 발생할수 있는 가능성은 염두해둬야 한다. ) 파싱하는 과정에서 제목이 없는글로 온다거나, 가끔 RSS url 응답이 404 또는 503 인 경우가 있었다. 참고로 필자는 출근이 늦은편이라 아침마다 늦잠을 자곤 했는데 이 서비스를 만들면서 덕분에(?) 9시 에는 메일에 보낼 데이터가 잘 만들어 졌는지, 10시에는 메일이 잘 갔는지 확인을 하다보니 일찍 일어나는 습관이 길러졌다 -ㅁ- 이러한 경우는 예외처리를 해서 제목이 없는 경우엔 임의로 ‘제목없음’ 이라는 제목으로 만들어주고, RSS url 응답이 정상적이지 않는 경우엔 해당 블로그 RSS를 무시할수 있도록 try-catch (python에서는 try-except) 구문을 사용하여 로직 도중에 어떠한 이유라도 중단되는 일이 없도록 하였다.언제나 예외처리는 항상 신경쓰자. 물론 내가 생각한 예외가 전혀 발생하지 않더라도 예외처리를 하는 습관을 기르다 보면 호미로 막을것을 가래로 막는 불상사는 만나지 않을것 같다. # 메일 내에 class를 적용하여 CSS 처리가 불가능하다.👉 해결방안 : Inline Style 처리개인적으로 디자인을 신경쓰는 개발자(?)인지라 메일보낼때도 그냥 텍스트보다는 그럴싸한 형식으로 메일을 보내고 싶었다. 그래서 Bootstrap을 활용하여 이쁘장하게 만들었는데 막상 보내보니 설정한 class는 다 날라가고 날것의(?) html으로만 보내지는 것이였다. 좀 찾아보니 이메일 내에는 여러 부분에서 html요소들이 제한된다고 한다. ( 관련링크: 이메일 클라이언트 CSS지원 )그래서 찾다보니 다행히도 class로 설정된 html을 이쁘게 inline style 로 바꿔주는 서비스(Emogrifier)가 있어서 이를 활용하여 전부 inline style로 바꾸게 되었다. 정말 세상엔 능력자들이 너무 많다… # 메일을 보냈으나 스팸으로 처리된다.👉 해결방안 : 도메인 구입과 DKIM 설정메일 발송은 본 서비스에서 가장 중요한 부분이다. 이런저런 과정을 통해 파싱해서 메일 보낼 데이터를 만들었는데 메일을 못보내면 말짱 꽝이기 때문. 그래서 메일보내는 방법을 다양하게 생각해봤는데 얕은 지식과 더 얕은 경험으로 두가지 방법을 생각할수 있었다. 자체 SMTP 구축 Google SMTP 서버 활용 우선 처음엔 자체 SMTP 구축을 해서 메일을 보내게 되었다. 메일보낼때 시스템 부하가 발생하면 어쩌지 하는 걱정을 하였지만 그 걱정에 앞서 수신자의 이메일이 naver.com 인 경우는 메일이 잘 보내지는데 google.com 인 경우에는 스팸으로 메일이 가게 된것이다. 스팸이라… 전혀 생각하지 못한 예외가 발생하였다!이 서비스를 만들면서 가장 큰 위기였던것 같다. 우선 자체 SMTP를 구축해서 메일을 보내고 있었기에 어떻게든 설정을 통해 ‘난 스팸메일을 보내지 않는다’를 알리고 싶었다. 그에 찾아본 키워드로는 DKIM(DomainKeys Identified Mail)과 SPF(Sender Policy Framework)이 있었으나 … 귀차니즘인건지 열정의 부족인건지 서버에 셋팅하다 포기, 다시 셋팅하다 포기를 몇번을 한지 모른다.가장 큰 위기, 내가 이 서비스를 만들수 있을까? (정치적 메세지는 아닙니다.)출처 : https://news.sbs.co.kr/news/endPage.do?news_id=N1003101192가장 큰 위기, 내가 이 서비스를 만들수 있을까? (정치적 메세지는 아닙니다.)출처 : https://news.sbs.co.kr/news/endPage.do?news_id=N1003101192서버설정으로 스팸을 회피하기는 내 실력으론 부족한걸 인지하고, 다른방법으로 Google SMTP서버를 사용해보았더니 놀랍게도 너무 간단하게 스팸으로 발송이 안되고 정상적으로 메일을 발송할수 있었다. 그러나 그 행복(?)도 잠시 무료라는 키워드엔 언제나 제한이 있기 마련. 계정당 하루 최대 500개밖에 보낼수 없었다. 그렇다면 계정을 여러개 만들면 되는게 아닐까하고 서비스에 적용하기 앞서 계정을 여러개 만들어 테스트를 해보니 실 user당 하루 500개를 넘지 못하는걸 확인할수 있었다. (구글 SMTP를 사용하기 위해서는 항상 본인인증을 하는데 이게 제한 포인트인것같다.) 그럼 결국 Google SMTP도 정답은 아니고…처음 서비스를 만들때 어떠케든 비용이 들지 않는 선에서 만들고 싶었으나 투자하는 시간에 비해 퍼포먼스가 나오지 않아 결국 돈을 지불하고서라도 해야겠다는 마음으로 SMTP 호스팅 업체를 찾다 AWS에 SES라는 서비스를 발견하게 된다. 비용은 1,000건당 0.1달라이니 구독자가 1,000명이고 30일을 발송한다고 가정하면 한달에 약 3달라가 드는셈. 원화로 따지면 한달에 3천원인데 나중에는 이 비용조차 줄여볼수 있는 방안을 고려해봐야 겠다. 이는 3부에서 정리해본다.AWS의 SES를 활용하면 앞서 그토록 어려웠던 스팸메일 우회를 아주 간단하게 할수 있었고, 그러다 보니 도메인도 구매하게 되고 깔끔하게 스팸메일 우회를 처리하면서 도메인도 이쁘게(?) 만들수 있었다. ( 물론 도메인을 구입하면서 부가세 포함 13.2달라가 발생하였지만… 열정페이로 이또한 수익을 얻는 구조로 생각을 해봐야 겠다. 1년만 서비스 할게 아니였으니… )AWS의 도메인을 구입하고 AWS-SES를 통해 스팸을 우회하여 메일을 보내는 과정은 아래 포스팅을 참조하였다. (워낙 정리가 잘되어있어 별도로 정리하지는 않고, 링크를 공유하고자 한다.) http://jojoldu.tistory.com/246 https://insidestory.kr/11477 http://blog.naver.com/my0biho/220937119874 # Elastic Stack을 사용할수 없다.👉 해결방안 : 직접 만들어버리자!로깅은 서비스를 운영하는데 있어 하나의 무기가 될수 있다. 기본 서버에서는 아파치와 Elasticsearch, kibana를 동시에 돌릴만한 메모리가 안되었기에 (1G….) 서버 한대를 더 받고 Elasticsearch와 Kibana를 설치하였고 가입자수, 포스팅수 등 다양한 로깅을 보려했는데 몇일이 지나고 AWS에서 메일이 온다.너님, 이대로 가다가는 돈낼수 있으니 참고해!너님, 이대로 가다가는 돈낼수 있으니 참고해!한달에 사용할수 있는 서버운영 제한시간이 곧 다가온다는 이야기. 이것저것 검색해보니 Free Tier 일 경우 한달간 서버 한대를 24시간 사용하는데만 무료이고 그 이상은 과금이 나간다고 한다. 덕분에 한 3달라 정도 과금이 발생해버렸다. (정리하면, AWS Free Tier를 사용할경우 24시간 서버 운영시 한달에 한대밖에 사용하지 못한다. 서버를 중지했다가 다시 살리는 수고를 하면 여러대를 사용할수도 있긴 하겠다만 구지…)해서 몇일간 Elasticsearch에 로깅해둔 데이터는 눈물을 머금고 기존 회원들의 이메일 리스트를 담고있는 sqlite3으로 백업 한 뒤 추가로 받았던 Elasticsearch용 서버는 다시 반납을 하게 되고, 이 데이터를 어찌 볼수 있을까 고민끝에 tui.chart를 사용하여 뷰를 만들게 된다. (아 물론, billboard.js도 훌륭하다. 개인 취향차이 같다. … )오픈소스인 kibana오픈소스인 kibana직접 만든 뷰, 직접 만들어서인지 더 이뻐보이는건 기분탓이겠지직접 만든 뷰, 직접 만들어서인지 더 이뻐보이는건 기분탓이겠지로깅 뷰는 http://daily-devblog.com/log/view 에서 확인이 가능하다. (최근 일주일의 데이터) 이번에 로깅 뷰를 만들면서 느낀거지만 kibana는 정말 좋은 오픈소스 툴인것 같다. 데이터 양이 많으면 자동으로 군집화(?)해서 보여주고… 이런게 스크립트단에서 처리를 하려면 엄청 복잡… 다시한번 ElasticStack을 찬양하며… # 메일 보내는 발송속도가 너무 느리다.👉 해결방안 : Thread 활용앞서 메일 보낼 데이터를 만드는 job과 메일을 보내는 job을 나눠서 메일을 보다 빠르게 발송할수 있었는데, 보내야 하는 사람이 많아지다 보니 그만큼 속도가 늦게 보내지는것이였다. 즉, 메일 보낼 사람의 메일이 구독 신청을 빨리해서 메일링 리스트 앞에 있다면 10시 부근에 받는데 최근에 구독 신청을 해서(늦게 구독 신청을 해서) 메일링 리스트의 뒷부분에 있다면 앞사람 다 보내고 받아야 하는 이슈가 있었다. 다시말하면 순차적으로 메일을 보내다보니 사람이 많아질수록 그만큼 늦게 받는 사람이 발생할수 있다는 것.이 문제는 간단히 스레드를 활용해서 해결할수 있었다. 테스트하면서 캡쳐해둔게 있는데 첫번째 스크린샷은 한사람에게 메일을 그냥 10번 반복해서 보냈고 두번째 스크린샷은 스레드를 활용해서 보내보았다. 확실히 스레드를 활용하니 속도가 빠른걸 확인할수 있었다.순차적으로 발송했을 경우 : 약 30초 소요순차적으로 발송했을 경우 : 약 30초 소요스레드를 활용했을 경우 : 거의 1초 내외스레드를 활용했을 경우 : 거의 1초 내외물론 AWS-SES에서 처리하는 메일발송 시간에 따라 달라지겠지만 실제로 스레드를 활용한 메일을 전부 열어보면 거의 동시에 온것을 확인할수 있었다. (구독자가 500명이 되었을때 확인해본 결과 1초 이내에 AWS-SES로 메일을 보낼수 있도록 처리한것을 확인할수 있었다. ) # 구독해제가 아닌 자체 수신거부는 어찌 처리할까?👉 해결방안 : AWS 반송/거부 확인 메일미약하지만 메일 발송도 AWS-SES를 사용하다보니 이제는 비용이 발생하게 되었다. 따라서 비용을 최소화 하기 위해서는 쓸떼없는 메일(?)을 발송해선 안된다. 그래서 매일 보내는 메일 하단에 구독 취소룰 할수 있도록 링크를 만들어 뒀는데 이렇게 정상적인 구독취소가 아닌 메일 자체를 사용하고 있는 메일 시스템에서 거부를 할 경우. 이를 어떻게 알고 처리할수 있을까?(역시 AWS~ AWS~)이러한 부분도 AWS에서 가이드를 해주고 있다. (무려 한글 ㅠㅠ 감사…)https://aws.amazon.com/ko/blogs/korea/automatic-managing-bounced-emails-by-amazon-ses/ 물론 또다른 문제들도 있었으나 메이저급(?) 문제들만 다뤄봤다. 가장 컸던건 앞서 말한 메일 스팸처리 ㅠㅠ 도메인도 구입하고 메일보내는데 비용도 들지만 그간 고생하고 삽질했던걸 생각하면 하나도 아깝지 않다. (어쩔수 없는 자본주의일까?)마지막으로 3부에서는 이러한 서비스를 이제 어떤식으로 운영할꺼고, 어떻게 발전시켜 나갈것이며, 수익 모델은 어떻게 생각하고 있는지에 대해 이야기 해보고자 한다. (수익모델이라 하지만… 그저 이 서비스를 운영하는 정도?;;)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[]},{"title":"기술블로그 구독서비스 개발 후기 - 1부","slug":"daily-dev-blog-1","date":"2018-08-05T01:27:21.000Z","updated":"2020-04-23T04:41:36.704Z","comments":true,"path":"2018/08/05/daily-dev-blog-1/","link":"","permalink":"https://taetaetae.github.io/2018/08/05/daily-dev-blog-1/","excerpt":"이번 포스팅은 약간의 자투리 시간을 활용하여 이것저것 만져보다 만들게 된 Daily DevBlog(기술블로그 구독서비스)에 대해 이야기 하려고 한다.","text":"이번 포스팅은 약간의 자투리 시간을 활용하여 이것저것 만져보다 만들게 된 Daily DevBlog(기술블로그 구독서비스)에 대해 이야기 하려고 한다. 하나의 글에 관련 내용을 모두 담기에는 양이 많아서 읽는사람도 지루하고, 글을 쓰는 필자 또한 어불성설 할것같아 크게 3개의 시리즈로 나눠서 최대한 자세하고 현장감(?)있게 글을 써보려고 노력했다. 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 글에 들어가기 앞서 최종 결과는 http://daily-devblog.com 에서 확인할수 있다. # 무엇이 나를 움직이게 했는가얼마전까지 오픈소스는 정말 실력있는 개발자나 유명한 사람들 말고는 금기의 영역(?)이라고 생각했었지만 최근 오픈소스 개발자 이야기 세미나를 다녀온뒤 마음속에 있었던 벽이 사라지는듯 했다. 세미나를 들으면서 ‘나도 무언가를 만들어 볼수는 없을까?’, ‘회사라는 명찰을 떼면 난 어느 수준에서 개발을 하고 있는 것일까?’ 등 여러 생각들이 머리를 멤돌다 개발자를 위한 글쓰기라는 글에서 기술블로그들을 모아놓은 awesome-devblog를 소개하는 글을 보게 되었고 내 머릿속에 정리안되던 그 생각들은 “이 데이터를 활용해서 무언가를 만들어보자!”로 귀결되었다. 다른 이야기 이지만, awesome-devblog 을 보고 당장 내 블로그도 등록해야지 했었는데 이미 등록이 되어 있었다;; 등록해주신 분께 감사하다는 생각이 들기전에 내 블로그가 누군가에게 보여지고 있구나 하며 새삼 놀라움이 더 컸다. # 요구사항과 도구 그리고 설계만들려고 생각해봤던 요구사항은 다음과 같다. 마치 회사에서 개발전 스펙을 정리하듯… 웹페이지를 활용해서 구독하고자 하는 사람들의 이메일을 수집할수 있어야 한다. 매일 전날 작성된 글을 수집하고 조합하여 구독하고자 하는 사람들에게 메일을 보낼수 있어야 한다. 위 두가지만 보면 너무 간단했다. 또한 기존에 사용하지 않았던 기술들을 사용해보면서 최대한 심플하게 개발하는것을 첫 개인 프로젝트의 목표로 하고 싶었다. 하여 생각한 아키텍처는 다음과 같다. 최대한 심플하게 설계해보자.최대한 심플하게 설계해보자. 데이터는 해당 github에 있길래 그냥 가져다 쓰려고 했으나 그래도 데이터를 관리하시는 분께 허락을 받고 사용하는게 상도덕(?)인것 같아 수소문끝에 연락을 해서 허락받는데 성공하였다. 데이터 사용을 허락해주신 천사같으신분...데이터 사용을 허락해주신 천사같으신분... 이 자리를 빌어 데이터를 사용할수 있도록 허락해주신분 께 감사인사를 표합니다. 홈페이지를 만들기 위해서는 이제껏 삼겹살에 소주처럼(응?) Java에 Spring을 사용해 왔었지만 이번엔 좀 다른 방식을 사용하고 싶었다. 물론 삼겹살에 맥주, 치킨에 소주를 먹어도 되긴 하지만...물론 삼겹살에 맥주, 치킨에 소주를 먹어도 되긴 하지만... 최근에 Flask라는 python기반 웹 프레임워크를 만져본 경험이 있어서 이렇다할 고민없이 빠른 결정을 할수 있었다. 또한 DB는 mysql 이나 기타 memory DB를 사용할까 했지만 이또한 심플하게 파일을 활용하는 sqlite3 을 사용하고자 하였다. # 웹서버_최종_수정_파이널_진짜_확정Flask를 활용하기 위해서는 당연히 웹서버가 필요했다. 처음엔 awesome-devblog에서도 사용하고 있던 https://www.heroku.com/ 를 이용해서 해보려 했으나 매일 구독자들에게 메일을 보내는 등 스케쥴러 기능같은건 구현하기 힘들었고 인스턴트 어플리케이션을 등록하는 형태라 사용자의 메일을 입력받고 저장하는 로직을 만들기는 어려워 보였다. (필자가 heroku를 너무 수박 겉핥기식으로 봐서 일수도 있다…)좀더 찾아보니 https://www.pythonanywhere.com/ 라는 제한적이지만 무료 서비스가 있었는데 웹콘솔도 지원하고 상당히 매력있어 보여서 이거다! 하며 개발을 시작을 했으나 (나름 도메인까지 그럴싸하게 만들었지만… http://dailydevblog.pythonanywhere.com/ ) 세상에 공짜는 없다는 말을 실감하며 앞서 말했던 요구사항을 완벽하게 구현할 수 없는 상황이였다.(request 제한, 스케쥴러 등록 개수 제한 등 보다 여러기능을 사용하기 위해서는 돈을 내고 써야…)마지막 희망으로 언제샀는지 서랍속 깊이 자고있던 라즈베리 파이를 꺼내서 공유기 DDNS설정을 하고 라즈베리안을 설치하며 웹서버를 위한 셋팅을 시도해보았으나 언제나 그렇듯 (시험공부 하기전에 책상 정리하고 괜히 방청소까지 하다가 피곤해서 자버리는듯한 느낌) 배보다 배꼽이 클것같아 이또한 진행하다가 중단하게 된다.결국 AWS에서 1년동안은 무료로 사용할수 있는 Free Tier 라는걸 발견하고 이참에 나도한번 사용해보자라는 마음을 가지고 과금되지 않게 조심조심 셋팅을 할수 있었다. 물론 뒤에서 이야기 하겠지만 약간의 과금은 필요했다ㅠ (나름 심도깊었던 고민을 한방에 해결해버리는 AWS 짱;; 이래서 AWS~ AWS~ 하는가 싶었다.) 돌고 돌다 킹갓엠페러제네럴 AWS를 만나게 된다.돌고 돌다 킹갓엠페러제네럴 AWS를 만나게 된다. # AWS와 DNS, 그리고 Elastic Stack까지AWS Free Tier 에서는 참 고맙게도 다른사람들이 접속할수있도록 공인 IP를 제공해 줬다. 그래서 아파치에 Flask를 연동까지 하고 최대한 심플하게 웹페이지를 작성하여 접속이 가능하도록 했는데 문제는 IP주소로 서비스를 하기에는 뭔가 2% 부족해보였다. 그래서 무료 도메인을 찾아다니던 도중 https://freedns.afraid.org/ 이라는 서비스를 찾고 결국 http://daily-devblog.mooo.com 라는 도메인으로 AWS에 설정된 공인IP를 연동시킬수 있었다. (지금은 redirect 시켜둔 상태, 2부에서 왜 도메인을 구입했는지에 설명할 예정이다.)또한 예전 조직장님의 말씀이 떠올라 서비스에서의 또다른 인사이트를 찾기 위해 EC2 서버 한대 더 발급받아 (한대에 전부 올리기에는 메모리가 부족했다;;) Elastic Stack 을 셋팅하여 로깅을 할수 있었고 이를 아래 그림처럼 키바나로 시각화 할수 있었다. (이 얼마나 아름다운가… 다시한번 Elastic Stack 에 무한 감동을;;) 각종 로깅을 통해 만들어낸 우아한 대시보드각종 로깅을 통해 만들어낸 우아한 대시보드 하지만 탄탄대로일것만 같았던 첫번째 개인 프로젝트는 여러가지 문제점들이 발생하였고 한 일주일동안 두세시간 자며 트러블 슈팅을 해야만 했었다. (업무시간을 피해가며 하다보니 시간이 나질 않았다…) 우선 여기까지, 서비스를 만들게 된 계기와 전체 구조에 대해 이야기를 해보았고 다음 2부에서 하게될 이야기는 이보다 훨씬더 복잡하고 피곤한(?) 이야기로 포스팅 될것같다. 이야기의 흐름이 문제 - 해결, 문제 - 해결 식의 java.util.Map 구조라고 해야하나… 2부를 기대해 본다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"aws","slug":"aws","permalink":"https://taetaetae.github.io/tags/aws/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"2018 오픈소스개발자이야기 후기","slug":"open-source-software-develpoer-story-review","date":"2018-07-01T01:00:00.000Z","updated":"2020-04-23T04:41:36.811Z","comments":true,"path":"2018/07/01/open-source-software-develpoer-story-review/","link":"","permalink":"https://taetaetae.github.io/2018/07/01/open-source-software-develpoer-story-review/","excerpt":"Facebook그룹들을 눈팅하다(?) OSS개발자 포럼에서 오픈소스 개발자이야기라는 주제로 세미나를 주최한다는 공지를 보게되었다. 언제부턴가 트랜드에 뒤쳐지지 않으려는 몸부림중 세미나같은 외부 개발 행사에 참여해보자는 마음으로 공지를 보자마자 홀린듯이 신청을 하게 되었고","text":"Facebook그룹들을 눈팅하다(?) OSS개발자 포럼에서 오픈소스 개발자이야기라는 주제로 세미나를 주최한다는 공지를 보게되었다. 언제부턴가 트랜드에 뒤쳐지지 않으려는 몸부림중 세미나같은 외부 개발 행사에 참여해보자는 마음으로 공지를 보자마자 홀린듯이 신청을 하게 되었고 세미나를 듣고 감흥이 가시기 전에 후기를 적고자 한다.(시간이 지나면 잊어버릴것만같은, 보고 들은 생생한 그 무언가를 얻었기에…) 비가오는 주말이였지만 많이 배우고 오자는 설레임을 갖고 서울 광화문 근처에 있는 한국마이크로소프트로 가게되었다. 말로만 듣던 MS사 로고를 보고 사람들이 하나둘씩 모이는걸 보니 뭔가 배울수 있겠구나 하는 기대감이 생겼다. 사실 오픈소스를 사용만 해본 입장이라 실제 오픈소스에 기여하시는 분들은 어떤 생각들을 갖고 계시는지가 가장 궁금했고 개발자인 나도 언젠간 오픈소스에 기여할수있지 않을까 하는 생각을 하며 발표를 들었다. # 회색지대 : 이상과 현실 - 오픈소스 저작권 / 신정규 님 오픈소스는 아무리 말그대로 Open이지만 오픈소스마다 다양한 저작권을 갖고있고 서로 쟁취하려는 싸움이 많이 발생한다고 한다. 그러다보니 어떠한 프로그램을 만들고 오픈소스화 시킬때도 라이센스의 종류를 잘 결정해야 추후 불이익을 당하는 상황을 모면할수 있다고 한다. 특허와 라이센스는 같은 말이면서도 다른데 아래 표처럼 각 상황에 따라 다른 부분을 확인할수 있었다. 특허 라이센스 권리발생 출원, 심사, 등록 창작과 동시 발생 권리내용 독점배타적 실시권 인격권/재산권 효력범위 아이디어의 동일성 표현의 실질적 유사성 첫 시간이기도 하였고 아무래도 주제가 끝장토론을 해도 안끝날 주제였던지라 정해진 시간을 넘길정도로 이야기를 많이 해주셨다. 특히 오픈소스 관련된 이야기는 사례를 이야기 해야 재밌다고 하셨는데 시간관계상 몇가지만 말씀해주셨다. 링크가 맞는지는 모르겠으나 첨부해본다. 엘림넷 vs 하이온넷 사건 EFM Networks 오라클 VS 구글 오픈소스 개발에 대해 단순하게 누구나 수정할수 있는 환경 이라고만 생각을 하고있다가 이런 복잡한 라이센스 문제가 나오니 약간 어려웠지만, 오픈소스의 생태계를 알고 발을 들이기 위해서는 어느정도의 히스토리는 알아야 겠다고 느끼게 되었다. # Elastic 에서 Remote 로 일하기 / 김종민 님 그전부터 Elastic 제품들을 실무에서도 사용해 왔었기에 개인적으로 오늘 발표중에 가장 궁금했었고, 관심이 있던 시간이였다. 발표에 앞서 어떻게 Elastic에 들어가게 되셨고 회사 소개를 간단히 해주셨는데 생각보다 어마어마한 회사다고 느낄수 있었다.(800여명중 한국엔 9명 / 네덜란드 출신 스타트업 인데 본사는 캘리포니아 마운틴뷰에 있고 등등)원격근무는 편하고 비용이 절약되는 장점이 있으나 동료들간의 유대감형성이 힘들거나 회의시 집중이 힘든 단점도 있다는 점을 말씀해 주셨다.시간관계상 몇가지 링크들을 소개해주셨는데 나중에 봐볼 생각이다. 침대에서 회사까지 1분 [마소 392호] 리모트 워크의 중심에 서보다 원격 툴로는 다음과 같이 사용한다고 한다. github : 슬랙연동, 개발뿐 아니라 운영/기획/이벤트 공유시 활용 Google Apps Slack : 봇 활용 (다양한 종류의 봇, 상황마다 특정 알림을 준다.) salesforce : CRM 툴 zoom : 화상회의 200명 동시콜 가능, 회의가 끝나면 녹음/녹화/스크립팅까지 가능하다고 한다 (wow) pinboard : 근태/인사 관리용 앱 jira는 사용 잘 안함 나중에 팀 내 Slack 봇으로 여러가지 다양한 자동화를 구성할수 있을것 같다는 생각이 들었다. # 오픈소스 생태계 일원으로서의 개발자 / 변정훈 님 사회자분이 “아웃사이더님”이라고 하시길래 설마 했다. 뭐가 잘 안되면 구글링을 하게되는데 내가 자주 보던 블로그를 운영하셨던 분이 내눈앞에 ㄷㄷ…언제부터 해야지~가 아니고 개발하다보니 어느새 오픈소스에 참여하고 있었다고 한다. 또한 참여하는게 아니고 이미 오픈소스 생태계속에서 살고있는 우리들이라 말씀하시고, 오픈소스 Contribution 방법으로는 사용/홍보/번역/리포팅/문서화/코드제출 등 다양하게 있으니 어렵게 생각하지 말자 라고 하셨다. 오픈소스에서 배울수 있는점은 커뮤니케이션의 방법, 협업의 방법과 중요성, 테스트코드의 중요성, 지속적 통합/지속적 배포, 코드의 품질관리 라고 한다.점점 발표를 들으면서 오픈소스에 대한 생각이 바뀌고 있는 내 자신을 느낄수 있었다. 너무 어렵게만 생각해 온것 같다는. # 해외 오픈소스 컨퍼런스 발표와 참여 / 송태웅 님 리눅스 파운데이션 이벤트에서 밸표했던 경험을 공유해주셨다. 발표에 앞서 나이가 어려보였는데 저렇게 세계적인 곳에 가서 발표를 할수있다는 용기와 대범함에 발표를 듣는 내내 놀라움을 감출수 없었다.특히 영어에 대해 이야기를 해주셨는데 영어지만 IT전문용어가 있기 때문에 어느정도 질문에 답변이 가능할수 있었고, 영어를 잘하는것보다 기술을 더 깊이 이해하는게 좋다고 한다.(물론 기본적인 영어실력은 필수) 그리고 인상깊었던 말중에 “세미나 같은곳에 가면 질문 하나는 꼭 하자, 그냥 듣는것보다 질문에 대한 내용은 오래 기억이 남으니까” 라는 말이 뒤통수를 쌔게 후려 쳤다. 하지만 오늘 질문은… 못했다. 다음엔 꼭 하리라 다짐을 하며… # 파이썬, 파이콘, 파이썬소프트웨어재단 / 김영근 님 PSF(Python Software Foundation)에서 하는 일과 간략한 파이썬에 대한 이야기를 해주셨다.여담으로, 파이썬 2.x는 파이썬 커뮤니티에서는 전전전 여친 수준으로 잊고 있으니 3.x를 사용하라는 말씀도 해주셨다. 존중과 다양성을 가치로 두고있고 예컨데, 필리핀에서는 어린아이도 발표를 하고 여성 발표자도 점점 늘어나는 추세라고 한다. 파이썬은 문제 해결 하는데 있어 적어도 두번째로 좋은 언어다 라고 자부할수 있고, 애니메이션ㆍ영화 에서도 파이썬을 사용하고 있다고 한다. 또한 스프린트 라는 행사를 개최하여 외부에서 사람들이 직접 Contribution을 할수 있도록 도와주면서 누구나 파이썬에 Contribution을 할수 있는 순환구조를 만든다고 한다.가장 인상깊었던 문구가 있었는데 사진은 못찍었고 대신 텍스트로 남겨본다.1234567동료 의식에서 시작되는 존중과 배려- 를 바탕으로한 소속감과 참여감- 에서 비롯되는 기여- 를 통해 발전하는 생태계- 에 모여드는 사람들- 사이에서 싹트는 동료의식(반복) 딱! 내가 바라는(?) 개발 문화다. 이번 파이콘 행사가 기대가 더욱더 기대가 된다! # 아파치 제플린, 프로젝트 시작부터 아파치 탑레벨 프로젝트가 되기까지 / 이문수 님 아파치 제플린은 실무에서도 자주 사용하고 있는 오픈소스이다. 그런데 이 발표를 하기위해 저멀리 미국에서 오셨다는 아파치 제플린의 CTO님. 개인적으로 정말 대단해 보였다. 특히 제플린을 만든 계기부터 아파치 탑레벨 프로젝트로 되기까지의 과정이 정말 매력적으로 보였는데 2013년 10월경 하둡관련 화면단의 분석시스템이 없어 불편해서 만들기 시작하였고, 이를 여러 커뮤니티에 직접 홍보를 해가며 인큐베이션을 거쳐 결국 아파치 프로젝트로 되었다는 이야기.왜 나는 못할까 라는 자괴감도 살짝 들었으나 저분의 끈기와 실행력은 높이 평가하고 싶고 또한 배우고 싶은 부분이였다. (외모는 여느 CTO처럼 거만하지 않았는데 말은 조리있게 잘하시고^^;) # 오픈소스 개발자에게 듣다(대담)질의응답 시간이였다.Q. 오픈소스에 쉽게 기여할수 있는 방법? Github에 가보면 초보자들을 위한 이슈를 남겨놓은 경우도 있다. 조그마한 문서 수정부터 시작하자. 기여는 코드만 있는게 아니다. (e.g. 버그리포트 등) Q. 오픈소스 홍수시대에 살고있는데 어떤것을 파야하나요? 유명한 코드는 너무 방대해서 보기 힘들다. 그걸 만든 사람의 토이 프로젝트를 보며 감을 익혀보자. Q. 처음으로 기여한 오픈소스는 무엇이며 어떻게 기여하였나? 문서수정, 간단한 텍스트를 수정하는것부터 시작 내가 사용하는 프로젝트를 관심있게 보다가 인사이트를 찾아 수정 번역 또한 오픈소스에 기여 Q. 오픈소스에서 활동하는것과 회사 업무와 비교시 장단점, 입사시 유리한가? 회사업무보단 재밌다. Github계정을 기준으로 평가하기도 한다. 오픈소스 컨튜리뷰션은 내 개발 역사라고 볼수있다. Q. 개발자라면 꼭 읽어봐야할 책이 있다면 추천 개발보단 사람과의 커뮤니케이션 관련된 책 기본서는 여러번 읽을수록 새롭다. 마치며전망좋은 MS전망좋은 MS 일명 네임드 개발자분들을 만나고 나니 한편으론 부럽기도 하였지만 한편으론 동기부여를 얻어간다. 첫불에 배부르랴, 조그마한것부터 시작하다보면 나도 언젠간 네임드 소리를 들으며 지금보다 조금이라도 발전된 내가 될수 있다는 자신감을 얻을수 있었던, 직장인으로써 황금같은 토요일이 아깝지 않을 만큼 정말 좋았던 시간이였다! (행사에 자주 와야겠다!)","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"oss","slug":"oss","permalink":"https://taetaetae.github.io/tags/oss/"},{"name":"open source software","slug":"open-source-software","permalink":"https://taetaetae.github.io/tags/open-source-software/"},{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"}]},{"title":"초간단 API서버 만들기 - 2부 (Python + Flask + Nginx)","slug":"simple-web-server-flask-nginx","date":"2018-06-30T17:00:00.000Z","updated":"2020-04-23T04:41:36.854Z","comments":true,"path":"2018/07/01/simple-web-server-flask-nginx/","link":"","permalink":"https://taetaetae.github.io/2018/07/01/simple-web-server-flask-nginx/","excerpt":"지난포스팅에 이어 이번엔 Flask와 Nginx를 연동하는 방법을 정리해보고자 한다. Apache로 연동했는데 왜 또 Nginx로 연동하는걸 정리하지(?)하며 의문이 들수 있는데 다른 포스팅을 봐도","text":"지난포스팅에 이어 이번엔 Flask와 Nginx를 연동하는 방법을 정리해보고자 한다. Apache로 연동했는데 왜 또 Nginx로 연동하는걸 정리하지(?)하며 의문이 들수 있는데 다른 포스팅을 봐도 Apache + Flask 조합보다 Nginx + Flask 조합이 더 많고 지난 포스팅에서도 알수있었듯이 (Apache VS Nginx) 둘중 어느것이 좋다고 할수도 없고 각 상황에서 연동하는 방법을 알고 있다면 이 또한 나만의 무기가 될것같아 Nginx를 연동하는 방법을 정리해보려 한다. 1부에서 왜 Flask인가, Flask의 장점에 대해 정리를 했으니 이번 포스팅에서는 별도로 작성하진 않는다. # Nginx 설치 ( https://nginx.org/en/ )역시 소스설치를 한다.123456789101112- 다운을 받고$ https://nginx.org/download/nginx-1.14.0.tar.gz- 압축을 푼 다음$ tar -zxvf nginx-1.14.0.tar.gz- 폴더로 이동해서 $ cd nginx-1.14.0- 설치할 디렉토리를 설정하고$ ./configure --prefix=/~~~/apps/nginx- make 파일을 만들고$ make- 설치를 진행한다.$ make install 이렇게 하면 일단 Nginx는 설치가 되었다. # uWSGI 설치 ( https://uwsgi-docs.readthedocs.io/ )앞서 Apache와 연동할때는 별도의 모듈을 Apache에게 등록하는 형태였다면 Nginx는 WSGI프로토콜을 활용하는 WSGI 어플리케이션을 실행하는 어플리케이션 서버를 활용해야 한다.12345678- 다운을 받고$ wget https://projects.unbit.it/downloads/uwsgi-latest.tar.gz- 압축을 풀고$ tar zxvf uwsgi-latest.tar.gz- 폴더로 이동하여$ cd uwsgi-2.0.17- make 명령어를 호출하면 &apos;uwsgi&apos;이라는 실행파일이 생성된다.$ make # Nginx 설정Apache와 비슷하게 uWSGI 관련 설정을 해준다.123456789server &#123; listen 80; server_name localhost; location / &#123; # ( / ) 경로로 들어올 경우 include uwsgi_params; # GET/POST 등 기본적으로 필요한 환경변수를 include 해준다. uwsgi_pass 127.0.0.1:3031; # 요청을 IP:PORT로 전달한다. &#125;&#125; 별도의 모듈을 사용하지 않기때문에 전달해주는 (proxy느낌) 설정을 해준다. # uWSGI 실행 및 Nginx 재시작앞서 설치한 uwsgi를 아래처럼 IP:port 를 명시적으로 적어주고 (위에서 전달받은 IP:PORT와 동일하게) Apache 연동시 활용했던 wsgi파일을 이번에도 동일하게 사용하도록 해서 실행한다.1$ ./uwsgi -s 127.0.0.1:3031 --wsgi-file /~~~/python_app/hello_world.wsgi 이렇게 하면 background로 실행되는게 아닌 foreground로 실행되기 때문에 &amp;을 사용한다던지 해서 background로 실행되도록 해준다. 그후 Nginx를 재시작 해주면 원하는 그토록 원했던 Hello World!를 만날수가 있게 된다. Apache연동과 조금 다른점은 모듈을 사용하지않고 별도의 전달 어플리케이션(?)이 필요하다는점이다. 간단히 Apache처럼 모듈만 넣으면 되는게 아니라서 불편할수도 있을것 같지만 한편으로는 관리할수있는 포인트가 더 늘어난 셈이라 어떤 측면에서는 활용할수 있는 방법이 하나 늘어난것으로 볼수도 있다. # 마치며막상 정리하고 나면 아무것도 아닌데 알기 위해서 몸부림을 쳐가며 책이며 구글링을 하는 과정을 통해 점점 성장을 하는것 같다. (성장통이라고나 할까) 이렇게 단순히 Flask를 할수있다 가 아닌 웹서버를 연동할수있다. 그것도 Apache와 Nginx 두개나. 이것도 언젠간 나만의 무기가 되지 않을까?","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"https://taetaetae.github.io/tags/nginx/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"초간단 API서버 만들기 - 1부 (Python + Flask + Apache)","slug":"simple-web-server-flask-apache","date":"2018-06-29T14:00:00.000Z","updated":"2020-04-23T04:41:36.851Z","comments":true,"path":"2018/06/29/simple-web-server-flask-apache/","link":"","permalink":"https://taetaetae.github.io/2018/06/29/simple-web-server-flask-apache/","excerpt":"Static한 HTML이 아닌 로직이 필요한 API서버를 구성한다고 가정해보자. (이제까지 지식으로)처음 머릿속에 떠오르는건 Java를 사용하고 스프링으로 어플리케이션을 만들고 apache에 tomcat을 연동한 다음 …","text":"Static한 HTML이 아닌 로직이 필요한 API서버를 구성한다고 가정해보자. (이제까지 지식으로)처음 머릿속에 떠오르는건 Java를 사용하고 스프링으로 어플리케이션을 만들고 apache에 tomcat을 연동한 다음 … 이러한 방법으로 API서버를 구성할수 있겠지만 프로토타이핑 또는 테스트 목적으로 만들기 위해서는 설정하는 시간이 은근 많이 소요된다. (물론 Java Config, Spring Boot 등 간소해졌지만…)얼마전부터 Python에 대한 매력을 뼈저리게 느끼고 있다보니 Python으로 API서버를 구성할순 없을까 알아봤고 (모바일 게임 듀랑고 서버가 python이라고 하기도 하고…) Flask와 Django가 있어서 둘다 써본 결과 필자는 Flask가 맞겠다고 생각해서 정리를 해볼까 한다. ‘장고’라고도 불리는 Django에는 모든것들이 다 들어가 있어서 사용하기 너무 편리하다. (DB, 어드민 등 ) 하지만 Flask는 내가 사용할 것들만 import해서 사용하는 방식이라 어떤 측면에서는 아무것도 없다 할수 있겠으나 커스터마이징에 용이하다고 볼수 있었기에 Flask를 선택하게 되었다. (Django가 Flask보다 안좋다는 말은 아니니 오해는 하지 마시길…) 글쓰기에 앞서 본 포스팅은 2개의 포스팅에 걸쳐 시리즈(?)형식으로 작성할 예정이다. 1부에서는 Flask가 무엇이고 이를 어떻게 사용하며 Apache와 연동하는 방법을 소개하고, 2부에서는 Nginx와 연동하는 방법을 소개한다.환경은 다음과 같다. CentOS 7.4 Python 3.6 (기본은 2.7이였으나 추가로 설치) # Flask ( http://flask.pocoo.org/ )공식 홈페이지에서도 보면 알수 있듯이 너~무 간단하다. 단지 아래 코드 몇줄만 작성하면 우리가 모든 프로그램 초기 작성시 항상 만나는 “Hello World”를 볼수 있다.12345678#hello_world.pyfrom flask import Flaskapp = Flask(__name__)@app.route(\"/\")def hello(): return \"Hello World!\" 위와같이 작성하고 python hello.py로 실행해두고 브라우저에서 http://127.0.0.1:5000 을 요청하면 반가운 Hello World를 만날수 있다. (너무 간단;;) 자세한 문법은 도큐먼트를 참조하면 될듯하고 이 Flask를 잘만 활용한다면 보다 빠르고 간단하게 API서버를 구성할수 있을거라 생각한다. Hello World를 찍었으면 된거 아닌가 라고 질문할수도 있겠으나 실제 서비스에서 사용하기 위해서는 앞단에 웹서버를 두는게 여러 측면에서 효율적이다. 주로 사용하는 웹서버는 Apache 와 Nginx가 있는데 여기서는 Apache와 연동하는 방법을 정리 해보고자 한다. # Apache 설치 ( http://archive.apache.org/ )우선 필자는 yum 이나 apt-get처럼 패키지 관리자로 설치하는것을 그렇게 좋아하지 않는다. 이유는 커스터마이징을 할 경우 시스템 어느곳에 설치되어있는지를 한눈에 파악하기 어렵고 윈도우경우 Program Files처럼 내가 추가로 설치하고 관리하는 프로그램들을 한곳에서 관리하고 싶기에 왠만하면 소스를 직접 컴파일하여 설치하곤 한다. 이번 역시 아파치도 소스로 설치하려고 한다.현재 아파치는 2.4버전이 Stable버전으로 되어있지만 보다 레퍼런스가 많은 2.2버전으로 설치하기 위해 어렵게 아카이빙된 경로를 통해 다운을 받고 설치를 한다.123456789101112- 다운을 받고$ wget http://archive.apache.org/dist/httpd/httpd-2.2.29.tar.gz - 압축을 푼 다음$ tar xvzf httpd-2.2.29.tar.gz- 해당 폴더로 들어가$ cd httpd-2.2.29- 컴파일 후 설치 경로를 정해주고$ ./configure --prefix=/~~~/apps/apache- make 파일을 만든다음$ make- 설치를 해준다.$ make install 이렇게 되면 /~~~/apps/apache/ 하위에 필요한 파일들이 설치가 되는데 root계정이 아닌 일반계정으로 실행하기 위해서는 /bin하위에 있는 httpd에 대한 실행/소유권한을 변경해줘야 한다. (아니면 그냥 root권한으로 시작/종료. 왜? Apache는 80port를 사용하는데 일반적으로 리눅스에서는 1024 아래 port를 컨트롤 하기 위해서는 root권한이 있어야 사용이 가능, 그게 아니라면 이처럼 별도의 설정이 필요하다.) 12$ sudo chown root:계정명 httpd$ sudo chmod +s httpd # mod_wsgi 설치 ( https://code.google.com/archive/p/modwsgi/ )웹 서버 게이트웨이 인터페이스(WSGI, Web Server Gateway Interface)는 웹서버와 웹 애플리케이션의 인터페이스를 위한 파이선 프레임워크다. 라고 정의되어있다. 즉, 웹서버(Apache)와 위에서 만든 Flask 어플리케이션을 연동해주기 위한 프레임워크이다. 이또한 소스로 설치해보자. (위와 같은 이유로~)12345678910111213- 다운을 받고$ wget https://github.com/GrahamDumpleton/mod_wsgi/archive/3.5.tar.gz- 압축을 푼 다음$ tar -zxvf 3.5.tar.gz- 폴더에 들어가서$ cd mod_wsgi-3.5- 아파치의 빌드툴인 apxs의 경로를 설정해주고- 필자와 같이 기본 python 버전을 사용하지 않을꺼라면 꼭 python경로를 설정해줘야 한다! (중요)$ ./configure --with-apxs=/~~~/apps/apache/bin/apxs --with-python=/usr/bin/python3.6- make 파일을 만들고$ make- 설치~$ make install 이렇게 설치를 하면 자동으로 아파치 하위 /modules 폴더안에 mod_wsgi.so 파일이 생긴다. (필자는 이것도 모르고 mod_wsgi.so파일을 다운 받으려고 구글링을 몇일째 했던 기억이 ㅠ) # Apache httpd.conf 설정Aapche에 mod_wsgi 모듈이 생겼고, 이를 적용하기 위해서 httd.conf 아파치 기본설정 파일을 수정해야 한다.123456789101112- 모듈을 사용하겠다고 정의LoadModule wsgi_module modules/mod_wsgi.so- mod_wsgi로 실행한 WSGI 파이썬 어플리케이션을 특정 URL( / )로 설정하기 위해 다음과 같이 등록해준다.WSGIScriptAlias / /~~~/python_app/hello_world.wsgi- 등록된 파이썬 어플리케이션을 데몬 프로세스로 실행하기위해 다음과 같이 설정해준다.WSGIDaemonProcess hello_world(어플리케이션 명) user=계정명 group=계정명 threads=5(스레드 개수)&lt;Directory /~~~/python_app&gt; WSGIApplicationGroup %&#123;GLOBAL&#125; # 해당 어플리케이션을 처리하는 프로세스에서 첫번째로 생성된 파이썬 인터프리터를 사용 Order deny,allow Allow from all&lt;/Directory&gt; 자세한 내용은 공식 홈페이지를 참고하는걸 추천한다. # WSGI 파일 작성Apache 설정에서 등록한 wsgi파일은 다음과 같이 작성해준다. 물론 이 내용도 공식 홈페이지를 참조하는게 좋다.123import syssys.path.insert(0, '/~~~/python_app')from hello_world import app as application 필자는 여기서 한참을 해맸던 부분이, sys.path.insert구문의 두번째 인자는 실제 실행될 Falsk 어플리케이션이 있는 경로를 적어줘야 하고, hello_world는 Flask 어플리케이션 파일의 확장자를 제외한 이름을 적어주면 된다. 이렇게 한 후 아파치를 시작해주면 앞서 만든 Flask 어플리케이션을 별도로 실행해 주지 않아도 Apache가 알아서 설정한 URL로 요청이 들어올경우 Flask 어플리케이션으로 전달해준다. # 마치며사실 이렇게 Apache를 연동하면서 까지 하게 된 계기는, Flask 어플리케이션 구동시 80 port를 받도록 구현하고 root권한을 가진 계정으로 실행하도록 해뒀는데 가끔 종료가 되는 부분을 해결하고자 시작하게 되었다.이 밖에도 Flask의 다양한 기능들과 Flask만 제외하면 일반 Python 이기 때문에 활용성은 무궁무진할것으로 보인다. 2부로는 또다른 웹서버인 Nginx를 연동하는 방법을 알아보고자 한다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"Apache냐 Nginx냐, 그것이 알고싶다.","slug":"apache-vs-nginx","date":"2018-06-27T08:17:33.000Z","updated":"2020-04-23T04:41:36.675Z","comments":true,"path":"2018/06/27/apache-vs-nginx/","link":"","permalink":"https://taetaetae.github.io/2018/06/27/apache-vs-nginx/","excerpt":"웹서버는 HTTP 프로토콜을 통해 읽힐수 있는 문서를 처리를 하며 일반적으로 웹 어플리케이션의 앞단에 배치되곤 한다. 동적인 리소스는 WAS에게 처리하도록 하고 정적인 리소스를 보다 효율적으로 처리하기 위한 방법일수도 있다. 크게 Apache와 Nginx가 사용되곤 하는데 이 둘의 차이는 무엇일까?","text":"웹서버는 HTTP 프로토콜을 통해 읽힐수 있는 문서를 처리를 하며 일반적으로 웹 어플리케이션의 앞단에 배치되곤 한다. 동적인 리소스는 WAS에게 처리하도록 하고 정적인 리소스를 보다 효율적으로 처리하기 위한 방법일수도 있다. 크게 Apache와 Nginx가 사용되곤 하는데 이 둘의 차이는 무엇일까? 사실 필자는 사내에서 주로 Apache만 사용하다보니 Nginx는 그저 Apache와는 다른 방식의 웹서버다 또는 보다 경량화 되었다 정도로만 알고있었는데 이번기회를 통해 제대로 알고 비교를 해보면서 결국 어떤게 좋은지 알아보고자 한다. 구글링을 조금만 해보면 Apache와 Nginx를 비교하는 포스팅이 많이 나온다. 이번 포스팅의 목적 이러한 정보들을 단순히 요약/종합 하려는게 아니고, 최대한 실무 서비스를 운영하는 시각으로 정리하고자 함을 밝힌다. # Apache ( https://httpd.apache.org/ )우리나라에서 웹어플리케이션을 개발하는 사람들은 한번쯤은 들어봤을 Apache. 국내 일반적인 기업에서 웹서버의 표준으로 자리잡았다고 해도 과언이 아닐것 같다. Client에서 요청을 받으면 MPM (Multi Processing Module : 다중처리모듈) 이라는 방식으로 처리를 하는데 대표적으로는 Prefork와 Worker방식이 있다. 간단하게 어떤식으로 처리하는지 알고 넘어가자. Prefork MPMWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_htmlWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_html 실행중인 프로세스를 복제되어 처리가 된다. 각 프로세스는 한번에 한 연결만 처리하고 요청량이 많아질수록 프로세스는 증가하지만 복제시 메모리영역까지 복제되어 동작하므로 프로세스간 메모리 공유가 없어 안정적이라 볼수 있다. Worker MPMWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_htmlWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_html Prefork 동작방식이 1개의 프로세스가 1개의 스레드로 처리가 되었다면 Worker 동작방식은 1개의 프로세스가 각각 여러 쓰레드를 사용하게 된다. 쓰레드간의 메모리를 공유하며 PreFork방식보다 메모리를 덜 사용하는 장점이 있다. 참고로 WAS로 tomcat을 연동하는 경우라면 mod_jk, mod_proxy, mod_proxy_ajp 방식을 Apache 자체적으로 지원해주기 때문에 다양하고 효율적으로 tomcat을 연동할수 있다. 참고링크 # Nginx ( https://nginx.org/en/ )Nginx에 대해 살펴보기 전에 구글 트랜드를 활용하여 Nginx에 대한 관심이 어느정도인지를 보고 넘어가자.최근 5년간 구글트랜드, 파란색이 Apache이고 빨간색이 Nginx최근 5년간 구글트랜드, 파란색이 Apache이고 빨간색이 Nginx전세계는 Nginx보다는 Apache에 대한 관심이 많은것으로 보이는데 국내는 아주 조금씩 Nginx에 대한 관심이 오르는것을 볼수있었다. (그래도 아직은 Apache가 월등히 우세한 편이다.)그럼 Nginx는 어떤식으로 돌아가는 것일까? 가장 유명한(?) 특징이라면 Event Driven 방식을 꼽을수 있을것 같다. Event Driven 방식에 대해 잠깐 언급을 하고 넘어가면 요청이 들어오면 어떤 동작을 해야하는지만 알려주고 다른요청을 처리하는 방식이다. (Producer Consumer Pattern과 유사하다.) 그러다보니 프로세스를 fork하거나 쓰레드를 사용하는 아파치와는 달리 CPU와 관계없이 모든 IO들을 전부 Event Listener로 미루기 때문에 흐름이 끊기지 않고 응답이 빠르게 진행이 되어 1개의 프로세스로 더 빠른 작업이 가능하게 될수 있다. 이때문에 메모리적인 측면에서 Nginx가 System Resource를 적게 처리한다는 장점이 있다고 한다.Nginx Process Model (https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale)Nginx Process Model (https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale) # 그래서 뭐가 좋은가?그것이 알고싶다.그것이 알고싶다. 이 포스팅을 적으면서 마지막엔 Apache가 더좋다 또는 Nginx가 더좋다로 마무리를 짓고 싶었는데 어느 시사/교양 프로그램처럼 어쩔수 없는 열린결말로 마무리를 지을수밖에 없을것 같다. (어찌보면 이게 정답일수도?) 기술의 선택에 있어서 정답은 없는것 같다.(물론 Spring 을 사용하느냐 서블릿을 직접 구현하는냐 와는 좀 다른 성격의 이야기;;) 운영하고 있는 서비스의 상황을 잘 알고 튜닝을 해가면서 가장 효율적인것을 선택하는게 정답이라고 말할수 밖에… 커뮤니티 파워를 무시 못하기 때문에 Apache를 선택할수도 있을테고, 점점 관심도가 올라간다는건 그만큼의 장점이 있고 또한 메모리 측면에서 동접자 처리시 효율적인 Nginx를 사용할수 있을것 같다. 정리하면. 내가 사용하기에 어려움이 없는 도구를 잘 활용하는것, 그렇다고 오래된 기술이 편한다고 집착해서는 안되며, 새로운 기술을 두려워 하지말고 경험을 해본 뒤에 결정을 할것 이라고 내릴수 있을것 같다. (어렵다…ㅠㅠ) 참고 포스팅http://www.mukgee.com/?p=293http://knot.tistory.com/88http://tmondev.blog.me/220737182315http://tmondev.blog.me/220731906490http://urin79.com/blog/20654191http://jaweb.tistory.com/entry/apache-%EC%99%80-Nginx-%EB%AD%90%EA%B0%80-%EC%A2%8B%EC%9D%80%EA%B1%B0%EC%95%BC","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"https://taetaetae.github.io/tags/nginx/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"Event Driven","slug":"Event-Driven","permalink":"https://taetaetae.github.io/tags/Event-Driven/"},{"name":"Prefork MPM","slug":"Prefork-MPM","permalink":"https://taetaetae.github.io/tags/Prefork-MPM/"},{"name":"Worker MPM","slug":"Worker-MPM","permalink":"https://taetaetae.github.io/tags/Worker-MPM/"}]},{"title":"시계열 데이터를 분석하여 미래 예측 하기(Anomaly Detection)","slug":"anomaly-detection","date":"2018-05-31T08:03:41.000Z","updated":"2020-04-23T04:41:36.648Z","comments":true,"path":"2018/05/31/anomaly-detection/","link":"","permalink":"https://taetaetae.github.io/2018/05/31/anomaly-detection/","excerpt":"급변하는 날씨를 예측하려면 어떠한 정보가 있어야 할까?또는 마트를 운영하는 담당자인 경우 매장 운영시간을 정해야 한다면 어떠한 기준으로?뜨거운 감자인 비트코인 시장에서 수익을 얻으려면 어떤 정보들이 있어야 물리지(?) 않을수 있을까?","text":"급변하는 날씨를 예측하려면 어떠한 정보가 있어야 할까?또는 마트를 운영하는 담당자인 경우 매장 운영시간을 정해야 한다면 어떠한 기준으로?뜨거운 감자인 비트코인 시장에서 수익을 얻으려면 어떤 정보들이 있어야 물리지(?) 않을수 있을까? 위 질문에 공통된 정답은 예전 기록들인것 같다. 날씨예측은 기상청에서 과거 기록들을 보고 비가 올지 말지를 결정하고 ( 과거 날씨 서비스를 담당해봤지만 단순히 과거 기록들로 예측한다는건 불가능에 가깝긴 하다. ) 매장 운영시간은 예전에 손님들이 언제왔는지에 대한 데이터를 보고. 비트코인이나 주식은 차트를 보고 어느정도는 상승장일지 하락장일지 추측이 가능하다고 한다. ( 물론 호재/악재에 따라 흔들리지만..ㅠㅠ..?? )이처럼 시간의 흐름에 따라 만들어진 데이터를 분석하는것을 시계열 데이터 분석이라 부르고 있다. 필자가 운영하는 서비스에서 시계열 데이터 분석을 통해 장애를 사전에 방지하는 사례를 공유 해보고자 한다. # 상황파악부터손자병법에는 지피지기 백전불태 라는 말이 있다. 그만큼 현 상황을 잘 알아야 대응을 잘할수 있다는것. 필자가 운영하는 서비스는 PG(Payment Gateway) 서비스로 쇼핑몰같은 온/오프라인 사업자와 실제 카드사와의 중간 역활을 해주고 있다. 이를테면 사용자가 생수를 10,000원에 XX카드로 구매해줘 라고 요청이 오면 그 정보를 다시 형식에 맞춰 카드사로 전달하여 사용자가 물건을 구매할수 있도록 해준다.PG서비스 : 쇼핑몰과 카드사의 중간에서 릴레이 해주는 역활이라 보면된다.PG서비스 : 쇼핑몰과 카드사의 중간에서 릴레이 해주는 역활이라 보면된다. # 요구사항 및 과거 데이터 분석서비스를 운영해보니 감지하기 어려운 상황들이 있었다. 연동하는 쇼핑몰에서 문제가 발생하거나 네트워크 문제가 발생할경우 즉, 트래픽이 평소보다 적게 들어올 경우 정상적인 에러(e.g. 잔액부족) 가 갑자기 많이 발생할 경우 이를 분석하기위해 기존의 트래픽/데이터를 분석해봐야 했다.결제건수 Kibana Visualize, 기영이 패턴결제건수 Kibana Visualize, 기영이 패턴위 그래프는 결제데이터 카운트 인데 어느정도 패턴을 찾을수 있다.에러건수 Kibana Visualize, 악어 패턴..(무리수..)에러건수 Kibana Visualize, 악어 패턴..(무리수..)위 그래프는 에러카운트 인데 일정한 패턴 속에서 어느 지점에서는 튀는것을 확인할수 있다. (빨간색 영역) 그렇다면 어떤 방법으로 장애상황보다 앞서서 감지를 할수 있을까? ( 장애 : 어떠한 내/외부 요인으로 인해 정상적인 서비스가 되지 않는 상태 ) # 장애발생 전에 먼저 찾아보자!가장 간단하게는 기존 데이터를 보고 수동으로 설정하는 방법이 있을수 있다. 예로들어 자정 즈음에는 결제량이 가장 많기때문에 약 xx건으로 설정해두고, 새벽에는 결제량이 가장 적기 때문에 약 yy건으로 설정해둔 후 에러 건수나 결제건수에 대해 실시간으로 검사를 해가면서 설정한 값보다 벗어날 경우 알림을 주는 방법이다.하지만 아무리 과거 데이터를 완벽하게 분석했다 할지라도 24시간 모든 시점에서 예측은 벗어날 수밖에 없다. (예로들어 쇼핑 이벤트를 갑작스럽게 하게되면 결제량은 예측하지 못할정도로 늘어날테고…) 또한 설정한 예측값을 벗어날 경우 수동으로 다시 예측값을 조정해줘야 하는데, 이럴꺼면 24시간 종합 상황실에서 사람이 직접 눈으로 보는것 보다 못할것 같다. (인력 리소스가 충분하다면 뭐… 그렇게 해도 된다.) # 지난 데이터와 비교하기일주일 기준으로 지난 일주일과의 데이터를 비교해보는 방법또한 있다. 간단하게 설명하면 이번주 월요일 10시의 데이터와 지난주 월요일 10시의 데이터의 차이를 비교해보는 방법이다. 키바나에서 클릭 몇번만으로 시각화를 도와주는 Visualize 기능을 통해 지난 일주일과 이번주를 비교해보면 아래 그래프처럼 표현이 가능하다.일주일 전 데이터와 단순 비교일주일 전 데이터와 단순 비교이 경우도 지난주 상황과 이번주 상황이 다른 경우에는 원하는 비교 항목 외에 다른 요인이 추가되기 때문에 원하는 비교를 할수가 없고 위에서 수동으로 설정하는 방법과 별반 다를바 없을것으로 생각된다. # 조금더 우아하게! (언제부턴가 우아하단 말을 좋아하는것 같다..)개발자는 문제에 대해서 언제나 분석을 토대로 접근을 하는것을 목표로 해야한다. 언제부턴가 Hot한 머신러닝을 도입해 보고 싶었으나 아직 그런 실력이 되질 못하고… 폭풍 구글링을 통해 알게된 Facebook에서 만든 Prophet이라는 모듈을 활용해보고자 한다.https://opensource.fb.com/#artificial 이곳에 가보면 여러 Artificial Intelligence 관련된 오픈소스들중에 Prophet 모듈을 찾을수 있다. 다행히도 BSD License라서 실무에서도 다양하게 활용할수 있을것으로 보인다. 친절하게도 Quick Start을 통해 어떤식으로 예측을 하는지 보여준다. 참고로 Python 과 R 을 지원한다. (python 의 대단함을 다시금 느끼며…)구성은 CentOS 7 + python3.6 + jenkins 를 활용한다. (python 경험이 부족하므로 코드가 허접할수도 있으니 양해바란다.)데이터 분석시 가장 많이 사용된다는 Pandas와 대규모 다차원 배열을 쉽게 처리 할 수 있게 해주는 numpy, 그리고 bprophet를 비롯한 필요한 모듈들을 pip로 설치해준다.123pip install pandaspip install fbprophetpip install numpy 필요한 모듈들을 import를 한다.123456import matplotlibmatplotlib.use('Agg') // centos 서버 환경에서 돌릴경우 예측결과를 그래프로 보는 과정에서 오류가 발생한다. 이를 방직하기 위해 해당 코드를 적어준다.import pandas as pdimport requests, datetimefrom fbprophet import Prophet 기존에 데이터들은 모두 실시간으로 elasticserach에 인덱싱 중이기 때문에 rest api 를 활용하면 쉽게 데이터를 얻을수가 있다. (RDB로 관리를 했더라면… 배보다 배꼽이 더 큰 상황이였지 않았을까 하는 생각이 든다.)123456789101112131415161718192021# 현재 시 기준 2주 (24 x 14 시간) 전의 데이터를 가져온다.day_gap = 14now_datetime = datetime.datetime.now()end_time = datetime.datetime(now_datetime.year, now_datetime.month, now_datetime.day, now_datetime.hour, 0,0)end_time_stamp = str(int(end_time.timestamp() * 1000))start_time = end_time - datetime.timedelta(days=day_gap)start_time_stamp = str(int(start_time.timestamp() * 1000))url = 'http://elasticsearch:9200/_msearch'headers = &#123; \"Content-Type\" : \"application/x-ndjson\"&#125;es_data = '&#123;\"index\":[\"index_name\"]&#125;\\n&#123;\"size\":0,\"query\":&#123;\"bool\":&#123;\"must\":[&#123;\"range\":&#123;\"log_time\":&#123;\"gte\":' + start_time_stamp + ',\"lte\":' + end_time_stamp + '&#125;&#125;&#125;]&#125;&#125;,\"aggs\":&#123;\"2\":&#123;\"date_histogram\":&#123;\"field\":\"log_time\",\"interval\":\"1h\", \"time_zone\":\"Asia/Tokyo\"&#125;&#125;&#125;&#125;\\n'response_list = requests.post(url=url, headers=headers, data=es_data.encode('utf-8')).json()['responses'][0]['aggregations']['2']['buckets']print('기준 : ' + str(start_time) + ' ~ ' + str(end_time))# es 데이터 중 시간값을 형식에 바꾸면서 key-value 형태로 정제한다.query_result = &#123;&#125;for data in response_list : query_result[data['key_as_string'].replace('T',' ').replace('.000+09:00', '')] = data['doc_count'] elasticsearch에서 aggregation을 활용하여 데이터를 1시간 단위로 가져올경우 데이터가 없는 경우를 대비해 비어있는 시간에 해당하는 카운트를 0으로 맞춰주기 위하여 loop를 돌며 데이터를 정제해준다. (이 방법보다 더 좋은 방법이 분명 있을것이다…) 123456789anomaly_detection_base_data = &#123;&#125;for data in range(24 * day_gap) : key_date_time = start_time.strftime('%Y-%m-%d %H:00:00') if key_date_time in query_result : anomaly_detection_base_data[key_date_time] = query_result[key_date_time] else : anomaly_detection_base_data[key_date_time] = 0 start_time = start_time + datetime.timedelta(hours=1) pandas 를 활용하여 데이터를 DataFrame 형식에 맞춰준 다음 시간기준으로 다음 24시간의 데이터를 예측하도록 한다.12345df = pd.DataFrame(list(anomaly_detection_base_data.items()), columns=['ds', 'y'])m = Prophet()m.fit(df)future = m.make_future_dataframe(periods=24 , freq='H')forecast = m.predict(future) 실제 집계한 값과 예측값을 비교하여 알림발생 유무를 결정한다.123456789# 현재시간 -1시간 전 실제 집계check_datetime = now_datetime - datetime.timedelta(hours=1)check_datetime = check_datetime.strftime('%Y-%m-%d %H:00:00')check_datetime_count = query_result[check_datetime]print('실제 집계수 : ' + check_datetime + ' → ' + str(check_datetime_count))# 예측 최대치forecast_max_count = forecast[forecast.ds == check_datetime]['yhat_upper'].values[0]print('예측 최대치 : ' + check_datetime + ' → ' + str(forecast_max_count)) 여기서는 에러 카운트가 평소와는 다르게 발생할 경우에 알림을 발송을 하려 하기때문에 아래처럼 구성을 해준다.12345if check_datetime_count &gt; forecast_max_count : # 예측한 결과를 이미지로 저장후 알림에 포함시켜준다. forecast_graph = m.plot(forecast, plot_cap=False, xlabel='time', ylabel='count'); file_name = 'forecast_graph.png' forecast_graph.savefig(file_name) 이런 python script를 jenkins 를 활용하여 1시간에 한번씩 실행될수있도록 구성해 두고 문제가 발생할때는 아래처럼 메일로 알림을 받는다.예측결과예측결과위 그래프를 분석하면 다음과 같다. 검정색 점 : 실제 데이터 파란 실선 : 트랜드 그래프 하늘색 음영 : 예측한 최대/최소치의 영역빨간색으로 표시한 점 두개를 보면 예측을 벗어난것을 확인할수 있고 실제로 저 시간대에 내부적 이슈로 인해 에러가 많이 발생한 경우이다. (장애 발생 시점) 이런식으로 구성을 해두면 앞서 수동으로 최대/최소 수치를 정해두고 이탈했는지에 대해서 모니터링 하는것보다 훨씬더 우아하게 모니터링을 할수있다.물론 해당 모듈이 완벽하다는 소리는 아니다. 필자도 해당 모듈을 사용하다보니 어떤 경우에서는 예측수치와 실제 데이터가 비슷한 경우가 있어 알람을 받은 경우도 있다. 예전 포스팅에서도 이야기 했듯 정답은 없는것 같다. 상황에 맞춰서 지속적으로 커스터마이징을 해야하는게 개발자의 숙명 아닐까 싶다. # 마치며사실 이렇게 시계열 데이터라는 단어나 Prophet같은 오픈소스를 검색할수 있었던건 작년 Elastic{ON}Tour 에서 봤던 Elastic X-pack의 머신러닝 기능 때문이다. 그때 받은 충격(?)이 아직까지 있는건지 이렇게 오픈소스로 비슷하게나마 구현할수 있었던 원동력이 된것 같다. 기회가 되면 X-pack을 구매해서 운영하는 서비스에 머신러닝을 활용해서 이러한 시계열 데이터에 대해 분석해보고싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"anomaly detection","slug":"anomaly-detection","permalink":"https://taetaetae.github.io/tags/anomaly-detection/"},{"name":"prophet","slug":"prophet","permalink":"https://taetaetae.github.io/tags/prophet/"},{"name":"facebook","slug":"facebook","permalink":"https://taetaetae.github.io/tags/facebook/"}]},{"title":"아파치 엑세스 로그에 408코드가?","slug":"apache-408-response-code","date":"2018-04-29T08:39:36.000Z","updated":"2020-04-23T04:41:36.652Z","comments":true,"path":"2018/04/29/apache-408-response-code/","link":"","permalink":"https://taetaetae.github.io/2018/04/29/apache-408-response-code/","excerpt":"예전에 아파치 로그를 엘라스틱 스택을 활용하여 내 서버에 누가 들어오는지를 확인할수 있도록 구성을 해두고 몇일간 지켜보니 다음과 같은 엑세스 로그가 발생하고 있었다.","text":"예전에 아파치 로그를 엘라스틱 스택을 활용하여 내 서버에 누가 들어오는지를 확인할수 있도록 구성을 해두고 몇일간 지켜보니 다음과 같은 엑세스 로그가 발생하고 있었다.1234567891011121314151617181.2.3.4 - - [26/Apr/2018:01:27:33 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 6001 30788 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 28 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 12 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:50 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5999 13521 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:14 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5996 19437 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:15 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5997 17553 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:15 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5998 17429 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 32 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 38 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 29 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:30:54 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 6000 17881 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 25 &quot;-&quot; &quot;-&quot; 한시간에 만건이상 응답코드는 408, referrer도 없고, useragent도 없는, ip들도 매우 다양한 이상한 녀석들이 요청되고 있었다. 이렇게 엑세스 로그를 분석할수 있는 구성을 해두고 나니 보였지 안그랬음 그냥 지나갔을 터.. 이러한 데이터를 키바나에서 보면 아래처럼 볼수있는데 한눈에 봐도 과연 의미있는 요청들일까? 하는 의구심이 들정도이다. (1시간 아파치 엑세스 로그)주황색이 408응답주황색이 408응답 그럼 이런 호출들은 도대체 뭘까? 천천히 생각좀 해보자. 정상적이지 않는 호출로 우리 서버의 취약점을 파악하려 하는것들일까? 응답코드 408은 요청시간초과 응답코드인데… 오히려 클라이언트 입장에서 문제가 있는건 아닐까? 어플리케이션 로직이 잘못되어 무한루프에 빠졌나; 위키백과에서는 아파치 응답코드 중 408에 대한 응답을 다음과 같이 알려주고 있다. The server timed out waiting for the request. According to HTTP specifications: “The client did not produce a request within the time that the server was prepared to wait. The client MAY repeat the request without modifications at any later time 즉, 아파치 단에서 타임아웃을 내버리는 상황. 여러 다양한 키워드들로 구글링을 해봐도 이렇다할 검색결과를 찾지 못하고 네트워크 관련상황인지 싶어 크롬 개발자도구를 열어 네트워크 지연 테스트를 해보았으나 별 효과가 없었다. 그렇게 범인찾는 형사의 심정으로 이것저것 알아보다 우연히 집에서 원격으로 회사 VPN 붙어서 테스트 하던도중 관련 증상을 재현 할수 있게 되었다. # 재현상황우선 아파치버전은 2.2이고 KeepAlive Off가 되어있는 상황. 아래그림처럼 집PC - 공유기 - VPN - Apache - tomcat jenkins 상황이였는데 젠킨스에 한번 접속후에는 항상 408 응답이 주루룩(?) 발생하는것을 알수 있었다. (사실 맨위에 엑세스 로그가 재현한 엑세스 로그이다.)요청의 흐름요청의 흐름 # 결론정확한 근거를 추론할수는 없었지만 재현한 바에 의해 결론을 내리자면, 특정 네트워크 환경에서 408응답이 발생하는것을 알수 있었다. (그중에 하나가 공유기 환경이라는 점)해당 요청을 막기에는 너무 다양한 ip들이고 서비스 특성상 keepAlive off로 설정한점을 미루어 볼때 해당 요청을 deny 시키기에는 다소 무리가 있다고 보여진다. 또한 시스템에 영향을 줄 정도가 아니므로 무시하는것으로 결론을 내렸다. 결과적으로는 그냥무시로 끝났지만 그래도 알고 무시해야지~ 하는 마음으로 재현까지 해볼수 있었던 좋은 경험이였다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"408","slug":"408","permalink":"https://taetaetae.github.io/tags/408/"}]},{"title":"내 서버에는 누가 들어오는걸까? (실시간 user-agent 분석기)","slug":"apache-access-log-user-agent","date":"2018-04-10T14:20:19.000Z","updated":"2020-04-23T04:41:36.660Z","comments":true,"path":"2018/04/10/apache-access-log-user-agent/","link":"","permalink":"https://taetaetae.github.io/2018/04/10/apache-access-log-user-agent/","excerpt":"Desktop 및 스마트폰의 대중화로 다양한 OS와 브라우저들을 사용하게 되었다. 이때, 내가 운영하는 웹서버에 들어오는 사람들은 무슨 기기로 접속을 하는 것일까? 혹여 특정 OS의 특정 브라우저에서만 안되는 버그를 잡기 위해 몇일밤을 고생하며 겨우 수정했는데… 과연 그 OS의 브라우저에서는 접속은 하기나 하는걸까? (ㅠㅠ)","text":"Desktop 및 스마트폰의 대중화로 다양한 OS와 브라우저들을 사용하게 되었다. 이때, 내가 운영하는 웹서버에 들어오는 사람들은 무슨 기기로 접속을 하는 것일까? 혹여 특정 OS의 특정 브라우저에서만 안되는 버그를 잡기 위해 몇일밤을 고생하며 겨우 수정했는데… 과연 그 OS의 브라우저에서는 접속은 하기나 하는걸까? (ㅠㅠ) 만약, 접속 사용자의 Device 정보를 알고있다면 고생하며 버그를 잡기 전에 먼저 해당 Device 사용율을 체크해 볼수도 있고(수정이 아닌 간단한 얼럿으로 해결한다거나?) 비지니스 모델까지 생각해야하는 서비스라면 타겟팅을 정하는 등 다양한 활용도가 높은 것이 바로 User-Agent라고 한다(이하 UA). 일반 Apache 를 웹서버로 운영하고 있다고 가정을 하고 어떻게 분석을 할수 있었는지, 그리고 분석을 하며 좀더 우아한(?) 방법은 없는지 알아 보고자 한다. # User-Agent가 뭐야?백문이 불여일타(?)라 했던가, 우선 http://www.useragentstring.com 를 들어가보자. 그러면 자신의 OS 및 브라우저 등 정보를 파싱해서 보여주는데 위키백과에 따르면 ‘사용자를 대신하여 일을 수행하는 소프트웨어 에이전트’라고 한다. 즉, UA만 알아도 어떤 기기/브라우저를 사용하는지 알 수 있다는것.mozilla에 가보면 스팩 등 다양한 UA를 볼수가 있는데 특히 맨 아래보면 기기/브라우저별로 지원정보가 나와있다. 여기서도 보면 모든 모바일 삼성 브라우저를 제외하고는 전부 지원이 되는걸 확인할수 있다.출처 : developer.mozilla.org출처 : developer.mozilla.org # 기존의 방법그럼 어떻게 내 서버에 들어온 사용자들의 UA를 확인할 수 있을까? (앞서 Apache를 웹서버로 운영한다고 했으니) Apache access log 에는 Apache에서 제공해주는 모듈을 이용해 접속한 클라이언트의 정보가 남겨지곤 한다. 그렇다면 이 access log를 리눅스 명령어든 엑셀로 뽑아서든지 활용해서 정규식으로 포맷팅 하고 그 결과를 다시 그룹화 시키면 얼추 원하는 데이터를 추출해 낼수 있다. ( 버거형들이 만들어둔 정규식을 가져다 사용할수도 있겠다. https://regexr.com/?37l4e )하지만, 우선 자동화가 안되어있어 데이터를 구하고 싶을때마다 귀차니즘에 걸릴수 있고 슈퍼 개발자 파워를 기반으로(?) 데이터 추출을 자동화 한들 실시간으로 보고싶을땐 제한사항이 많다. # 좀더 나은 방법(?)실시간 데이터를 모니터링 하는데에는 다양한 오픈소스와 다양한 툴이 있겠지만 경험이 부족한건지 아직까진 ElasticStack 만한걸 못본것 같다. 간단하게 설명을 하면 access log 를 사용하지 않고 front단에서 javascript 로 UA를 구한다음 이러한 정보를 받을수 있는 API를 만들어 그쪽으로 보내면 서버에서 해당 UA를 분석해서 카프카로 보내고 ..!@#$%^blabla…^^; 그림으로 보자.좀더 나은 방법좀더 나은 방법front단에서는 navigator.userAgent를 활용하여 UA를 구할수 있었고, API에서는 UA를 받고 파싱을 하는데 관련 코드는 다음과 같이 작성하였다.12345678910111213141516171819202122232425262728293031323334353637private static final String VERSION_SEPARATOR = \".\";private void userAgentParsingToMap(String userAgent, Map&lt;String, Object&gt; dataMap) &#123; HashMap browser = Browser.lookup(userAgent); HashMap os = OS.lookup(userAgent); HashMap device = Device.lookup(userAgent); dataMap.put(\"browserName\", browser.get(\"family\")); dataMap.put(\"browserVersion\", getVersion(browser)); dataMap.put(\"osName\", os.get(\"family\")); dataMap.put(\"osVersion\", getVersion(os)); dataMap.put(\"deviceModel\", device.get(\"model\")); dataMap.put(\"deviceBrand\", device.get(\"brand\"));&#125;private String getVersion(HashMap dataMap) &#123; String majorVersion = (String)dataMap.get(\"major\"); if (StringUtils.isEmpty(majorVersion)) &#123; return StringUtils.EMPTY; &#125; String minorVersion = (String)dataMap.get(\"minor\"); String pathVersion = (String)dataMap.get(\"path\"); StringBuffer sb = new StringBuffer(); sb.append(majorVersion); if (!StringUtils.isEmpty(minorVersion)) &#123; sb.append(VERSION_SEPARATOR); sb.append(minorVersion); &#125; if (!StringUtils.isEmpty(pathVersion)) &#123; sb.append(VERSION_SEPARATOR); sb.append(pathVersion); &#125; return sb.toString();&#125; 참고로 Java단에서 UA를 파싱하는 parser가 여러가지가 있는데 그중 uap_clj라는 모듈이 그나마 잘 파싱이 되어서 사용하게 되었다. 모듈별 비교 모듈 Browser OS Device eu.bitwalker.useragentutils.UserAgent O 불명확함 (Android 5.x) X net.sf.uadetector.UserAgentStringParser O O 불명확함 (Smartphone) uap_clj.java.api.* O O O Parsing 비교 UA 1&quot;Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Mobile Safari/537.36&quot; 결과 123- Browser : &#123;patch=3239, family=Chrome Mobile, major=63, minor=0&#125;- OS : &#123;patch=1, patch_minor=, family=Android, major=5, minor=1&#125;- Device : &#123;model=Nexus 6, family=Nexus 6, brand=Generic_Android&#125; 위와 같이 구성을 하면 Elasticsearch에 인덱싱된 데이터를 Kibana에서 입맛에 맞게 실시간으로 볼수있게 되었다!하지만 API를 만드는 부분 + front에서 별도의 javascript가 들어간다는 부분 + 운영하는 모든 페이지에 해당 javascript를 넣어야지만 볼수있는 부분 등 약간 아쉬운 감이 있다. 뭔가 더 좋은 방법이 없을까? 깔끔하면서도 우아한 방법. 짱구를 열심히 더 굴려보자. # 좀더 우아한 방법?Logstash 에는 UA를 파싱해주는 filter plugin 이 있다고 한다. (링크) 이 방법과 예전에 포스팅한 access log 를 elasticsearch 에 인덱싱 하는 방법을 혼합하면 위에서 걱정했던 문제들을 아주 깔끔하게 해결할 수 있을것 같다. 지난 포스팅 : 링크 물론 글보다는 그림설명이 더 빠르니 전체적인 흐름을 그림으로 보자.좀더 우아한 방법좀더 우아한 방법① front 단에서 javascript로 UA를 구하고 ② 별도로 만든 API에 데이터를 전송한뒤 ③ API에서는 이를 또 파싱하는 작업을 logstash 의 user-agent filter plugin으로 깔끔하게 해결할 수 있었다.우선 apache에서 access log format 은 다음과 같고1&quot;%h %l %u %t \\&quot;%r\\&quot; %&gt;s %b %D \\&quot;%&#123;Referer&#125;i\\&quot; \\&quot;%&#123;User-Agent&#125;i\\&quot;&quot; logstash쪽 설정은 다음과 같이 해주었다.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162input &#123; file &#123; path =&gt; &quot;/~~~/access.log.*&quot; start_position =&gt; &quot;beginning&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^%&#123;IPORHOST&#125;&quot; negate =&gt; true what =&gt; previous auto_flush_interval =&gt; 1 &#125; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [&quot;%&#123;IPORHOST:clientip&#125; (?:-|%&#123;USER:ident&#125;) (?:-|%&#123;USER:auth&#125;) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;NOTSPACE:uri&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER:bytes&#125;) (?:-|%&#123;NUMBER:bytes2&#125;)( \\&quot;%&#123;DATA:referrer&#125;\\&quot;)?( \\&quot;%&#123;DATA:user-agent&#125;\\&quot;)?&quot;] &#125; remove_field =&gt; [&quot;timestamp&quot;,&quot;@version&quot;,&quot;path&quot;,&quot;tags&quot;,&quot;httpversion&quot;,&quot;bytes2&quot;] &#125; useragent &#123; source =&gt; &quot;user-agent&quot; &#125; if [os_major] &#123; mutate &#123; add_field =&gt; &#123; os_combine =&gt; &quot;%&#123;os&#125; %&#123;os_major&#125;.%&#123;os_minor&#125;&quot; &#125; &#125; &#125; else &#123; mutate &#123; add_field =&gt; &#123; os_combine =&gt; &quot;%&#123;os&#125;&quot; &#125; &#125; &#125; if [os] =~ &quot;Windows&quot; &#123; mutate &#123; update =&gt; &#123; &quot;os_name&quot; =&gt; &quot;Windows&quot; &#125; &#125; &#125; if [os] =~ &quot;Mac&quot; &#123; mutate &#123; update =&gt; &#123; &quot;os_name&quot; =&gt; &quot;Mac&quot; &#125; &#125; &#125;&#125;output &#123; if [user-agent] != &quot;-&quot; and [user-agent] !~ &quot;Java&quot; &#123; kafka &#123; bootstrap_servers =&gt; &quot;~~~&quot; topic_id =&gt; &quot;~~~&quot; codec =&gt; json&#123;&#125; &#125; &#125;&#125; 설정하는데 가장 큰 어려웠던 점은 grok filter 패턴을 작성하는데 많은 시간을 할해야만 했다. 개인적으로 http://grokconstructor.appspot.com/do/match?example=2 에서 테스트 해보면서 패턴을 작성할수 있어 그나마 다행이였다.또한 logstash에서 파싱해주는 정보를 조금 다듬기 위해 mutate filter 를 사용해서 필드를 조합/수정 하였다.위처럼 적용을 하고 한두시간 수집을 해보니 Elasticsearch에 아래와 같은 도큐먼트가 생성이 된것을 확인할수 있었다.이 얼마나 우아한가...이 얼마나 우아한가...이 정보들을 잘 조합해서 Kibana에서 보면 다음과 같이 볼수 있다.user-agent 아 모바일 에서도 안드로이드 7.0에서 많이 들어오는구나~user-agent 아 모바일 에서도 안드로이드 7.0에서 많이 들어오는구나~UA는 아니지만 기왕 한김에 Access log의 uri를 분석해보면 아래처럼 볼수도 있겠다.외부에서 막 찔러 대는구나~ 시스템에 영향줄 정도면 막아야지!외부에서 막 찔러 대는구나~ 시스템에 영향줄 정도면 막아야지! # 마치며사실 logstash filter plugin을 작성하면서 뭔가 if-else로 분기처리한게 아쉽긴 하지만 만족할만한 결과를 얻을수 있어서 다행이라고 생각하고, 역시 데이터를 raw형태로 보는것보다 시각화해서 보는게 훨씬더 이해도를 높일수 있다는걸 다시금 실감할수 있었던 좋은 시간이였다.그런데.. 이 방법말고 더 우아한 방법은 없으려나..?;;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"user-agent","slug":"user-agent","permalink":"https://taetaetae.github.io/tags/user-agent/"},{"name":"apache access log","slug":"apache-access-log","permalink":"https://taetaetae.github.io/tags/apache-access-log/"},{"name":"Elasitcsearch","slug":"Elasitcsearch","permalink":"https://taetaetae.github.io/tags/Elasitcsearch/"},{"name":"Logstash","slug":"Logstash","permalink":"https://taetaetae.github.io/tags/Logstash/"},{"name":"Kibana","slug":"Kibana","permalink":"https://taetaetae.github.io/tags/Kibana/"}]},{"title":"gzip 설정으로 속도를 더 빠르게!","slug":"apache-gzip","date":"2018-04-01T08:58:11.000Z","updated":"2020-04-23T04:41:36.664Z","comments":true,"path":"2018/04/01/apache-gzip/","link":"","permalink":"https://taetaetae.github.io/2018/04/01/apache-gzip/","excerpt":"내가 운영중인 웹서비스의 응답속도를 보다 더 빠르게 하기 위해서는 어떤 방법이 있을까?웹 서비스를 위해 서버를 구성할 경우 일반적으로 앞단에 웹서버를 두고 그뒤에 WAS를 두는 설계를 하곤 한다.","text":"내가 운영중인 웹서비스의 응답속도를 보다 더 빠르게 하기 위해서는 어떤 방법이 있을까?웹 서비스를 위해 서버를 구성할 경우 일반적으로 앞단에 웹서버를 두고 그뒤에 WAS를 두는 설계를 하곤 한다. 여기서 웹서버는 대표적으로 Apache나 Nginx가 있고 WAS는 tomcat이나 기타 다른 모듈을 사용하는데 이렇게 두단계로 나누는 이유는 여러가지가 있겠지만 여기서는 앞단의 웹서버(Apache)의 설정으로 응답속도를 줄일수 있는 방법을 알아 보고자 한다. # 웹페이지의 응답속도를 줄일수 있는 ‘일반적인’방법들꼭 서버의 설정들을 건드리지 않고도 웹페이지의 응답속도를 줄일수 있는 방법은 다양하다. 가장 간단하게 코드 레벨에서 설정할수 있는 방법으로는 스타일시트를 위에 선언하거나 java script는 코드 아래부분에 넣는것만으로도 어느정도 응답속도를 줄일수 있다고 한다. (사족) 신입시절 회사 대표님이 필수로 읽어보라고 전 직원들에게 선물해주셨던 웹사이트 최적화기법 (스티브 사우더스 저)이 생각이 난다. 모두 사주려면 돈이 얼마야… 그만큼 웹개발자들에게 중요하면서도 한편으로는 기본이 되는 부분들이니 한번쯤 목차라도 읽어보는게 좋을듯 하다. 사실 이 포스팅을 작성하게된 가장 큰 계기는 얼마전 사내 해커톤을 하면서 경험한 부분 때문이다. ( + 들어만 봤지 실제로 해보지는 않아서… ) 서버에서 node(React)를 띄우고 그 앞단에 Apache로 단순 Port Redirect ( 80 → 3000 ) 시켜주고 있었는데 react 에서 사용하는 bundle.js의 용량이 크다보니 최초 페이지 접근시 로딩시간이 5초 이상되어버린 것이다. bundle.js를 줄여보는등 다양한 방법을 사용했다가 결국 Apache 설정을 통해 1초 이내로 줄일수 있었다. # gzip우선 gzip이란 파일 압축에 쓰이는 응용 소프트웨어로 GNU zip의 준말이라고 한다. (참고 : 위키백과 Gzip) 이를 사용하기 위해서는 브라우저가 지원을 해야하는데 https://caniuse.com/#search=gzip 을 보면 대부분의 브라우저에서 지원하는것을 볼수 있다. # 데이터 흐름그럼 gzip 을 사용했을때와 사용하지 않았을때의 차이는 어떻게 다를까? 우선 Request/Response Flow 를 잠깐 살펴보면 다음과 같다. gzip 사용 전출처 : betterexplained.com출처 : betterexplained.com 브라우저가 서버측에 /index.html을 요청한다. 서버는 Request를 해석한다. Response에 요청한 내용을 담아 보낸다. Response를 기다렸다가 브라우저에 보여준다. (100kb) gzip 사용 후출처 : betterexplained.com출처 : betterexplained.com 브라우저가 서버측에 /index.html을 요청한다. 서버는 Request를 해석한다. Response에 요청한 내용을 담아 보낸다. 여기서 해당 내용을 압축하는 과정이 추가가 된다. Response header에 압축이 되어있다는 정보를 확인후 브라우저는 해당 내용을 받고(10kb), 압축을 해제한 후 사용자에게 보여준다. 정리하면, gzip을 사용하면 서버는 Client에게 보낼 Response를 압축하기 때문에 네트워크 비용을 줄일수 있어 응답속도가 빠른 장점이 있다. # 무조건 사용해야 하는가?물론 무조건 좋은 (마치 show me the money 같은)정답은 없다. 서버에서 압축을 하여 Client에게 보내면 그대로 사용자에게 보여주는것이 아니라 압축을 해제하는 과정이 추가적으로 필요하다. 이러한 과정에서 브라우저는 cpu를 사용하게 되어 오히려 랜더링 하는 과정이 느려질수 있어 자칫 응답속도는 빨라졌다 하더라도 사용자 체감상 더 느려진것처럼 보여질수 있다. 따라서 상황에 맞춰 gzip을 사용해야 할것인지 말것인지에 대해 테스트가 필요하다. # 마치며학부시절 또는 신입시절, 아파치는 정적인 리소스를 담당하고 톰켓은 서블릿 처럼 데이터 가공이 필요한 페이지를 담당한다고 주문을 외우듯 하였지만, 웹서버에서 압축을 하면 어떤 효과가 있는지 실제로 경험해보는게 가장 중요한것 같다.적용 방법은 복붙하는 느낌이라 아파치 공식 문서를 링크 하는것으로 해당 포스팅을 마무리 하겠다. Apache Document (사용법) : https://httpd.apache.org/docs/2.2/ko/mod/mod_deflate.html 적용 테스트 사이트 https://developers.google.com/speed/pagespeed/insights http://www.whatsmyip.org/http-compression-test","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"gzip","slug":"gzip","permalink":"https://taetaetae.github.io/tags/gzip/"},{"name":"mod_deflate","slug":"mod-deflate","permalink":"https://taetaetae.github.io/tags/mod-deflate/"}]},{"title":"RestClientException 처리","slug":"rest-client-exception","date":"2018-03-17T10:19:39.000Z","updated":"2020-04-23T04:41:36.832Z","comments":true,"path":"2018/03/17/rest-client-exception/","link":"","permalink":"https://taetaetae.github.io/2018/03/17/rest-client-exception/","excerpt":"Spring 환경에서 (Spring5는 달라졌지만…) 외부 API로의 호출을 할때 자주 쓰이는 RestTemplate. 이때 request에 대해 정상적인 응답이 아닌 경우에 대한 처리는 어떻게 할까? 막연하게 생각을 해보면 Http Status Code를 받아서 판별을 하면 되지만 Http Status Code만 봐도 엄청~많다.","text":"Spring 환경에서 (Spring5는 달라졌지만…) 외부 API로의 호출을 할때 자주 쓰이는 RestTemplate. 이때 request에 대해 정상적인 응답이 아닌 경우에 대한 처리는 어떻게 할까? 막연하게 생각을 해보면 Http Status Code를 받아서 판별을 하면 되지만 Http Status Code만 봐도 엄청~많다. ( 위키피디아 참고 : https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )if~else 로 다 나눌수도 없고… 우선 성공/실패에 대한 판별은 어떻게 할까 하고 코드를 파고파고 들어가다가 알게된 부분에 대해 정리를 해보겠다. # Http Status Code 그룹정의Spring-project github에 가보면 HttpStatus 하위에 내부 Enum으로 아래처럼 정의되어 있다. ( 링크 )12345678910111213141516171819202122232425262728293031public enum HttpStatus &#123; ... public Series series() &#123; return Series.valueOf(this); &#125; ... public enum Series &#123; INFORMATIONAL(1), SUCCESSFUL(2), REDIRECTION(3), CLIENT_ERROR(4), SERVER_ERROR(5); ... public static Series valueOf(int status) &#123; int seriesCode = status / 100; for (Series series : values()) &#123; if (series.value == seriesCode) &#123; return series; &#125; &#125; throw new IllegalArgumentException(\"No matching constant for [\" + status + \"]\"); &#125; &#125;&#125; 약 80여개 응답코드들을 크게 5개 묶음으로 정의해 놓은것을 볼수있다. 즉, 201 / 201 / 202 같은 녀석들은 전부 SUCCESSFUL 성공 으로 그룹핑이 되는것을 확인할수 있다. # 그럼 RestTemplate 에서는 어떻게 처리하고 있나?코드를 쭉쭉 따라가다 보면 아래처럼 4xx, 5xx 는 에러라고 판단하고 그에 따라 RestClientException 을 반환하는것을 확인할수 있다. RestTemplate 호출 링크 123456789101112131415try &#123; ClientHttpRequest request = createRequest(url, method); if (requestCallback != null) &#123; requestCallback.doWithRequest(request); &#125; response = request.execute(); handleResponse(url, method, response); if (responseExtractor != null) &#123; return responseExtractor.extractData(response); &#125; else &#123; return null; &#125;&#125;catch (IOException ex) &#123; 에러 판별 링크 123protected boolean hasError(HttpStatus statusCode) &#123; return (statusCode.series() == HttpStatus.Series.CLIENT_ERROR || statusCode.series() == HttpStatus.Series.SERVER_ERROR);&#125; # 성공/실패 처리는 어떻게 할것인가각자 정의하기 나름일것 같다. 우선 아주 간단하게 RestTemplate 를 사용할때 예외처리를 하여 정의된 대로 4xx, 5xx가 에러라고 판단할 수 있을것 같고12345try &#123; responseBody = restTemplate.postForObject(url, httpEntity, byte[].class);&#125; catch (RestClientException e) &#123; // 에러인 경우 RestClientException 을 내뱉는다. log.error(\"##### restTemplate error, url = &#123;&#125;\", url, e);&#125; 정의된 에러(?)와는 조금 다르게 처리하고 싶다면 DefaultResponseErrorHandler을 상속받고 hasError메소드를 무조건 패스하도록 Override 하고난 다음 응답 코드를 받아서 처리하는 방법이 있을수 있겠다.(물론 이러한 방법 말고 다양한 방법이 있을것 같다.)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"RestTemplate","slug":"RestTemplate","permalink":"https://taetaetae.github.io/tags/RestTemplate/"},{"name":"RestClientException","slug":"RestClientException","permalink":"https://taetaetae.github.io/tags/RestClientException/"}]},{"title":"소나큐브 이용 코드 정적분석 자동화","slug":"jenkins-sonar-github-integration","date":"2018-02-08T11:10:54.000Z","updated":"2020-04-23T04:41:36.778Z","comments":true,"path":"2018/02/08/jenkins-sonar-github-integration/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/jenkins-sonar-github-integration/","excerpt":"코드 정적분석이라 함은 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석을 말한다. 이를테면 냄새나는 코드(?)라던지, 위험성이 있는 코드, 미리 정의된 규칙이나 코딩 표준을 준수하는지에 대한 분석을 말하는데 java 기준으로는 아래 다양한 (잘 알려진) 정적분석 도구들이 있다.","text":"코드 정적분석이라 함은 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석을 말한다. 이를테면 냄새나는 코드(?)라던지, 위험성이 있는 코드, 미리 정의된 규칙이나 코딩 표준을 준수하는지에 대한 분석을 말하는데 java 기준으로는 아래 다양한 (잘 알려진) 정적분석 도구들이 있다. PMD 미사용 변수, 비어있는 코드 블락, 불필요한 오브젝트 생성과 같은 Defect을 유발할 수 있는 코드를 검사 https://pmd.github.io FindBugs 정해진 규칙에 의해 잠재적인 에러 타입을 찾아줌 http://findbugs.sourceforge.net CheckStyle 정해진 코딩 룰을 잘 따르고 있는지에 대한 분석 http://checkstyle.sourceforge.net 이외에 SonarQube 라는 툴이 있는데 개인적으로 위 알려진 다른 툴들의 종합판(?)이라고 생각이 들었고, 그중 가장 인상깊었던 기능이 github과 연동이 되고 적절한 구성을 하게 되면 코드를 수정하는과 동시에 자동으로 분석을 하고 리포팅까지 해준다는 부분이였다. ( 더 좋은 방법이 있는지는 모르겠으나 다른 도구들은 수동으로 돌려줘야 하고 리포팅 또한 Active하지 못한(?) 아쉬운 점이 있었다. ) 지금부터 Jenkins + github web-hook + SonarQube 를 구성하여 코드를 수정하고 PullRequest를 올리게 되면 수정한 파일에 대해 자동으로 정적분석이 이뤄지고, 그에대한 리포팅이 해당 PullRequest에 댓글로 달리도록 설정을 해보겠다. (코드리뷰를 봇(?)이 자동으로 해주는게 얼마나 편한 일인가…) # 기본 컨셉전체적인 컨셉은 다음 그림과 같다.전체 컨셉전체 컨셉 IDE에서 코드수정을 하고 remote 저장소에 commit &amp; push를 한다.그 다음 github에서 master(혹은 stable한 branch)에 대해 작업 branch를 PullRequest 올린다. 미리 등록한 github의 web-hook에 의해 PullRequest 정보들을 jenkins에 전송한다. 전달받은 정보를 재 가공하여 SonarQube로 정적분석을 요청한다. SonarQube에서 분석한 정보를 다시 jenkins로 return 해준다. SonarQube으로부터 return 받은 정보를 해당 PullRequest의 댓글에 리포팅을 해준다. 간단히 보면 (뭐 간단하니 쉽네~) 라고 볼수도 있겠지만 나는 이런 전체 흐름을 설정하는데 있어 어려웠다. 사실 셋팅하는 과정에서 적지않은 삽질을 했었기에, 이 포스팅을 적는 이유일수도 있겠다.더불어 검색을 해봐도 이렇게 전체흐름이 정리된 글이 잘 안보여서 + 내가 한 삽질을 다른 누군가도 할것같아서(?) # Maven 설치기본적으로 Maven의 H2DB를 사용하므로 SonarQube를 설치하기전에 Maven부터 설치해줘야 한다.123456$ wget http://apache.mirror.cdnetworks.com/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz$ tar -zxvf apache-maven-3.5.2-bin.tar.gz(환경변수 셋팅후 )$ mvn -versionApache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T16:58:13+09:00)... # SonarQube 설치정적분석을 도와주는 SonarQube를 설치해보자.123456$ wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.7.1.zip$ unzip sonarqube-6.7.1.zip$ cd sonarqube-6.7.1/bin/linux-x86-64$ ./sonar.sh startStarting SonarQube...Started SonarQube. 기본적으로 9000포트를 사용하고 있으니 다른포트를 사용하고자 한다면 /sonarqube-6.7.1/conf/sonar.properties 내 sonar.web.port=9000 을 수정해주면 된다. (SonarQube도 Elasticsearch를 사용하구나…)설치후 실행을 한뒤 서버IP:9000을 접속해보면 아래 화면처럼 나온다. (혹시 접속이 안된다거나 서버가 실행이 안된다면 ./sonar.sh console로 로그를 보면 문제해결에 도움이 될수도 있다. )SonarQube 메인화면SonarQube 메인화면 # SonarQube Scanner 설치소스를 연동시켜 정적분석을 하기 위해서는 SonarQube Scanner 라는게 필요하다고 한다. 아래 url에서 다운받아 적절한 곳에 압축을 풀어두자.https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner12$ wget https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-3.0.3.778-linux.zip$ unzip sonar-scanner-cli-3.0.3.778-linux.zip # jenkins 설치 및 SonarQube 연동jenkins 설치는 간단하니 별도 언급은 안하고 넘어가…려고 했으나, 하나부터 열까지 정리한다는 마음으로~https://jenkins.io/download/ 에서 최신버전을 tomcat/webapps/ 아래에 다운받고 server.xml 을 적절하게 수정해준다.12345$ wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war$ vi tomcat/conf/server.xml&lt;Connector port=&quot;19001&quot; protocol=&quot;HTTP/1.1&quot; # 포트 변경&lt;Context path=&quot;/jenkins&quot; debug=&quot;0&quot; privileged=&quot;true&quot; docBase=&quot;jenkins.war&quot; /&gt; #추가# tomcat/bin/startup.sh jenkins 설치를 완료 한 후 필요한 플러그인을 추가로 설치해준다. Python Plugin GitHub Pull Request Builder GitHub plugin 접속 : 서버IP:19001 (참고로 한 서버에서 다 설치하다보니 port 충돌을 신경쓰게되었다. )처음 jenkins를 실행하면 이런저런 설정을 하는데 특별한 설정 변경없이 next버튼을 연신 눌러면 설치가 완료 되고, SonarQube를 사용하기 위해 SonarQube Scanner for Jenkins라는 플러그인을 설치해주자. (이건 각 버전마다 궁합(?)이 안맞을수도 있으니 확인이 필요할수도 있다. 내가 설치한 버전은 jenkins 2.89, SonarQube Plugin 2.6.1이다.)설치를 하면 jenkins &gt; configure 에서 SonarQube servers정보를 등록해준다.SonarQube 젠킨스 설정SonarQube 젠킨스 설정authentication token 은 SonarQube에 처음 접속했을때 아래화면을 볼수 있는데, 여기서 authentication token을 발급받을수 있다. 또한 language 와 build 방법을 선택하자. (이부분은 나중에 변경이 가능하니 개발상황에 맞춰서 설정하면 될듯하다.)authentication token 발급authentication token 발급그 다음 jenkins &gt; configureTools 에서 SonarQube scanner 정보를 다음 화면과 같이 등록해준다.SonarQube scanner 젠킨스 설정SonarQube scanner 젠킨스 설정우선 여기까지 하면 jenkins 와 SonarQube 연동은 된걸로 보면 된다. # Github과 Jenkins 연동앞서 설명한 전체 컨셉 그림과 같이 github 에 pullRequest가 발생하면 web-hook을 이용하여 jenkins로 정보를 전달하는 방식인데, 이럴려면 우선 github과 jenkins가 연동이된 상태여야 한다. 관련 방법은 아래 링크에 정리를 해놨으니 참고해도 좋을듯 하다. Github과 Jenkins 연동하기 # Jenkins job 등록Jenkins job은 두개를 만들어 줘야 한다. 1번 job pullrequest_receiver : github으로부터 web-hook을 통해 pullRequest정보를 받는 job 2번 job sonaqube-job : 1번 job으로 부터 정보를 받아 SonarQube를 이용해 정적분석후 해당 pullRequest에 댓글로 리포팅 하는 Job - 1번 job편의상 job 이름은 pullrequest_receiver으로 정하였고, 매개변수로 String Parameter에 이름은 payload라 정하였다. 그리고 Build 에서 Execute Python 으로 아래 코드를 실행하도록 하였다.12345678910111213141516import os, json, sys, requestsurl = 'http://~~~/jenkins/job/sonaqube-job'hookData = json.loads(os.environ['payload'])branch = hookData['pull_request']['head']['ref'];prId = hookData['pull_request']['number'];componentName = hookData['repository']['name'];if action == 'closed' or action == 'synchronize' or action == 'assigned' : # 이 부분은 적절하게 수정이 필요하다. sys.exit(0)repository = hookData['repository']['full_name']url = url + '/buildWithParameters?branch=' + branch + '&amp;pr=' + str(prId) + '&amp;repository=' + repository + '&amp;componentName=' + componentNameresponse = requests.post(url) WebHook을 이용해서 Jenkins Job을 실행시키는 과정은 아래 링크에 별도로 정리해놨으니 참고해도 좋을듯 하다. Github의 WebHook을 이용하여 자동 Jenkins Job 실행 위와 같이 설정을 하고 pullRequest가 발생을 하면 해당 Job이 실행이 되면서 정보들을 적절하게 조합하여 2번 job으로 보내게 된다. 여기서 job을 하나로 나누지 않고 github정보를 받는 job, SonarQube 분석 job 이렇게 두개로 나눈 이유는 여러 Repository에 대해서 셋팅을 하고 싶은경우 Repository에서는 1번 job으로 쏘게 하고 1번 job내부에서 payload의 정보를 분석하여 Repository별 SonarQube 분석 job url로 보내면 되기 때문이다. 1번 job 은 하나, 2번 job은 Repository 별로 분기 되는 구조. - 2번 jobSonarQube job을 작성할 차례다. 1번 job에서 파라미터를 보내주기 때문에 역시 2번 job에서도 파라미터를 미리 받도록 설정해주자. 파라미터 목록 pr branch repository componentName 이 job에서는 SonarQube 분석이 되는데, 해당 repo에 대한 접근권한이 있어야 하기 때문에 앞서 설정한 Github과 Jenkins 연동 방법으로 Repository 와 Credentials 을 지정해주고, 브랜치 경로 또한 파라미터로 받은 값으로 지정해 준다.소스코드 관리 설정소스코드 관리 설정 마지막으로 최종 목적이였던 SonarQube 분석을 할 차례인데, 이번에 SonarQube가 버전업이 되면서 java 기분 .java 파일만 가지고 하는게 아니라 .java파일을 컴파일 하면 나오는 .class파일 경로를 필수로 지정해줘야 한다고 한다. (예전에는 .class 파일 경로를 지정하지 않아도 되었는데 홈페이지 설명에 의하면 바이너리 파일까지 같이 분석하게 되면 분석 신뢰도가 높아진다고 한다.)https://docs.sonarqube.org/display/PLUG/Java+Plugin+and+Bytecode 딴 이야기로, 난 여태까지 정적분석이라는건 코드만을 가지고 하는줄 알았다. 자바 기준에서는 .java . 하지만 이번에 SonarQube 설정을 하면서 알게된 부분으로, 정적분석이라는건 맨 위에 적어놨듯 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석 이기 때문에 java 기준에서는 .class 파일또한 분석의 대상으로 볼수가 있다. (실제로 구버전 - .java만 분석 / 신버전 - .java + .class 분석 을 해봤더니 불필요한 분석결과(?)가 줄어든것을 알수 있었다. ) 그래서 결국 컴파일을 한 뒤에 SonarQube 분석을 하면 되겠다.빌드 및 SonarQube 분석 설정빌드 및 SonarQube 분석 설정SonarQube 설정파일은 다음과 같다.1234567891011121314151617sonar.projectKey=$&#123;componentName&#125; # 유니크한 프로젝트 키 네이밍 값sonar.projectVersion=0.1sonar.sourceEncoding=UTF-8 sonar.analysis.mode=preview sonar.github.repository=$&#123;repository&#125;sonar.github.endpoint=https://api.github.cosonar.github.login= # github login idsonar.github.oauth= # github 개인키sonar.login=adminsonar.password=admin sonar.github.pullRequest=$&#123;pr&#125;sonar.host.url= # SonarQube urlsonar.issuesReport.console.enable=true sonar.github.disableInlineComments=truesonar.sources=.sonar.exclusions=sonar.java.binaries=target/classes # 빌드 결과물 경로 이렇게 긴~~과정을 거치고 나면 pullRequest 를 올릴때마다 아래 그림처럼 수정한 파일에 대해서 자동으로 분석을 하여 알려준다.SonarQube 분석결과SonarQube 분석결과 또한 분석결과에 따라 (이 부분은 SonarQube 설정으로 조정이 가능할것같다) merge 가 가능/불가능이 조정되어 진다.SonarQube 분석 중 critical 항목이 없어서 merge 가능한 화면SonarQube 분석 중 critical 항목이 없어서 merge 가능한 화면 # 마치며반복적인 일에 대해서 자동화 구성을 한다는것은 참 의미있는 일인것 같다. 팀내 이런 구성을 도입하구서 획기적으로 코드 품질이 나아지진 않았지만 자칫 잘못해서 merge가 되는 위험한 코드들은 어느정도 SonarQube가 잡아주는것 같다. 사족이지만, 이렇게 적고나니 간단한데 이런 구성에 대한 정보를 알기까지는 엄청난 시간이 들었기에… 오랜만의 포스팅에 대한 뿌듯함을 다시한번 느낀다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"},{"name":"SonarQube","slug":"SonarQube","permalink":"https://taetaetae.github.io/tags/SonarQube/"},{"name":"integration","slug":"integration","permalink":"https://taetaetae.github.io/tags/integration/"}]},{"title":"Github의 WebHook을 이용하여 자동 Jenkins Job 실행","slug":"github-web-hook-jenkins-job-excute","date":"2018-02-08T08:10:54.000Z","updated":"2020-04-23T04:41:36.744Z","comments":true,"path":"2018/02/08/github-web-hook-jenkins-job-excute/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/github-web-hook-jenkins-job-excute/","excerpt":"PullRequest가 발생하면 알림을 받고싶다거나, 내가 관리하는 레파지토리에 댓글이 달릴때마다 또는 이슈가 생성될때마다 정보를 저장하고 싶다거나. 종합해보면 Github에서 이벤트가 발생할때 어떤 동작을 해야 할 경우 Github에서 제공하는 Webhook 을 사용하여 목적을 달성할 수 있다.","text":"PullRequest가 발생하면 알림을 받고싶다거나, 내가 관리하는 레파지토리에 댓글이 달릴때마다 또는 이슈가 생성될때마다 정보를 저장하고 싶다거나. 종합해보면 Github에서 이벤트가 발생할때 어떤 동작을 해야 할 경우 Github에서 제공하는 Webhook 을 사용하여 목적을 달성할 수 있다.아 당연한 이야기이지만 언급하고 넘어갈께 있다면, Github에서 Jenkins Job을 호출하기 위해서는 Jenkins가 외부에 공개되어 있어야 한다. (내부사설망이나 private 한 설정이 되어있다면 호출이 안되어 Webhook기능을 사용할 수 없다.) # Jenkins Security 설정Jenkins Job을 외부에서 URL로 실행을 하기 위해서는 아래 설정이 꼭 필요하다. (이 설정을 몰라서 수많은 삽질을 했다.)CSRF Protection 설정 체크를 풀어줘야 한다. 이렇게 되면 외부에서 Job에 대한 트리거링이 가능해 진다.Jenkins > Configure Global SecurityJenkins > Configure Global Security # Jenkins Job 설정Github 에서 Webhook에 의해 Jenkins Job을 실행하게 될텐데, 그때 정보들이 payload라는 파라미터와 함께 POST 형식으로 호출이 되기 때문에 미리 Job에서 받는 준비(?)를 해둬야 한다.설정은 간단하게 다음과 같이 Job 파라미터 설정을 해주면 된다.Jenkins > 해당 Job > configureJenkins > 해당 Job > configure # Github Webhook 설정이제 Github Repository 의 Hook 설정만 하면 끝이난다. 해당 Repository &gt; Settings &gt; Hooks 설정에 들어가서 Add webhook을 선택하여 Webhook을 등록해준다.URL은 {jenkins URL}/jenkins/job/{job name}/buildWithParameters식으로 설정해주고 Content Type 은 application/x-www-form-urlencoded으로 선택한다. 언제 Webhook을 트리거링 시킬꺼냐는 옵션에서는 원하는 설정에 맞추면 되겠지만 나는 pullRequest가 등록 될때만 미리 만들어 놓은 Jenkins Job을 실행시킬 계획이였으니 Let me select individual events.을 설정하고 Pull Request에 체크를 해준다. 아래 그림처럼 말이다.해당 Repositroy > Settings > Hooks해당 Repositroy > Settings > Hooks이렇게 등록하고 다시 들어가서 맨 아랫 부분Recent Deliveries을 보면 ping test 가 이루어져 정상적으로 응답을 받은것을 확인할수가 있다.Webhook 등록 결과Webhook 등록 결과 이렇게 설정을 다 한 뒤 PullRequest를 발생시키면 Jenkins 해당 Job에서는 파라미터를 받으며 실행이 된것을 확인할수가 있다.Jenkins Job 실행 결과Jenkins Job 실행 결과 끝~","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"},{"name":"Webhook","slug":"Webhook","permalink":"https://taetaetae.github.io/tags/Webhook/"}]},{"title":"Github과 Jenkins 연동하기","slug":"github-with-jenkins","date":"2018-02-08T08:10:21.000Z","updated":"2020-04-23T04:41:36.748Z","comments":true,"path":"2018/02/08/github-with-jenkins/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/github-with-jenkins/","excerpt":"Jenkins에서 Github의 소스를 가져와서 빌드를 하는 등 Github과 Jenkins와 연동을 시켜줘야하는 상황에서, 별도의 선행 작업이 필요하다. 다른 여러 방법이 있을수 있는데 여기서는 SSH로 연동하는 방법을 알아보고자 한다.","text":"Jenkins에서 Github의 소스를 가져와서 빌드를 하는 등 Github과 Jenkins와 연동을 시켜줘야하는 상황에서, 별도의 선행 작업이 필요하다. 다른 여러 방법이 있을수 있는데 여기서는 SSH로 연동하는 방법을 알아보고자 한다. 우선 Jenkins가 설치되어있는 서버에서 인증키를 생성하자. 1234567891011121314151617181920212223$ ssh-keygen -t rsa -f id_rsaGenerating public/private rsa key pair.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in id_rsa.Your public key has been saved in id_rsa.pub.The key fingerprint is:SHA256:~~~~~ ~~~@~~~~~The key&apos;s randomart image is:+---[RSA 2048]----+| o*+**=*=**+ || o B=o+o++o || E+.o+ + oo .|| oo. * o ...|| .+ S = o || . + o . || . . . || . || |+----[SHA256]-----+$ lsid_rsa id_rsa.pub 개인키(id_rsa)는 젠킨스에 설정해준다. (처음부터 끝까지 복사, 첫줄 마지막줄 빼면 안된다… )젠킨스에 SSH 개인키 설정젠킨스에 SSH 개인키 설정그 다음 공개키(id_rsa.pub)는 Github에 설정을 해준다.Github에 SSH 공개키 설정Github에 SSH 공개키 설정이렇게 한뒤 Jenkins 에서 임의로 job을 생성하고 job 설정 &gt; 소스코드 관리 에서 git 부분에 아래처럼 테스트를 해서 정상적으로 연동이 된것을 확인한다. Credentials 값을 위에서 설정한 개인키로 설정하고, repo 주소를 SSH용으로 적었을때 에러가 안나오면 성공한것이다.정상 연결되면 Jenkins 오류도 없고, github SSH 키에 녹색불이 들어온다.정상 연결되면 Jenkins 오류도 없고, github SSH 키에 녹색불이 들어온다. 끝~","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"}]},{"title":"linux(centOS)에서 selenium 설정하기 (feat. python)","slug":"linux-selenium","date":"2018-02-01T05:52:10.000Z","updated":"2020-04-23T04:41:36.793Z","comments":true,"path":"2018/02/01/linux-selenium/","link":"","permalink":"https://taetaetae.github.io/2018/02/01/linux-selenium/","excerpt":"테스트 코드로 안되는 실제 브라우저단 사용성 테스트를 하고싶은 경우가 있다. 이를테면 화면이 뜨고, 어떤 버튼을 누르면, 어떤 결과가 나와야 하는 일련의 Regression Test. 이때 활용할수 있는게 다양한 도구가 있지만 이번엔 selenium 에 대해서 알아보고자 한다.","text":"테스트 코드로 안되는 실제 브라우저단 사용성 테스트를 하고싶은 경우가 있다. 이를테면 화면이 뜨고, 어떤 버튼을 누르면, 어떤 결과가 나와야 하는 일련의 Regression Test. 이때 활용할수 있는게 다양한 도구가 있지만 이번엔 selenium 에 대해서 알아보고자 한다. 처음부터 사실 web application 테스트를 하려고 selenium 를 알아보게 된건 아니고, 내가 참여하고 있는 특정 밴드(네이버 BAND)에서 일주일에 한번씩 동일한 형태의 글을 올리고 있는데 (일종의 한주 출석체크 같은…) 이를 자동화 해볼순 없을까 하며 밴드 API를 찾아보다 selenium 라는것을 알게되었고, 매크로처럼 어떤버튼 누르고 그다음 어떤버튼 누르고 하는 일련의 과정을 코드로 구성할수 있다는 점에 감동을 받아(?) + 별도의 API를 발급받지 않아도 되어 사용하게 되었다. (물론 UI가 바뀌면 골치아프겠지만…) 여기서는 selenium 이 무엇인지에 대한 설명은 하지 않는다. (인터넷에 나보다 정리 잘된글이 많으니…) 단, linux 환경에서 셋팅하는 정보가 너무 없고 몇일동안 삽질을 한게 아쉬워서 그 과정을 포스팅 해본다. (나같은 분이 이 글을 보고 도움이 되실꺼라는 기대를 갖으며…) ※ 주의 : 본 포스팅은 밴드 서비스에 글을 올릴수 있는 비 정상적인 방법의 공유가 아닌, selenium에 대한 사용 후기(?)에 대한 글입니다. (참고로 막혔어요 -ㅁ-) 설정하기서버 환경은 CentOS 7.4 64Bit + Python 3.6.3 + jdk 8 이다. 우선 selenium 을 설치해준다.1$ sudo python3.6 -m pip install selenium 그 다음 CentOS에서 크롬브라우저를 설치하기 위하여 yum 저장소를 추가한다. (꼭 크롬이 아니더라도 파이어폭스나 지금은 지원이 끊긴 팬텀JS 같은것으로 활용할수도 있으나 다른것들도 해봤는데 자꾸 설정에서 걸려서 크롬에 대한 내용을 포스팅 한다.)1234567$ sudo vi /etc/yum.repos.d/google-chrome.repo[google-chrome]name=google-chromebaseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64enabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 그리고는 yum 으로 크롬 브라우저를 설치한다. (내가 설치했을때의 버전은 google-chrome-stable.x86_64 0:64.0.3282.119-1)1$ yum install google-chrome-stable 크롬드라이버를 설치해야한다. 다음 url에서 받을수 있는데 https://sites.google.com/a/chromium.org/chromedriver/downloads 나는 2.35 linux64 버전을 받았다. 다운받고 unzip 하면 딱하나 파일이 있는데 나중에 selenium 을 사용할때 이용되니 path를 알아두자. 그다음 파이썬 코드를 작성한다. 내가 짠 파이썬 코드는 다음과 같은 순서로 실행이 된다. 밴드 접속 ( https://band.us/home ) 로그인 특정 밴드 선택 글쓰기 버튼 누르고 양식에 맞춰 글 작성 글 등록 파이썬 코드는 아래처럼 작성하였다. (중요부분만.. 그 아래는 자유)1234567891011121314from selenium import webdriverfrom selenium.webdriver.chrome.options import Options# 초기화 --------------------------------------------chrome_options = Options()chrome_options.add_argument(\"--headless\")driver = webdriver.Chrome(executable_path='home/~~~/chromedriver', chrome_options=chrome_options)driver.implicitly_wait(3)driver.get('https://band.us/home')# 로그인 --------------------------------------------driver.find_element_by_class_name('_loginLink').click()...생략 그리고 실행을 해보면 작동이 잘~ 된다.selenium + python 으로 자동작성된 밴드 글selenium + python 으로 자동작성된 밴드 글 마치며selenium 에 대해 찾아보면 거의 윈도우 환경에서 돌아가는것들에 대한 포스팅이 많았다. 난 리눅스 환경에서 스케쥴러(젠킨스 같은)를 통해 자동으로 화면없이 작동시키고 싶었는데 아무리 찾아봐도 + 삽질해도 잘 안되었다. 결국 사내에도 나같은 삽질을 하신 분을 찾고 묻고 물어 크롬드라이버만 있어야 하는것이 아니라 크롬앱또한 있어야 동작을 한다는것을 알게 되었다.역시, 내가 한 삽질은 누군가 이미 한 삽질이라는걸 다시한번 깨닳은 좋은(?) 시간이였다. 이걸로 나중에 내가 맡고있는 서비스에 대한 웹 자동 테스트 툴도 만들어 볼 생각이다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"linux","slug":"linux","permalink":"https://taetaetae.github.io/tags/linux/"},{"name":"selenium","slug":"selenium","permalink":"https://taetaetae.github.io/tags/selenium/"},{"name":"chromedriver","slug":"chromedriver","permalink":"https://taetaetae.github.io/tags/chromedriver/"}]},{"title":"아파치 엑세스 로그를 엘라스틱서치에 인덱싱 해보자.","slug":"apache-access-log-to-es","date":"2018-01-25T12:18:35.000Z","updated":"2020-04-23T04:41:36.655Z","comments":true,"path":"2018/01/25/apache-access-log-to-es/","link":"","permalink":"https://taetaetae.github.io/2018/01/25/apache-access-log-to-es/","excerpt":"apache access log 를 분석하고 싶은 상황이 생겼다. 아니 그보다 apache access에 대해서 실시간으로 보고싶었고, log를 검색 &amp; 데이터를 가공하여 유의미한 분석결과를 만들어 보고 싶었다. 그에 생각한것이 (역시) ElasticStack.","text":"apache access log 를 분석하고 싶은 상황이 생겼다. 아니 그보다 apache access에 대해서 실시간으로 보고싶었고, log를 검색 &amp; 데이터를 가공하여 유의미한 분석결과를 만들어 보고 싶었다. 그에 생각한것이 (역시) ElasticStack. 처음에 생각한 방안은 아래 그림처럼 단순했다.처음 생각한 단순한 구조처음 생각한 단순한 구조하지만, 내 단순한(?) 예상은 역시 빗나갔고 logstash에서는 다음과 같은 에러를 내뱉었다. retrying individual bulk actions that failed or were rejected by the previous bulk request request가 많아짐에 따라 elasticsearch가 버벅거리더니 logstash에서 대량작업은 거부하겠다며 인덱싱을 멈췄다. 고민고민하다 elasticsearch에 인덱싱할때 부하가 많이 걸리는 상황에서 중간에 버퍼를 둔 경험이 있어서 facebook그룹에 문의를 해봤다.https://www.facebook.com/groups/elasticsearch.kr/?multi_permalinks=1566735266745641역시 나보다 한참을 앞서가시는 분들은 이미 에러가 뭔지 알고 있으셨고, 중간에 버퍼를 두고 하니 잘된다는 의견이 있어 나도 따라해봤다. 물론 답변중에 나온 redis가 아닌 기존에도 비슷한 구조에서 사용하고 있던 kafka를 적용.아, 그전에 현재구성은 Elasticsearch 노드가 총 3대로 클러스터 구조로 되어있는데 노드를 추가로 늘리며 스케일 아웃을 해보기전에 할수있는 마지막 방법이다 생각하고 중간에 kafka를 둬서 부하를 줄여보고 싶었다. (언제부턴가 마치 여러개의 톱니바퀴가 맞물려 돌아가는듯한 시스템 설계를 하는게 재밌었다.) 아래 그림처럼 말이다.그나마 좀더 생각한 구조그나마 좀더 생각한 구조그랬더니 거짓말 처럼 에러하나 없이 잘 인덱싱이 될수 있었다. logstash가 양쪽에 있는게 약간 걸리긴 하지만, 처음에 생각한 구조보다는 에러가 안나니 다행이라 생각한다. 이 구조를 적용하면서 얻은 Insight가 있기에, 각 항목별로 적어 보고자 한다. ( 이것만 적어놓기엔 너무 없어보여서.. ) # access log 를 어떻게 분석하여 인덱싱 할것인가?apache 2.x를 사용하고 별도의 로그 포맷을 정하지 않으면 아래와 같은 access log가 찍힌다.1123.1.1.1 - - [25/Jan/2018:21:55:35 +0900] &quot;GET /api/test?param=12341234 HTTP/1.1&quot; 200 48 1144 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (iPhone; CPU iPhone OS 11_1_2 like Mac OS X) AppleWebKit/604.3.5 (KHTML, like Gecko) Mobile/15B202 NAVER(inapp; blog; 100; 4.0.44)&quot; 그럼 이 로그를 아무 포맷팅 없이 로깅을 하면 그냥 한줄의 텍스트가 인덱싱이 된다. 하지만 이렇게 되면 elasticsearch 데이터를 다시 재가공하거나 별도의 작업이 필요할수도 있으니 중간에 있는 logstash에게 일을 시켜 좀더 nice 한 방법으로 인덱싱을 해보자. 바로 logstash 의 filter 기능이다. 그중 Grok filter 라는게 있는데 패턴을 적용하여 row data 를 필터링하는 기능이다. 조금 찾아보니 너무 고맙게도 아파치 필터 예제가 있어 수정하여 적용할수 있었다. http://grokconstructor.appspot.com/do/match?example=2그래서 적용한 필터설정은 다음과 같다.1234567filter &#123; grok &#123; match =&gt; &#123; message =&gt; &quot;%&#123;IP:clientIp&#125; (?:-|) (?:-|) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;URIPATH:uri&#125;%&#123;GREEDYDATA&#125;(?: HTTP/%&#123;NUMBER&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER&#125;)&quot; &#125; &#125;&#125; 이렇게 하고 elasticsearch 에 인덱싱을 하면 키바나에서 다음과 같이 볼수 있다.키바나에 내가 원하는 구조대로 이쁘게 들어가 있는 access log키바나에 내가 원하는 구조대로 이쁘게 들어가 있는 access log # 각 필드가 아닌 한줄로 인덱싱이 되어버린다.Elasticsearch 에 인덱싱이 되긴 하는데 로그 한줄이 통째로 들어가 버린다. message라는 이름으로… 알고보니 현재 구조는 logstash가 kafka 앞 뒤에 있다보니 producer logstash 와 consumer logstash 의 codec이 맞아야 제대로 인덱싱이 될수 있었다.먼저 access log에서 kafka 로 produce 하는 logstash 에서는 output 할때 codec 을 맞춰주고1234567output &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125;&#125; kafka 에서 consume 하는 logstash 에서는 input 에서 codec 을 맞춰준다.1234567input &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125;&#125; 그렇게 되면 codec이 맞아 각 필드로 이쁘게 인덱싱을 할수 있게 되었다. # 필요없는 uri는 제외하고 인덱싱할수 있을까?/으로는 uri 라던지 /server-status같이 알고있지만 인덱싱은 하기 싫은 경우는 간단하게 아래처럼 if문으로 제외시킬수 있었다.(당연하게 보일진 모르겟지만 내겐 너무 생각보다 편하게 이슈를 해결할수 있어서 좋았다.)123456789output &#123; if [uri] =~ /.+/ and [uri] != &quot;/server-status&quot; &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125; &#125;&#125; # 하나의 index로는 관리가 힘든데 나눌순 없을까?사실 이 항목은 logstash에 해당하는 옵션이긴 하지만. 겸사겸사 적어본다.이미 지난 로그는 지워야 할 상황이 온다. 이를테면 1년이 지났거나. 그럴경우 마지막 elasticsearch 로 output 하는 logstash 설정에서 다음과 같이 설정할수 있다.1234567output&#123; elasticsearch &#123; hosts =&gt; [&quot;123.1.2.3:9200&quot;, &quot;123.1.2.4:9200&quot;] index =&gt; &quot;apache-log-%&#123;+YYYY.MM&#125;&quot; document_type =&gt; &quot;apache&quot; &#125;&#125; 이렇게 하고 apache 라는 템플릿을 지정해 놓으면 달이 바뀔때마다 자동으로 해당 템플릿에 맞추어 index가 만들어지게 되고, 원하는 달 전체의 데이터를 한번에 지울수 있는 장점이 있는것 같다. 이런 저런 삽질의 과정의 끝에는 달콤한 보상이 따르는것 같다.(항상 그러는건 아니지만..) 아래처럼 대시보드를 만들어 한눈에 apache 단에서의 request를 분석할수 있게 되었다. 물론 이보다 더 한것도 할수 있을것 같다.키바나 대시보드 모습키바나 대시보드 모습 매번 새로운 기술을 습득할때마다 느끼는거지만, 고전 기술로 어렵게 어렵게 시간을 소비하며 구성하는 것보다 새로운 기술을 빨리 습득하고 삽질할 시간에 더 다양한 생각을 해보는게 좋은것 같다. 특히 이 Elasticsearch 는 설정 몇번만으로 이렇게 강력한(?) 구성을 만들수 있다는거에 너무 신기하면서도 감사하다. 자, 다음엔 또 어떤걸 해볼수 있을까!? 가즈아~ 2018. 01. 26 추가각 버전은 다음과 같다. (es를 어서 업그레이드 해야하는데…)elasticsearch : 2.4.0logstash : logstash-5.4.3kafka : 0.11.0.0apache : 2.2.x","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"https://taetaetae.github.io/tags/logstash/"},{"name":"kafka","slug":"kafka","permalink":"https://taetaetae.github.io/tags/kafka/"},{"name":"access log","slug":"access-log","permalink":"https://taetaetae.github.io/tags/access-log/"}]},{"title":"파이썬 버전 업그레이드 (2.6 > 3.6)","slug":"python-2-to-3","date":"2018-01-08T04:44:50.000Z","updated":"2020-04-23T04:41:36.822Z","comments":true,"path":"2018/01/08/python-2-to-3/","link":"","permalink":"https://taetaetae.github.io/2018/01/08/python-2-to-3/","excerpt":"파이썬 2.x 에서는 depreate 된 모듈도 많고 3.x에서만 지원되는 버전들이 많아지면서 실컷 개발을 해도 파이썬 버전때문에 다시 짜야하는 상황이 생긴다. 파이썬 버전업을 하고싶어 구글링을 해보면 이렇다할 정리된 문서가 잘 안나온다. (영어로된 포스트는 많이 있긴 하나, 필자의 환경과는 맞지 않는 …)","text":"파이썬 2.x 에서는 depreate 된 모듈도 많고 3.x에서만 지원되는 버전들이 많아지면서 실컷 개발을 해도 파이썬 버전때문에 다시 짜야하는 상황이 생긴다. 파이썬 버전업을 하고싶어 구글링을 해보면 이렇다할 정리된 문서가 잘 안나온다. (영어로된 포스트는 많이 있긴 하나, 필자의 환경과는 맞지 않는 …) 그래서 이것저것 삽질을 한 결과 파이썬 버전을 올릴수 있었고, 이를 포스팅 해보고자 한다. 그러보고니 2018년 첫 포스팅이네… 올해는 정말 적어도 한달에 1~2개는 올릴수 있는 내가 되기를… # 환경 CentOS 6.9 기본으로 python 2.6 이 설치되어 있는것을 확인할수 있다. (환경마다 다를수 있음.)12$ python -VPython 2.6 # 설치순서 필요한 유틸리티를 설치한다. 123$ sudo yum update$ sudo yum install yum-utils$ sudo yum groupinstall development yum 저장소에서는 최신 파이썬 릴리즈를 제공하지 않으므로 RPM 패키를 제공하는 IUM 이라는 추가 저장소를 설치 1$ sudo yum install https://centos6.iuscommunity.org/ius-release.rpm 파이썬 3.6 버전을 설치 1$ sudo yum install python36u pip 등 패키지 관련 모듈도 함께 설치 12sudo yum install python36u-pipsudo yum install python36u-devel 여기까지 하면 기존 파이썬 2.6과 새로 설치된 파이썬 3.6 이 설치되어있다. 1234567891011$ ll /usr/bin/python*-rwxr-xr-x 1 root root 9997450 Jan 2 16:02 pythonlrwxrwxrwx 1 root root 6 Jan 1 06:02 python2 -&gt; python-rwxr-xr-x 1 root root 9032 Aug 19 2016 python2.6-rwxr-xr-x 1 root root 1418 Aug 19 2016 python2.6-config-rwxr-xr-x 2 root root 6808 Oct 12 08:19 python3.6lrwxrwxrwx 1 root root 26 Jan 2 20:48 python3.6-config -&gt; /usr/bin/python3.6m-config-rwxr-xr-x 2 root root 6808 Oct 12 08:19 python3.6m-rwxr-xr-x 1 root root 173 Oct 12 08:19 python3.6m-config-rwxr-xr-x 1 root root 3339 Oct 12 08:16 python3.6m-x86_64-configlrwxrwxrwx 1 root root 16 Apr 25 2017 python-config -&gt; python2.6-config 환경변수를 설정해준다. 12$ sudo mv python python_backup$ sudo ln -s python3.6 python 확인 12$ python -VPython 3.6.3 # pip 를 이용한 모듈 설치 pip란 Python Package Index 의 약자로 공식홈페이지는 다음과 같다. ( https://pypi.python.org/pypi/pip ) 설치할 모듈을 다음과 같이 설치해주면 된다. ex : requests 모듈인 경우1$ sudo python3.6 -m pip install requests","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"}]},{"title":"Elastic{ON}Tour","slug":"elastic-on-tour","date":"2017-12-14T03:02:45.000Z","updated":"2020-04-23T04:41:36.729Z","comments":true,"path":"2017/12/14/elastic-on-tour/","link":"","permalink":"https://taetaetae.github.io/2017/12/14/elastic-on-tour/","excerpt":"작년에 팀을 옮기면서 로깅에 대해서 관심을 갖기 시작 하였고 찾아보다 ElasticStack 이 적합하다고 판단, 팀 내에서 나홀로 삽질해가며 지금의 로그 모니터링 시스템을 구축하였다. 그에 ElasticStack 에 관심을 갖던 찰나 지난 화요일(12월 12일)에 있었던 Elastic On Tour에 참석을 하였고 다양한 기술적 인사이트를 얻을수 있었는데 그 감동(?)을 잃기 싫어 정리해보고자 한다.","text":"작년에 팀을 옮기면서 로깅에 대해서 관심을 갖기 시작 하였고 찾아보다 ElasticStack 이 적합하다고 판단, 팀 내에서 나홀로 삽질해가며 지금의 로그 모니터링 시스템을 구축하였다. 그에 ElasticStack 에 관심을 갖던 찰나 지난 화요일(12월 12일)에 있었던 Elastic On Tour에 참석을 하였고 다양한 기술적 인사이트를 얻을수 있었는데 그 감동(?)을 잃기 싫어 정리해보고자 한다. # Registration + Partner Showcase코엑스 인터컨티넨탈 호텔에서 진행되었다. 역시 외국계 기업이여서 그런지 행사 규모가 어마어마 했다. 이정표를 따라 지하로 가서 등록을 하고, ElasticStack 을 이용해서 서비스를 하고 있는 파트너사들의 부스를 기웃거리며 ElasticStack의 저력(?)을 다시한번 실감을 할수 있었다. 특히 Elatic 본사에서 나온듯한 외국인들이 Q&amp;A 같은걸 해줬는데 답변을 해주는 외국인도 대단해 보였는데 질문을 하는 한국사람(?)들이 더 대단하게 보였다. 과연 난 저렇게 아무렇지 않고 프로페셔널(?)하게 질문을 할수 있을까?Registration + Partner ShowcaseRegistration + Partner Showcase # Track 1 : Partner SessionsTrack 1 과 2로 나뉘였는데 2는 Elastic Stack 을 경험하지 못해봤거나 소개하는 자리같아서 Track 1를 듣기로 하였다. 내가 도입을 할때만 해도 관련 자료가 잘 없었고, 정말 특이 케이스가 아닌 이상엔 잘 사용하지 않겠구나 하는 느낌이였는데 발표하시는 분들을 보고서는 생각이 180도 바뀌었다. 너무 활용들을 잘 하며 서비스를 하고 있었고 단순하게 검색엔진이 아닌 상황에 맞는 커스터 마이징이나 다른 기술 스택을 함께 사용함으로써 시너지 효과를 내고 있었다. Microsoft OpenSource 에 안좋은 이미지가 있으나 오래전부터 투자를 많이 해왔다고 설명을 하며 Azure라는 서비스에서 Elastic Stack 을 어떤식으로 활용하는지 발표를 하였다. 상당히 심플하고 처음 접하는 사람도 클릭 몇번으로 ES Cluster를 구성할수 있다는게 장점이였으나, 유료 + 커스터마이징 제한 이 아쉬웠다. S-Core : 에스코어 경험에 기반한 Elastic 활용법 EZFarm (외모 비하는 아니지만)농부 처럼 생기신 분이 나와서 기술에 대해 말씀하시는게 신기한 발표였다. 간단히 말하면 돼지가 물 먹는 량 등 농업/축산업의 데이터를 ES에 담고 머신러닝을 통하여 효율화 하는 방안 이였던것 같다. MEGAZONE 파트너 부스에서 티셔츠를 준(?) 곳이였는데 Elastic Cloud Seoul 을 발표하였다. (드디어 한국에도 이런 서비스가!) OpenBase 키바나 플러그인을 직접 개발하고 커스텀 UI의 사례를 보여주었다. 키바나 소스중에 엑셀 다운로드가 한글로 안되어 고쳐본것 말곤 플러그인을 개발할 생각은 없었는데 정말 개발자 스러운 발표였다. DIREA 결제 관련 장애추적 및 예측 시스템을 발표하였다. 마침 내가 하고있는 서비스와 비슷하고, 내가 구현해보려고 했던 부분과 거의 일맥상통한 부분이 있어서 소름이였다. Track 1 : Partner SessionsTrack 1 : Partner Sessions # Opening Keynote앞서 어떤 발표에서 ElasticSearch가 탄생하게된 계기가 어떤 분이 요리사가 되려는 아내를 위해 조리법을 더 빨리 검색할수 있는 엔진을 만들었다고 하는데 그 어떤분이 내눈앞에 나타나 발표를 하셨다. 현 Elastic CEO 이신 Shay Banon 이였다. (어색한 동시 통역으로 이해하였지만) 그분이 강조하신 Elastic 회사 정신인 “간단한건 간단하게 만들어야 하며 쉬워야 한다.” 가 연설중에 가장 인상적이였고, 통역하신 아주머님(?)때문이였는지 전달하시는 의도를 정확히 파악하긴 어려웠으나 일단 CEO를 포함한 전체 회사 분위기가 젊어보인다는걸 느낄수 있었다.Shay Banon key noteShay Banon key note # Break (식사시간)이런 세미나? 컨퍼런스? 를 많이 다녀본건 아니지만 역대급으로 좋았던 점심식사였다. ;)사실 혼자와서 밥을 어떻게 해결하나 했는데 우르르르 호텔 직원분들이 각 자리에 도시락을 대령(?)해주셔서 맛있게 먹을수 있었다.참 사람이 간사한게, 아침에 졸린눈 비벼가며 지옥철 고생을 뚫고 와서 힘들었지만 밥을 먹으면서 아침의 그 고생은 눈녹듯 사라졌다.호텔 도시락!!호텔 도시락!! # Deep Dive (Elasticsearch, Ingest, Kibana, Machine Learning)각 스택(?)에 대해서 변화된 부분, 그리고 활용가능성과 최근 출시한 6.X 버전에 대해서 소개하는 시간이였다. 사실 아직 2.4 버전을 운영중이라서인지 6.X의 변화된 부분, 그리고 5.x 버전에서의 차이를 설명해주는데 확 와닿지는 못하였다. 아직 5.x 버전의 필요성을 못느껴서 버전업을 안하고 있었는데 발표를 듣고 꼭 유료 라이센스를 구매하지 않아도 용량 단축이나 안정성, 추가 기능등 5.X로의 버전업은 해서 나쁠것도 없을듯 하다는 생각이 들었다. (단, 무조건 신버전이 좋은것은 아닌듯 잘 알아보고 해야겠지…)특히 키바나를 활용해 머신러닝을 눈앞에서 보여준건 너무 좋았다. 성능이 좋아서인지 데모시연 하는데 전혀 막힘이 없었으니…노란색 음영이 머신러닝으로 계산한 그래프의 위치값들노란색 음영이 머신러닝으로 계산한 그래프의 위치값들 # Customer PresentationsElastic Stack 및 X-Pack 을 활용하여 어떤 문제를 어떻게 해결하고 있는지에 대한 세션이였다. 삼성, NSHC, Naver 에서 약 20분씩 발표를 해주셨는데 약간 시간에 쫒기듯(?) 발표가 진행되어 집중이 잘 안되었다. 마지막에 우리 회사분이 발표하신 Application Logging with Elasticsearch at Naver 라는 주제는 실제로 내가 사용하고 있는 모듈인지라 관심갖고 들었다. 최근들어 자주 버벅이고 에러알림지연(기능중 하나)등 문제가 있었는데 이러한 부분들을 Multi-Cluster 등 튜닝을 통하여 해결했다는 부분을 설명해주셨다. (워낙에 양이 많으니…)NELONELO # 마치며밥도 맛있고 발표 내용들도 좋은 인사이트를 얻을수 있었고, 마지막에 경품추첨등 딱딱하지 않는 세미나 였다고 생각한다. 올해는 무료로 하는 행사였지만 내년부터는 유료라고 하는데 난 내년에도 사비를 내서라도 갈 생각이다. (회사지원이 된다면 좋겠지만 ^^;) 한가지 아쉬운건 너무 준비 없이 갔다는게 아쉽다. 사실 구버전(?) 2.X를 사용중이고 5.x 6.x 를 전혀 경험해보지 못해서 공감가는 부분이 없었는데, 나름 높은 버전으로 올려야 겠다는 생각을 할수있었던 좋은 시간이였던것 같다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"}]},{"title":"What is Kafka?","slug":"what-is-kafka","date":"2017-11-02T12:30:13.000Z","updated":"2020-04-23T04:41:36.894Z","comments":true,"path":"2017/11/02/what-is-kafka/","link":"","permalink":"https://taetaetae.github.io/2017/11/02/what-is-kafka/","excerpt":"필자가 맡고있는 서비스에 Elastic Stack 을 도입하면서 중간에 버퍼가 필요하여 Message-Queue 시스템들을 알아보던 중 Kafka 에 대해 알아보고, 정리를 해보게 된다.","text":"필자가 맡고있는 서비스에 Elastic Stack 을 도입하면서 중간에 버퍼가 필요하여 Message-Queue 시스템들을 알아보던 중 Kafka 에 대해 알아보고, 정리를 해보게 된다. # 기본설명 및 기존 메세징 시스템과 다른점 메세징 큐의 일종 말 그대로 분산형 스트리밍 플랫폼, LinkedIn에서 여러 구직 + 채용 정보들을 한곳에서 처리(발행/구독)할수 있는 플랫폼으로 개발이 시작 대용량의 실시간 로그 처리에 특화되어 설계된 메시징 시스템, 기존 범용 메시징 시스템대비 TPS가 매우 우수 메시지를 기본적으로 메모리에 저장하는 기존 메시징 시스템과는 달리 메시지를 파일 시스템에 저장 → 카프카 재시작으로 인한 메세지 유실 우려 감소 기존의 메시징 시스템에서는 broker가 consumer에게 메시지를 push해 주는 방식인데 반해, Kafka는 consumer가 broker로부터 직접 메시지를 가지고 가는 pull 방식으로 동작하기 때문에 consumer는 자신의 처리능력만큼의 메시지만 broker로부터 가져오기 때문에 최적의 성능을 낼 수 있다. # 카프카 주요 개념 producer : 메세지 생산(발행)자. consumer : 메세지 소비자 consumer group : consumer 들끼리 메세지를 나눠서 가져간다.offset 을 공유하여 중복으로 가져가지 않는다. broker : 카프카 서버를 가리킴 zookeeper : 카프카 서버 (+클러스터) 상태를 관리하고 cluster : 브로커들의 묶음 topic : 메세지 종류 partitions : topic 이 나눠지는 단위 Log : 1개의 메세지 offset : 파티션 내에서 각 메시지가 가지는 unique id # 카프카는 어떤식으로 돌아가는가 zookeeper 가 kafka 의 상태와 클러스터 관리를 해준다. 정해진 topic 에 producer 가 메세지를 발행해놓으면 consumer 가 필요할때 해당 메세지를 가져간다. (여기서 카프카로 발행된 메세지들은 consumer가 메세지를 소비한다고 해서 없어지는게 아니라 카프카 설정log.retention.hours(default : 168[7일])에 의해 삭제된다.) partition 개수와 consumer group 개념 하얀색(consumer-01) : 파티션 개수가 4개인데 비해 컨슈머가 3개, 이렇게 되면 어느 컨슈머가 두개의 파티션을 담당해야하는 상황이 생긴다. 주황색(consumer-02) : 파티션 개수가 4개인데 비해 컨슈머가 5개, 이렇게 되면 하나의 노는(?) 컨슈머가 생기는 상황이 생긴다. 가장 적절한 개수는 정해지지 않았지만 통상 컨슈머그룹의 컨슈머 개수와 파티션 개수를 동일하게 가져가곤 한다. # 참고 url http://kafka.apache.org/ http://www.popit.kr/author/peter5236/ http://jinhokwon.tistory.com/168 http://programist.tistory.com/entry/Apache-Kafka-클러스터링-구축-및-테스트 https://www.elastic.co/kr/blog/just-enough-kafka-for-the-elastic-stack-part1 https://www.slideshare.net/springloops/apache-kafka-intro20150313springloops-46067669","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://taetaetae.github.io/tags/kafka/"}]},{"title":"Deview-2017 Day1 리뷰","slug":"deview-2017-review","date":"2017-10-16T08:29:55.000Z","updated":"2020-04-23T04:41:36.715Z","comments":true,"path":"2017/10/16/deview-2017-review/","link":"","permalink":"https://taetaetae.github.io/2017/10/16/deview-2017-review/","excerpt":"벌써 10번째 Naver에서 주최하는 Deview. 올해도 어김없이 참석을 하게 되었고, 이번엔 보고 듣고 느꼈던 부분들을 조금이라도 간직하고 싶은 마음에 바로 블로깅을 하려고 한다. (오랜만에 블로깅이긴 하지만…)","text":"벌써 10번째 Naver에서 주최하는 Deview. 올해도 어김없이 참석을 하게 되었고, 이번엔 보고 듣고 느꼈던 부분들을 조금이라도 간직하고 싶은 마음에 바로 블로깅을 하려고 한다. (오랜만에 블로깅이긴 하지만…)항상 Deview에 올때마다 느끼는 부분인데 이번참석이 3번째 되는듯 하다 세상은 좁고 능력자는 많으며 내가 한번쯤 본것들은 이미 지나간 기술들이라는것, 더불어 단상위에 올라가 발표하는 사람들도 예전엔 나와 똑같이 발표를 듣는 일반 사람이였다는것. 이번에도 많은 생각을 하게 되었다. 구구절절 개인적으로 느낀점을 적는것에 앞서 강한 기억이 남았던 몇몇 세션들에 대해서 간략하게 리뷰를 먼저 하는게 맞는 순서같다. # 책 읽어주는 딥러닝 ( 김태훈 / 데브시스터즈 )슬라이드 자료네이버에서 유인나의 목소리로 책을 읽어주는것을 보고 흥미를 얻어 개발하기 시작했다고 한다.음성합성은 데이터가 많아야 머신러닝이나 딥러닝 기술을 접목시키는데 도움이 되는데 박근혜 전 대통령, 문재인 대통령, 손석희 아나운서의 영상에서 데이터를 추출하여 문장별로 텍스트-음성을 맞추고(pair) 머신러닝 + 딥러닝 기술을 이용해서 만들수 있었다고 한다. 추후 누구나 사용할 수 있도록 파이썬 모듈로 제공한다고 하니, 감사할 따름이다.사실 머신러닝에 관심만있었지 이렇다할 공부나 직접 구현은 단한번도 안해보고 해당 세션을 들어보니 그냥 우와 신기하다정도였는데. 이번기회에 작은것부터 하나씩 시작하면서 요즈음 핫한(?) 트랜드를 따라가는 것도 괜찮은 방법같아 보인다. (앗, 우선 파이썬부터…) # 그런 REST API로 괜찮은가 ( 이응준 / 비바리퍼블리카 )슬라이드 자료발표자분을 어디서 많이 봤다 했더니만 예전에 우리 회사 사람이였다. 수업도 들어봤고, 같이 알고리즘 스터디도 했고(한번 나갔지만…). 발표 첫 부분에 자신이 10년전에 데뷰 staff 를 시작했는데 10년을 다 못채우고 퇴사를 했다고 ㅎㅎ.. 아무튼 개인적으로 나름 반가운 분이라 더 관심갖고 듣게 되었다.REST 가 무엇인가?에 대한 발표다. 결론부터 말하자면 아래 3가지중 하나를 사용하면 될것이라고 한다. REST API 를 구현하고 REST API라고 부른다. REST API 구현을 포기하고 HTTP API 라고 부른다. REST API 가 아니지만 REST API 라고 부른다. (현재 대부분의 API들의 상태) REST API를 구성하는 스타일중 눈여겨 볼만한 부분은 크게 두가지가 있다고 한다. (uniform interface) self-descriptive messages : 메시지는 스스로 설명이 되어야 한다. hypermedia as the engine of application state (HATEOAS) : 전이(상태의 이동)가 될수있는 정보가 있어야 한다. 정리를 해보면 REST API로 만들려면 제대로 알고 만들어라 라는 메시지가 강한 발표내용같다. 나도 이제까지는 그냥 json 으로 내려준다는 것, GETㆍPOST 등 HTTP Method 사용하는 것으로만 알고있었는데 개인적으로는 발표자분이 말씀하신 두가지 내용은 지키는게 맞다고 생각한다. 즉, 정말 REST 하게 만들꺼면 정확한 사용법을 알고 만드는게 좋아보인다. # 동네 커피샵도 사이렌오더를 쓸 수 있을까? ( 허형, 나동진 / 삼성전자[Lunch class] )슬라이드 자료오늘 발표중에 가장 들어보고 싶었던 세션. 예전부터 사이렌오더가 어떤식으로 동작하는지 + 우리회사 커피숍도 사내 앱을 활용해서 만들어 볼순 없을지(아이디어) 이런저런 생각이 많았었는데 딱! 원하던 발표가 있어 듣게 되었다.삼성전자 소속이신 분들이 따로 그룹을 만들어 진행하면서 만난 부분들을 발표해주셨는데 신기한 기술들이 많아 듣는 내내 흥미진진 했다. PWA(Progressive Web App) : PWA 로 모바일 청첩장을 만들었다고 한다. (결혼식 전날입니다. 오늘 결혼합니다. 이벤트[추첨]를 진행합니다. 등등..) Physical Web(Beacon), NFC … Browser Fingerprint (Device 구분) Push Nofification Web Payment 결국 정리를 해보면 동네 커피샵에서 사이렌 오더를 사용하기위해 이러저러한 기술들을 시도해봤다~인데. 각 기술들에 있어 현실적인 상황에 한계점이 있고, 그래서 결국 처음에 이야기 된 동네 커피샵에서 사이렌 오더를 사용에 대한 결과물이 없어서 아쉬웠다. 엄청 기대했는데 말이다.하지만 PWA를 이용해서 모바일 청접장을 만든 부분은 정말 찬사를 보내주고 싶은 아이디어 같다.나도 나중에 해야지~예전 “날씨”라는 웹서비스를 만들면서 웹이라는 환경에서 기상속보나 갑작스러운 눈/비 알림을 단순히 화면에 뿌려주는것이 아니라 사용자 기기에 노티(푸시)해줄수는 없을까하며 잠깐 본 기술이 PWA 였는데 난 프로토타이핑만 해본 수준이지만 이분들은 실제로 본업과는 별개로 구현을 해보는 노력을 했다는것에 내 자신이 부끄러워 진다. 여전히 이번에도 뒤통수 여러대 맞은 Deivew 2017. 우물안의 개구리라는 마음을 잃지 말고 + 나는 개발자라는 것을 잊지 말아야겟다고 또 다짐해본다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"deview","slug":"deview","permalink":"https://taetaetae.github.io/tags/deview/"}]},{"title":"Apache keepAlive","slug":"apache-keep-alive","date":"2017-08-28T10:56:40.000Z","updated":"2020-04-23T04:41:36.668Z","comments":true,"path":"2017/08/28/apache-keep-alive/","link":"","permalink":"https://taetaetae.github.io/2017/08/28/apache-keep-alive/","excerpt":"서버를 운영하다보면 간혹 문제가 발생하곤 한다. 이를테면 메모리가 다른이유없이 올라간다거나, 사용자 입장에서 응답속도가 간헐적으로 느린다거나. 그럴때마다 선배개발자분들께서 가장먼저 입에 오르내리는 단어. keepAlive.","text":"서버를 운영하다보면 간혹 문제가 발생하곤 한다. 이를테면 메모리가 다른이유없이 올라간다거나, 사용자 입장에서 응답속도가 간헐적으로 느린다거나. 그럴때마다 선배개발자분들께서 가장먼저 입에 오르내리는 단어. keepAlive. 대충 검색을 해보면 접속을 유지하거나 그렇지 않거나 하는 설정이구나 만으로 생각했었는데 제대로 짚고 넘어가는 의미에서 정리를 해볼 필요가 있을것 같다. # 정의우선 2.4버전 기준 도큐먼트의 내용을 볼 필요가 있다.https://httpd.apache.org/docs/2.4/mod/core.html#keepalive The Keep-Alive extension to HTTP/1.0 and the persistent connection feature of HTTP/1.1 provide long-lived HTTP sessions which allow multiple requests to be sent over the same TCP connection. In some cases this has been shown to result in an almost 50% speedup in latency times for HTML documents with many images. To enable Keep-Alive connections, set KeepAlive On. 발번역(파파고의 힘!)으로 이해한 내용으로는, keepAlive를 사용하면 50%까지 대기시간을 단축할수 있다는 설정이라 나와있다. 하지만 나같은 영어울렁중이 있는 사람들(?)은 영어 문서만을 가지고 완벽히 이해할수는 없다. 그래서 이것저것 찾아보고 다시 정리를 해본다. # 정리기본적으로 외부 사용자에 의해 요청(access)이 들어오게 되면 mpm방식이 어떤거든 간에 (아파치 MPM에 대해서도 정리를 해야겠군..) 아파치가 처리를 하든 WAS에게 넘겨주든 하나의 흐름이 들어오는데 동일한 사용자에 대해서 이 흐름을 끊고 다시 요청 받을것인가 아니면 연결을 유지하고 바로 처리를 할것인가에 대한 설정값으로 이해하면 좋을듯 싶다. 필자가 표현을 잘 못해서 그런것일수도 있으니 그림으로 보는게 가장 빠를지도 모르겠다.[출처 : https://www.svennd.be/keepalive-on-or-off-apache-tuning][출처 : https://www.svennd.be/keepalive-on-or-off-apache-tuning]위 그림은 IT전공자라면(?) 한번쯤은 봤을 tcp 3-way handshake 인데, keepAlive 를 on 하면 초기 연결하는 비용을 조금이나마 줄일수 있다는걸 보여준다. 즉, 다시말하면 keepAlive는 한번 연결된 상대에 대해서 연결을 잃지 않고(이런저런 설정값에 의존) 지속적으로 요청에 응답을 해줄수 있다는 옵션이다. 간단하게 생각하면 이 설정값을 이용하면 모든 요청에 의해서 지속적으로 응답을 해줄수 있으니 무조건 on하면 되는거 아닐까? 하는 생각이 먼저든다. default 값도 on 이니. 허나 자칫 잘못하면 메모리가 모든 접속자 마다 연결 유지를 해 놓아야 하기 때문에 아파치 프로세스수가 기하 급수적으로 늘어나 MaxClient값을 초과하게 된다. 또한 On상태일때 접속유지 하는 프로세스들 때문에 메모리를 그 만큼 많이 사용하게 된다.이 양날의 검 keepAlive는 과연 어떻게 사용해야 하는걸까? # 사용 시기우선 앞서 말했던것과 같이 기본값은 On이다. 즉, 아파치 설치시 기본으로 연결유지를 하는것. 하지만 상황에 따라 On할지 Off할지를 결정해야 한다. On 해야할 경우접속자의 수 상관없이 메모리가 충분할경우 메모리가 충분하다 : 접속자가 maxclient 값에 도달했을경우, swap메모리를 사용하지 않는 상태 Off 해야할 경우 동시 접속자수가 많을경우 메모리가 충분하지 못할경우 하지만 위의 경우는 지극히 일반적인경우고 운영하는 서버의 메모리의 상태나 접속자의 수를 확인하면서 조정이 필요하다. 꼭 keepAlive에 국한되는것은 아니지만 모든 개발에는 절대적인 답은 없는것 같다. # keepAlive 를 On할 경우의 추가 셋팅부분(참고 : https://abdussamad.com/archives/169-Apache-optimization:-KeepAlive-On-or-Off.html) MaxKeepAliveRequest [회수]하나의 지속적인 연결에서 서비스를 제공할 요청의 최대 값을 설정한다. 50 과 75 사이 정도면 충분하다고 한다. KeepAliveTimeout [초]연결된 사용자로부터 새로운 요청을 받기까지 서버가 얼마나 기다릴 것인가를 설정한다. (default : 15초) 일반적으로 1~5초 정도로 설정하곤 한다. MaxClients [회수]자식프로세스들의 최대 값을 설정한다. 이는 메모리의 크기와 상관관계가 있다. MaxRequestsPerChild [회수]클라이언트들의 요청 개수를 제한. 만약 자식 프로세스가 이 값만큼의 클라이언트 요청을 받았다면 이 자식 프로세스는 자동으로 죽게 된다. 0 일 경우엔 무한대. 이 설정값으로 메모리누수를 방지할수 있다. 어쨋든, 정답은 없다. 상황에 맞춰서 설정할것! 그에따른 책임은 본인이 가져가야하는걸 명심!","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"keepAlive","slug":"keepAlive","permalink":"https://taetaetae.github.io/tags/keepAlive/"}]},{"title":"hexo 블로그에 tranquilpeak 테마 적용하기","slug":"hexo-themes-tranquilpeak","date":"2017-08-27T08:52:56.000Z","updated":"2020-04-23T04:41:36.763Z","comments":true,"path":"2017/08/27/hexo-themes-tranquilpeak/","link":"","permalink":"https://taetaetae.github.io/2017/08/27/hexo-themes-tranquilpeak/","excerpt":"여러가지 hexo 테마중에 그나마(?) 영어로 된 문서가 있어서 적용해보게 된 tranquilpeak 라는 테마. 오늘은 해당 테마를 적용하면서 겪은 문제, 그리고 적용 방법에 대해서 간략하게나마 정리해보고자 한다. (다른 테마들은 거의다 중국쪽이나 일본…)","text":"여러가지 hexo 테마중에 그나마(?) 영어로 된 문서가 있어서 적용해보게 된 tranquilpeak 라는 테마. 오늘은 해당 테마를 적용하면서 겪은 문제, 그리고 적용 방법에 대해서 간략하게나마 정리해보고자 한다. (다른 테마들은 거의다 중국쪽이나 일본…)먼저 hexo 공식사이트에서 알려주는 테마들은 다음 사이트에서 확인해 볼수 있다. https://hexo.io/themes/index.html 기존에는 hueman이라는 테마를 사용하고 있었는데 (링크), 오랜만에 블로그를 다시(?) 시작하는 느낌을 내보고 싶었고 보다 더 심플하고 유행에 안탈것 같은(순전히 필자 생각) 테마를 찾아보다 tranquilpeak이라는 테마를 선택하게 되었다. 공식홈페이지 : https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak 샘플사이트 : http://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/ 우선 간략하게 설치과정을 나열해보면 다음과 같다. themes 폴더내에 테마파일을 받은후 압축 해제 테마 폴더 이름을 변경 _config.yml 파일 내에 테마 설정 부분 변경 ( theme: tranquilpeak ) hexo clean → hexo generate → hexo server(or hexo deploy) 이렇게 하면 아주 간단하게 테마가 변경이 된다. 혹여나(필자처럼) 기존 테마를 커스터마이징 하고 싶을 경우는 별도의 과정이 추가로 필요하다. 기존에는 css나 js만 변경하면 간단히 수정되었는데 이 테마는 약간의 빌드(?)를 필요로 한다. 따라서 css나 js등 html 요소들을 수정하였다면 다음과 같은 과정이 필요하다.(테마폴더 최상위에서) npm install bower install css 나 js 변경 grunt build hexo clean → hexo generate → hexo server(or hexo deploy) 나같은 경우는 테마에 적용된 폰트를 바꾸기 위해 블로그 를 참조하였다. (해당 아티클에다 댓글폭탄을 ㅎㅎ;;)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"},{"name":"tranquilpeak","slug":"tranquilpeak","permalink":"https://taetaetae.github.io/tags/tranquilpeak/"}]},{"title":"다시 시작하자","slug":"refresh","date":"2017-07-09T08:16:23.000Z","updated":"2020-04-23T04:41:36.829Z","comments":true,"path":"2017/07/09/refresh/","link":"","permalink":"https://taetaetae.github.io/2017/07/09/refresh/","excerpt":"마지막 포스팅을 한지 벌써 3개월이 지났다. 그렇게 바빴던것도 아니고 블로그포스팅을 할 시간이 안난것도 아닌데 어느덧 다시 정신차리고 블로그를 포스팅 하려고보니 3개월이라는 시간이 흘러버렸네","text":"마지막 포스팅을 한지 벌써 3개월이 지났다. 그렇게 바빴던것도 아니고 블로그포스팅을 할 시간이 안난것도 아닌데 어느덧 다시 정신차리고 블로그를 포스팅 하려고보니 3개월이라는 시간이 흘러버렸네챗바퀴같은 일상, 느즈막히 일어나서 회사출근하고 정신없이 일하다가 퇴근, 그리고 늦게까지 잠못이루다 또 다음날이면 느즈막히 일어나고… 뭔가 변화가 필요하다. 매일 일기쓰기 : 일기라고 해봤자 거창한건 아니고 딱 3개월만 써보자. 오늘 뭐했는지. 자기전에 딱 10분이면 좋을듯 아침에 일찍 일어나기 : 월수금 수영에 화목 배드민턴. 주말에도 일찍일어나고. 일찍일어나면 먹이도 더 먹는다고 하지 않았던가 달력활용하기 : 운동하는것도 그렇지만, 달력을 자주 보면서 빼먹지 말아야 할 중요한 날들은 반드시 메모하고 기억하자 기타 : 책좀 많이 읽고 운동도 꾸준히 해야겠다. 물론 기술블로그 포스팅도 잊지말고. 첫술에 배부르랴. 하나둘씩 퍼즐 맞춰나가듯 해보다보면 내 자신이 바뀌어 있겠지.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"mybatis insert/update 쿼리실행후 결과 가져오기","slug":"mybatis-useGeneratedKeys","date":"2017-04-04T02:41:28.000Z","updated":"2020-04-23T04:41:36.804Z","comments":true,"path":"2017/04/04/mybatis-useGeneratedKeys/","link":"","permalink":"https://taetaetae.github.io/2017/04/04/mybatis-useGeneratedKeys/","excerpt":"Select문이 아닌 다른 SQL Query(insert, update 등) 를 실행하고서 결과를 봐야하는 상황이 생긴다. 정확히 잘 수행되었나에 대한 확인. 어떻게 쿼리가 잘 수행되었나를 확인하는 방법은 다음과 같다.※ 참고 url : http://www.mybatis.org/mybatis-3/ko/sqlmap-xml.html","text":"Select문이 아닌 다른 SQL Query(insert, update 등) 를 실행하고서 결과를 봐야하는 상황이 생긴다. 정확히 잘 수행되었나에 대한 확인. 어떻게 쿼리가 잘 수행되었나를 확인하는 방법은 다음과 같다.※ 참고 url : http://www.mybatis.org/mybatis-3/ko/sqlmap-xml.html # useGeneratedKeys, keyProperty 옵션사용하는 데이터베이스가 자동생성키를 지원한다면(mySql 같은) 해당옵션을 이용해 결과를 리턴 받을수 있다.예로들어 파라미터로 아래 모델객체를 넘긴다고 가정하고123456public Student &#123; int id; String name; String email; Date regist_date;&#125; 아래 mybatis 구문으로 insert를 시도하게되면, 파라미터로 넘긴 Student 객체의 id값에 insert 했을때의 key값(id)이 들어오게 된다.123456Student student = new Student();student.setName('bla');student.setEmail('bla@naver.com');mapper.insertStudents(student); // 쿼리실행student.getId(); // 추출 가능 1234&lt;insert id=\"insertStudents\" useGeneratedKeys=\"true\" keyProperty=\"id\" parameterType=\"Student\"&gt; insert into Students ( name, email ) values ( #&#123;name&#125;, #&#123;email&#125; )&lt;/insert&gt; # selectKey 옵션Oracle 같은 경우는 Auto Increment 가 없고 Sequence를 사용해야만 하기 때문에 위 옵션을 사용할수가 없다. 하지만 다른 우회적인(?) 방법으로 위와같은 효과를 볼수가 있다.파라미터의 모델이나 java구문은 위와 동일하고 xml 쿼리 부분만 아래와 같이 설정해주면 된다.123456789&lt;insert id=\"insertStudents\" parameterType=\"Student\"&gt; &lt;selectKey keyProperty=\"id\" resultType=\"int\" order=\"BEFORE\"&gt; select SEQ_ID.nexyval FROM DUAL &lt;/selectKey&gt; insert into Students (id, name , email) values (#&#123;id&#125;, #&#123;name&#125;, #&#123;email&#125;)&lt;/insert&gt; 위와같은 코드에서 쿼리가 실행되기 전에 id값에 Sequence에 의해 값을 셋팅하게 되고, 자동적으로 해당 값을 Student의 id에 set하게 되서 동일한 결과를 볼수가 있다. 항상 테이블의 key값에만 해당하는것이 아니다. key값과는 전혀 상관없는 값도 selectKey 구문으로 리턴할수가 있는데 order옵션을 AFTER로 주고 리턴하고자 하는 값을 명시해주면 된다.아래 코드에서는 입력할시 id값을 Sequence에서 가져오는게 아니라 수동으로 넣어주고, 입력했던 id에 맞는 regist_date 값을 리턴받아 위에서처럼 동일하게 값를 가져올수 있다.123456789&lt;insert id=\"insertStudents\" parameterType=\"Student\"&gt; &lt;selectKey keyProperty=\"regist_date\" resultType=\"java.util.Date\" order=\"AFTER\"&gt; select regist_date FROM students WHERE id = #&#123;id&#125; &lt;/selectKey&gt; insert into Students (id, name , email, regist_date) values (#&#123;id&#125;, #&#123;name&#125;, #&#123;email&#125;, syadate)&lt;/insert&gt;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://taetaetae.github.io/tags/mybatis/"},{"name":"oracle","slug":"oracle","permalink":"https://taetaetae.github.io/tags/oracle/"}]},{"title":"Oracle + Mybatis 환경에서의 Date 다루기","slug":"oracle-mybatis-date","date":"2017-03-23T02:16:05.000Z","updated":"2020-04-23T04:41:36.815Z","comments":true,"path":"2017/03/23/oracle-mybatis-date/","link":"","permalink":"https://taetaetae.github.io/2017/03/23/oracle-mybatis-date/","excerpt":"# 상황 Oracle, Java 8, mybatis3 환경 Date컬럼에 데이터가 있는데 이를 select query로 조회하여 Model에 바인딩 시키고자 함.","text":"# 상황 Oracle, Java 8, mybatis3 환경 Date컬럼에 데이터가 있는데 이를 select query로 조회하여 Model에 바인딩 시키고자 함. 쿼리에 아무 기능을 추가하지 않고 Date 형태로 Model에 바인딩을 하면 시분초가 없어진 2017-01-01 00:00:00 형태로 남게됨 그래서 아래처럼 쿼리 작성할 때마다 TO_CHAR를 사용해서 포맷에 맞추어 형변환을 시키고 Date 또는 String으로 Model에 바인딩 하곤 했음. 1234SELECTTO_CHAR(reg_ymdt, 'YYYY-MM-DD HH24:MI:SS') AS registDateFROM... 이렇게 하다보니 query 만들때마다 형변환하는 쿼리를 만들어줘야하고, 자칫 포맷형식을 다르게 적으면 엉뚱한 결과를 초래하거나, Date형을 그대로 받아 사용해야하는 상황에서는 다시 형변환하는 과정(String to Date)을 해줘야만 함. .. 귀차니즘의 시작 : 삽질 1. 삽질의 시작1-1. 오라클의 DATE형 → java.sql.Date 의 경우 mybatis에서는 자동적으로 org.apache.ibatis.type.SqlDateTypeHandler를 호출하게됨 mybatis 3 문서 참고 해당 핸들러의 내부 데이터 변환 코드는 다음과 같음 1234@Overridepublic Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; return rs.getDate(columnName);&#125; java.sql.ResultSet.getDate()메소드를 호출하면 실제 ‘yyyy-mm-dd’ 만 가져와 리턴하게됨 (여기서 디버깅 해보면 rs.getTimestamp(columnName)값은 시분초까지 다 들어가 있음) 따라서 시간값이 없는 yyyy-mm-dd 형태로 리턴이 됨1-2. 오라클의 DATE형 → java.util.Date 의 경우 mybatis에서는 자동적으로 org.apache.ibatis.type.DateOnlyTypeHandler를 호출하게됨 mybatis 3 문서 참고 해당 핸들러의 내부 데이터 변환 코드는 다음과 같음 12345678@Overridepublic Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; java.sql.Date sqlDate = rs.getDate(columnName); if (sqlDate != null) &#123; return new java.util.Date(sqlDate.getTime()); &#125; return null;&#125; 위의 org.apache.ibatis.type.SqlDateTypeHandler 변환코드에서 발생한 문제점과 같이 yyyy-mm-dd 만 가져와서 java.sql.Date 객체를 만들고, 이 정보를 토대로 java.util.Date 객체를 만들게 되는데 앞서 시간값을 뺀 정보로 만들어졌기 때문에 결국 동일하게 yyyy-mm-dd 형태로 리턴이 됨 2. 삽질완료, 해결의 시작 오라클 + mybatis 환경에서 Date타입을 다루기 위해서는 타입핸들러를 명시적으로 만들어줘야 한다는걸 알게됨.2-1. 오라클의 DATE형 → java.sql.Date 의 경우 아래처럼 코드를 작성하여 커스텀 핸들러를 만들어 등록을 시켜준다. mybatis-config.xml 123&lt;typeHandlers&gt; &lt;typeHandler handler=\"com.naver.dbill.admin.common.handler.CustomDateHandler\"/&gt;&lt;/typeHandlers&gt; CustomDateHandler.java 123456789101112131415...import java.sql.Date;...public class CustomDateHandler extends BaseTypeHandler&lt;Date&gt; &#123; ... @Override public Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; Timestamp sqlTimestamp = rs.getTimestamp(columnName); if (sqlTimestamp != null) &#123; return new Date(sqlTimestamp.getTime()); &#125; return null; &#125; ...&#125; 위 코드를 작성하고 실행해보면 정상적으로 시분초 값이 있는 완전한 Date 형태를 볼수 있다.2-2. 오라클의 DATE형 → java.util.Date 의 경우 아래처럼 코드를 작성하여 커스텀 핸들러를 만들어 등록을 시켜준다. 단, mybatis 3 문서를 보면 java.sql.Date 와는 다르게 기본으로 설정된 typeHandler가 JDBC에 따라 3가지가 있다. 따라서 작성한 커스텀 핸들러를 적용하기 위해서는 명시적으로 자바타입 과 JDBC타입 을 적어줘야 정상적으로 오버라이딩이 되어 해당 핸들러를 사용하게 된다. mybatis-config.xml 123&lt;typeHandlers&gt; &lt;typeHandler handler=\"com.naver.dbill.admin.common.handler.CustomDateHandler\" javaType=\"java.util.Date\" jdbcType=\"DATE\"/&gt;&lt;/typeHandlers&gt; CustomDateHandler.java 는 위와 동일하다. ( import java.util.Date; 사용으로 변경 ) # 삽질하며 알게된 보너스 지식 java.sql.Date 는 java.util.Date 을 상속받았다. 12public class Date extends java.util.Date &#123;&#125; 검색을 하다보면 알수있겠지만 java.sql.Date 는 JDBC등을 이용해서 데이터베이스의 데이터를 사용하는데 적합하고, java.util.Date 은 보다 범용적인 날짜나 시각정보를 다룰때 적합하다고 한다. toString 메소드의 리턴 Format 형태 java.sql.Date : yyyy-mm-dd java.util.Date : EEE MMM dd HH:mm:ss zzz yyyy mybatis 에서 형변환은 mybatis 3 문서에 나와있는 자바타입과 JDBC타입이 일치할 경우에 해당 타입 핸들러를 기본으로 사용하게 된다. # 정상혁 님 조언 ( http://d2.naver.com/helloworld/645609 작성하신분 ) Oracle의 JDBC 드라이버가 예상 밖으로 동작하네요. Oracle의 DATE 타입도 문서를 보니 시분초까지 저장하게 되어 있는데, Oracle JDBC 구현체가 DATE 타입의 철학을 오해한게 아닌가하는 생각도 듭니다. 참고로 java.sql.Date, java.sql.TimeStamp는 잘못된 설계라는 비판이 많습니다. 저도 Java의 날짜와 시간 API 라는 글에서 아래와 같이 적은 적이 있습니다. 123java.sql.Date 클래스는 상위 클래스인 java.util.Date 클래스와 이름이 같다. 이 클래스를 두고 Java 플랫폼 설계자는 클래스 이름을 지으면서 깜빡 존 듯하다는 조롱까지 나왔다.[24]그리고 이 클래스는 Comparable 인터페이스에 대한 정의를 클래스 선언에서 하지 않았기 때문에 Comparable과 관련된 Generics 선언을 복잡하게 만들었다.[25]java.sql.TimeStamp 클래스는 java.util.Date 클래스에 나노초(nanosecond) 필드를 더한 클래스이다. 이 클래스는 equals() 선언의 대칭성을 어겼다. Date 타입과 TimeStamp 타입을 섞어 쓰면 a.equals(b)가 true라도 b.equals(a)는 false인 경우가 생길 수 있다.[26] 이런 이유 때문에 저는 가급적 Java8에 나온 ZonedDateTime 류를 모델객체에서는 쓰고 있기는합니다. 하지만 그 클래스도 JDBC 레벨에서 제대로 매핑을 안 해주는 경우가 있어서 converter, typeHandler류를 따로 만들어야합니다. 참고로 Spring JDBC에서는 아래와 같이 Converter를 만들어서 해결했습니다. 123456public class ZonedDateTimeConverter implements Converter&lt;Timestamp, ZonedDateTime&gt; &#123; @Override public ZonedDateTime convert(Timestamp source) &#123; return ZonedDateTime.ofInstant(source.toInstant(), ZoneId.of(\"UTC\")); &#125;&#125; java.sql.Date vs java.util.Date 둘 중에 선택한다면 모델 객체에서는 java.util.Date가 더 어울린다는 생각이 듭니다. 모델에 java.sql.Date가 있는 것은 Controller에서 SqlException이 있는 것 같은 비슷한 느낌이랄까요..^^; # 마치며 삽질을 하더라도 가급적이면 영양가 있는 삽질이 되야 할것같다. (하루종일 이것 붙잡다가 업무를 못해버리는;;) API문서, 블로그문서, 검색결과에 맹신하지말고 실제 소스까지 들어가봐서 확신을 갖자.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"date","slug":"date","permalink":"https://taetaetae.github.io/tags/date/"},{"name":"mybatis","slug":"mybatis","permalink":"https://taetaetae.github.io/tags/mybatis/"},{"name":"oracle","slug":"oracle","permalink":"https://taetaetae.github.io/tags/oracle/"}]},{"title":"스프링환경에서의 파라미터 관련 정리","slug":"spring-parameter","date":"2017-03-12T09:03:01.000Z","updated":"2020-04-23T04:41:36.880Z","comments":true,"path":"2017/03/12/spring-parameter/","link":"","permalink":"https://taetaetae.github.io/2017/03/12/spring-parameter/","excerpt":"일반적인 웹 프로젝트 구성에서는 Controller레벨에서 응답을 받고 비지니스 로직을 처리 후에 다시 View레벨로 넘어가는게 통상적인 흐름이다. 이 부분에서 파라미터 관련한 여러가지 부분에 대해 정리해보고자 한다.","text":"일반적인 웹 프로젝트 구성에서는 Controller레벨에서 응답을 받고 비지니스 로직을 처리 후에 다시 View레벨로 넘어가는게 통상적인 흐름이다. 이 부분에서 파라미터 관련한 여러가지 부분에 대해 정리해보고자 한다. httpServletRequest.getParameter()아래소스처럼 HttpServletRequest의 getParameter() 메서드를 이용하여 파라미터값을 가져올 수 있다.123456@RequestMapping(\"/\")public String home(HttpServletRequest httpServletRequest) &#123; String id = httpServletRequest.getParameter(\"id\"); return \"home\";&#125; @RequestParam또다른 방법으로는 @RequestParam 어노테이션을 이용하면 간단하게 파라미터값을 가져올수 있다. 우선, 해당 어노테이션의 옵션값들에 대해 간략하게 확인하고 넘어가는게 좋을듯 싶다. API문서 4.3.6 기준 이름 타입 설명 name, value (Alias for name) String 파라미터 이름 required boolean 해당 파라미터가 반드시 필수인지 여부, 기본값은 true defaultValue String 해당 파라미터의 기본값 위 옵션값들을 조합하여 컨트롤러 메소드에 적용해보면 아래 소스와 같이 만들어지고, 이렇게 reqeust에서 파라미터값을 가져올수 있다.1234@RequestMapping(\"/\")public String home(@RequestParam(value=\"id\", defaultValue=\"false\") String id) &#123; return \"home\";&#125; 이 어노테이션을 이용하게되면 자칫 잘못하다간 에러를 만날수가 있는데 required값을 true로 해놓고 (필수 파라미터 설정) 해당 파라미터를 사용하지 않고 요청을 보내게 되면 HTTP 400 에러를 받게 되니 각 옵션들을 정확히 확인하고 사용해야 할 것 같다.물론 컨트롤러의 메소드에서 해당 어노테이션을 사용하지 않고도 아래 코드처럼 바로 받을수 있다. 이렇게 바로 받을 경우는 필수 파라미터값이 false로 설정이 되고 변수명과 동일한 파라미터만 받을수 있게 되며 기본값 설정을 할수는 없다. 방법의 차이라서 상황에 따라 맞춰 사용하면 될듯 하다.1234@RequestMapping(\"/\")public String home(String id) &#123; return \"home\";&#125; @RequestBody@RequestBody어노테이션을 사용할 경우 반드시 POST형식으로 응답을 받는 구조여야만 한다. 이를테면 JSON 이나 XML같은 데이터를 적절한 messageConverter로 읽을때 사용하거나, POJO형태의 데이터 전체로 받을경우에 사용된다. 단, 이 어노테이션을 사용하여 파라미터를 받을 경우 별도의 추가 설정(POJO 의 get/set 이나 json/xml 등의 messageConverter 등)을 해줘야 적절하게 데이터를 받을수가 있다.1234@PostMapping(\"/\")public String home(@ReqeustBody Student student) &#123; return \"home\";&#125; @ModelAttribute@RequestParam과 비슷한데 1:1로 파라미터를 받을경우는 @RequestParam를 사용하고, 도메인이나 오브젝트로 파라미터를 받을 경우는 @ModelAttribute으로 받을수 있다. 또한 이 어노테이션을 사용하면 검증(Validation)작업을 추가로 할수 있는데 예로들어 null이라던지, 각 멤버변수마다 valid옵션을 줄수가 있고 여기서 에러가 날 경우 BindException 이 발생한다. Spring command 객체컨트롤러에서 파라미터로 받은 정보에 대해서는 view 에서 바로 사용이 가능하다. 예로 들어 아래그림처럼 이렇게 컨트롤러가 구성되어있고 이렇게 모델이 구성되어있을때 view 에서 이런식으로 구성되어있다고 가정해보자. 이때 /student?name=taetaetae&amp;age=32&amp;address=green-factory로 호출을 해보면 구지 Model에 값을 셋팅해주지 않아도 다음과 같이 정보를 읽을수 있게 된다. 마치며스프링에서 파라미터를 받는 방법은 상당히 다양하다. 이게 정답이다 정의할수 없을정도로. 상황에 따라 맞는 방법으로 파라미터를 받아야 하겠고, 각 방법에 장/단점을 최대한 살려서 좀더 깔끔한 코드를 작성할수 있어야 하겠다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"ReqeustParam","slug":"ReqeustParam","permalink":"https://taetaetae.github.io/tags/ReqeustParam/"},{"name":"RequestBody","slug":"RequestBody","permalink":"https://taetaetae.github.io/tags/RequestBody/"},{"name":"ModelAttribute","slug":"ModelAttribute","permalink":"https://taetaetae.github.io/tags/ModelAttribute/"},{"name":"spring command 객체","slug":"spring-command-객체","permalink":"https://taetaetae.github.io/tags/spring-command-객체/"}]},{"title":"벌써 3월, 다시 일어서야할 때","slug":"a-quarter-of-this-year","date":"2017-03-08T04:44:21.000Z","updated":"2020-04-23T04:41:36.679Z","comments":true,"path":"2017/03/08/a-quarter-of-this-year/","link":"","permalink":"https://taetaetae.github.io/2017/03/08/a-quarter-of-this-year/","excerpt":"벌써 3월이다. 뭐하나 제대로 한것도 없는데 시간은 야속하게도 멈추지 않고 지나가고 있다.오랜만에 동기형을 만났다.","text":"벌써 3월이다. 뭐하나 제대로 한것도 없는데 시간은 야속하게도 멈추지 않고 지나가고 있다.오랜만에 동기형을 만났다. 신입사원이 되기 전 연습생(?)시절 동거동락하며 개발에 대해 고군분투 하던 사이인지라. 오랜만에 만나도 서로 이야기 하고자 하는 주제는 언제나 동일하다. 개발자로서의 삶이런저런 이야기를 하며 다시 나를 돌아보게 되었다. 내 노력에 의해, 아니면 운이 좋아 지금 다니고있는 회사에 들어온 이후로 예전만큼의 열정은 온데간데 없으며, 그만큼 간절하지도 않고 치열하지도 않는 내 자신이 너무 미안하고 쪽팔릴정도로 한심하기 그지 없었다. 무엇때문일까, 도대체 왜 이렇게 안일해졌고 적극적이지 못하게 되었을까.그 질문에 대한 정답은 이것이다 라고 정의를 할수는 없겠지만 확실한건, 현재 내 상황에 안주하고 타협하려하는 마음가짐이 생겼다는건 회피할수 없을정도로 나도 정말 많이 변해버린것 같다. 물론 지금 상황이 잘못되었다는건 아니지만 내 직업 특성상 끊임없이 노력하고 도전하며 배워야 하는 상황인데 지금 난 퇴근하고 집에가면 쉬고싶고 자기 바쁘고 다음날 늦잠자고… 계속된 생활패턴에 젖어 사는것 같다.일단 독서좀 많이 해야겟다. 회사에, 집에 쌓인 책만 벌써 몇권인지… 기본이 되는 전공서적 하나 정하고 끝까지 완독해보자. 그게 자바든 스프링이든, 최신 신기술보다 기본이 탄탄해져야 하는건 백번 천번 말해도 당연하기에. 다시 정신차리자. 오늘 걷지 않으면 내일은 달려야 한다고 누군가 그랬듯..","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"github api 사용방법","slug":"github-api","date":"2017-03-02T02:18:05.000Z","updated":"2020-04-23T04:41:36.741Z","comments":true,"path":"2017/03/02/github-api/","link":"","permalink":"https://taetaetae.github.io/2017/03/02/github-api/","excerpt":"github 에서는 레파지토리의 전반적인 상황에 대해 다양한 API를 제공해주고 있다. 이번에는 그 API를 사용하는 방법에 대해 알아보고자 한다.","text":"github 에서는 레파지토리의 전반적인 상황에 대해 다양한 API를 제공해주고 있다. 이번에는 그 API를 사용하는 방법에 대해 알아보고자 한다. # Personal access tokens 발급우선 정상적인 API를 사용하기 위해 Personal access tokens를 발급받아야 한다. github 초기화면 &gt; 우측상단 프로필사진 클릭 &gt; setting &gt; Personal access tokens 에 들어가 토큰을 생성을 한다.해당 토큰의 허용범위를 설정한뒤 생성을 하면 만들어 지는데 여기서 발급되는 문자열은 따로 보관하는게 좋다. (나중에 다시 확인하려면 새로 재 생성하는 방법말고는 없기 때문에 한번 만들때 메모해 두는게 좋다.)아래와 같이 생성완료. # API 사용방법권한이 없는 Repository 의 내용을 확인할수 없듯이 github에서 제공하는 API또한 권한이 있는 Repository에 대해서만 API를 제공한다. 위에서 발급한 token 을 권한 체크할때 사용하는데 다양한 방법이 있을수 있겠으나 나는 간단하게 헤더에 포함시켜서 일반 GET 호출을 하는 방식으로 하였다. 윈도우 환경에서는 헤더 셋팅하고 호출하는게 조금 어려울수 있으니 이러한 부분을 설정할수 있는 Postman이라는 프로그램으로 호출을 해본다.아래처럼 url은 https://api.github.com/으로 설정하고 Headers파라미터에 Authorization라는 key에 value를 위에서 발급받았던 token을 이용하여 token abcd~~식으로 입력해준다음 send버튼을 눌러주면 응답을 받을수가 있는데, 아래 그림은 제공하는 api의 모든 url을 확인하는 방법이다.아래애서는 위에서 확인된 api url을 활용하여 내가 권한이 있는 레파지토리 내에서 확인할수있는 정보에 대한 API를 호출해보았다. API 호출시 가장 보편화되어있는(?) 스팩인 JSON으로 응답이 내려오기때문에 어떠한 환경에서도 충분히 활용할수 있을것이라 생각한다.나는 개인적으로 팀 내에서 하나의 Organizations내에 여러 Repository가 있는데 각각의 PullRequest에 대해 코드리뷰를 해야하는 상황에서 일일히 다 찾아보기 귀찮아 github-api를 활용해 open된 PullRequest가 있으면 알림을 주는 걸 만들어 보았다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"}]},{"title":"리눅스상에서 json 파싱","slug":"shell-script-json","date":"2017-02-28T08:50:44.000Z","updated":"2020-04-23T04:41:36.847Z","comments":true,"path":"2017/02/28/shell-script-json/","link":"","permalink":"https://taetaetae.github.io/2017/02/28/shell-script-json/","excerpt":"리눅스 상에서 json형태의 String 을 파싱해야하는 상황이라면 아래 라이브러리를 사용해보는것을 추천해본다.","text":"리눅스 상에서 json형태의 String 을 파싱해야하는 상황이라면 아래 라이브러리를 사용해보는것을 추천해본다. # jq사용방법은 너무너무 간단하다. 자신의 시스템에 맞는 라이브러리를 다운받고 1234(32-bit system)$ wget http://stedolan.github.io/jq/download/linux32/jq(64-bit system)$ wget http://stedolan.github.io/jq/download/linux64/jq 실행 권한을 설정해 준 뒤 1chmod +x ./jq root 권한으로 해당 파일을 이동시킨다. 1sudo cp jq /usr/bin 실행은 다음과 같이 한다.Json String 이 아래와 같이 있다고 가정했을때 12345678910111213141516171819202122232425&#123;\"name\": \"Google\",\"location\": &#123; \"street\": \"1600 Amphitheatre Parkway\", \"city\": \"Mountain View\", \"state\": \"California\", \"country\": \"US\" &#125;,\"employees\": [ &#123; \"name\": \"Michael\", \"division\": \"Engineering\" &#125;, &#123; \"name\": \"Laura\", \"division\": \"HR\" &#125;, &#123; \"name\": \"Elise\", \"division\": \"Marketing\" &#125; ]&#125; 실제 사용과 결과는 다음과 같이 이루어 진다.1234567891011121314$ cat json.txt | jq &apos;.name&apos;&quot;Google&quot;$ cat json.txt | jq &apos;.location.city&apos;&quot;Mountain View&quot;$ cat json.txt | jq &apos;.employees[0].name&apos;&quot;Michael&quot;$ cat json.txt | jq &apos;.location | &#123;street, city&#125;&apos;&#123; &quot;city&quot;: &quot;Mountain View&quot;, &quot;street&quot;: &quot;1600 Amphitheatre Parkway&quot;&#125; 보다 자세한 사용방법은 공식홈페이지( https://stedolan.github.io/jq/ )를 참조하면 좋을듯 하다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://taetaetae.github.io/tags/linux/"},{"name":"json","slug":"json","permalink":"https://taetaetae.github.io/tags/json/"},{"name":"jq","slug":"jq","permalink":"https://taetaetae.github.io/tags/jq/"}]},{"title":"eclipse에서 spring-boot로 web 만들기","slug":"spring-boot-eclipse","date":"2017-02-27T05:37:27.000Z","updated":"2020-04-23T04:41:36.862Z","comments":true,"path":"2017/02/27/spring-boot-eclipse/","link":"","permalink":"https://taetaetae.github.io/2017/02/27/spring-boot-eclipse/","excerpt":"Spring 환경에서 웹 어플리케이션을 만들어야 한다면 pom.xml 에 이런저런 설정들을 적어줘야 했다. 하지만 이런 수고(?)를 덜어줄수 있는 방법중에 한가지가 바로 Spring Boot로 만드는 방법인데, 이클립스 환경에서 만드는 법을 정리하고자 한다.","text":"Spring 환경에서 웹 어플리케이션을 만들어야 한다면 pom.xml 에 이런저런 설정들을 적어줘야 했다. 하지만 이런 수고(?)를 덜어줄수 있는 방법중에 한가지가 바로 Spring Boot로 만드는 방법인데, 이클립스 환경에서 만드는 법을 정리하고자 한다. # new &gt; Maven Project빈 Maven Project 를 만드는 방법은 아주 간단하니 생략하고… 만들게 되면 pom.xml 은 아래처럼 아주 깔끔한(?)상태로 만들어지게 된다.12345678&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;boot&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 그러면 이 비어있는 pom.xml 에 Spring-Boot 에 필요한 설정들을 추가해주기로 한다.12345678910111213141516171819202122&lt;parent&gt; &lt;!--boot의 스타터를 사용하겠다고 명시적으로 설정--&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;relativePath /&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;!--boot에서 스타터패키지로 제공해주는 것들중에 web 설정 부분 --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 그다음 임의의 java 클래스를 하나 만들고 거기에 아래처럼 설정하면 끝123456@SpringBootApplication // @Configuration + @EnableAutoConfiguration + @ComponentScan 들의 종합 어노테이션public class TestApplication&#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(TestApplication.class, args); &#125;&#125; Spring Boot 에서는 내장WAS를 가지고 있기 때문에 main 메소드에서 우클릭후 run AS → Spring Boot App 을 선택해주면 8080포트로 띄워지게 된다. # new &gt; Spring Stater project(STS가 설치되어있다는 가정하에)이 메뉴를 사용하면 위에서 했던 일련의 설정들을 자동으로 해주게 된다. 간단한 내용이니 next를 해주다 마지막에 Dependencies 설정하는 부분에서 Web 을 체크해주고 Finish 버튼을 누르면 끝 # Spring Initializr (start.spring.io)http://start.spring.io/ 에 들어가보면 구지 설명하지 않아도 친절하게 Generate 해주는 페이지가 보인다. 여기서 web 을 Dependencies에 추가하고 Generate를 하면 해당 프로젝트가 압축된 상태로 다운이 받아지게 되고 이를 IDE 에서 열어보면 위에서 했던 일련의 과정들이 설정되어 있는것을 확인해볼수가 있다. # 내장톰켓을 사용안하고 별도 톰켓을 사용해야 하는 경우Spring boot는 자체적으로 내장 WAS를 가지고 있다. 하지만 관리포인트나 이런저런 이유로 내장톰켓을 사용하지 못하는 환경이라면 다음과 같은 설정을 해주면 된다. 일반적으로 빌드가 되면 jar로 만들어 질텐데 war로 빌드 되도록 수정을 해야한다. (was가 WAR를 물고 떠야하기 때문..) 1&lt;packaging&gt;war&lt;/packaging&gt; dependency 에 tomcat을 추가해준다. 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 아래처럼 main 메소드가 있는 클래스에 SpringBootServletInitializer를 상속받게 한 후 configure메소드를 오버라이딩 해준다. 12345678910@SpringBootApplicationpublic class TestApplication extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(TestApplication.class); &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(TestApplication.class, args); &#125;&#125; 톰켓에 띄우기 위하여 프로젝트 설정(Project Facets)에서 Dynamic Web Module을 체크해준다. 참고 URL http://www.donnert.net/86 http://opennote46.tistory.com/124","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring-boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"eclipse","slug":"eclipse","permalink":"https://taetaetae.github.io/tags/eclipse/"}]},{"title":"lombok(롬복)소개 및 설치","slug":"lombok","date":"2017-02-22T08:48:42.000Z","updated":"2020-04-23T04:41:36.800Z","comments":true,"path":"2017/02/22/lombok/","link":"","permalink":"https://taetaetae.github.io/2017/02/22/lombok/","excerpt":"일반적으로 자바개발을 하다보면 Model 을 만들고 각 멤버변수를 접근할수 있는 (각 요소들이 private 접근권한을 가지고 있을때) method 를 만들게 된다. IDE에서 제공하는 아래처럼… (윈도우/이클립스 기준)","text":"일반적으로 자바개발을 하다보면 Model 을 만들고 각 멤버변수를 접근할수 있는 (각 요소들이 private 접근권한을 가지고 있을때) method 를 만들게 된다. IDE에서 제공하는 아래처럼… (윈도우/이클립스 기준) get/set 메소드 : Alt + Shift + S + R toString 메소드 : Alt + Shift + S + S 기타 등등…12345678910111213141516171819202122232425262728293031323334353637383940414243public class Student &#123; private int id; private String name; private int grade; private String department; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getGrade() &#123; return grade; &#125; public void setGrade(int grade) &#123; this.grade = grade; &#125; public String getDepartment() &#123; return department; &#125; public void setDepartment(String department) &#123; this.department = department; &#125; @Override public String toString() &#123; return \"Student [id=\" + id + \", name=\" + name + \", grade=\" + grade + \", department=\" + department + \"]\"; &#125; &#125; 이렇게 하는 방법도 있지만 어노테이션 설정으로 적용할수 있는 간단한 라이브러리를 소개하고자 한다.바로 lombok, 공식 홈페이지 : https://projectlombok.org설치 및 사용방법은 아주 간단하다. 공식 홈페이지에서 jar를 다운받고 실행, 아래처럼 이클립스 실행파일 경로를 설정해준다음에 인스톨을 누르면 된다.maven 환경에서 dependency를 가져오기 위해서는 당연히 추가설정을 해줘야 한다.12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.10&lt;/version&gt; &lt;!--버전은 그때 맞춰서--&gt;&lt;/dependency&gt; 실제로 코드상에서 사용방법은 다음과 같다. 정말 간단히, 어노테이션만 적용해주면 끝!123456789import lombok.Data;@Datapublic class Student &#123; private int id; private String name; private int grade; private String department;&#125; 그럼 이렇게 기본적인 method들이 생성된다.일반적으로 @Data를 사용하고 상황에 따라 필요한 어노테이션만 지정도 가능하다고 한다. @Getter and @Setter @NonNull @ToString @EqualsAndHashCode @Data @Cleanup @Synchronized @SneakyThrows참고 URL : http://jnb.ociweb.com/jnb/jnbJan2010.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"lombok","slug":"lombok","permalink":"https://taetaetae.github.io/tags/lombok/"}]},{"title":"logback 설정하기","slug":"logback","date":"2017-02-19T06:10:45.000Z","updated":"2020-04-23T04:41:36.797Z","comments":true,"path":"2017/02/19/logback/","link":"","permalink":"https://taetaetae.github.io/2017/02/19/logback/","excerpt":"자바 개발자라면 한번쯤은 들어봤고, 한번쯤은 사용했을법한 logger 로 log4j가 있을것이다. 하지만 최근들어 logback이라는것을 알게되었고, 왜 logback을 사용해야 하는 이유라는 글이 있을정도로 여러 측면에서 개선이 된듯 하다.","text":"자바 개발자라면 한번쯤은 들어봤고, 한번쯤은 사용했을법한 logger 로 log4j가 있을것이다. 하지만 최근들어 logback이라는것을 알게되었고, 왜 logback을 사용해야 하는 이유라는 글이 있을정도로 여러 측면에서 개선이 된듯 하다. (링크)이번에 작성할 글의 목적은 logback을 설정하고 어떻게 사용하는지에 대해 작성해 보고자 한다.※ 공식사이트 : https://logback.qos.ch/ # pom.xmlmaven구조라고 가정했을때 logback Dependency를 가져오기 위해서는 아래와 같이 pom.xml 에 설정해 주면 된다.12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.7&lt;/version&gt; &lt;!--버전은 상황에 따라 --&gt;&lt;/dependency&gt; # 로그레벨ERROR, WARN, INFO, DEBUG or TRACE # logback 설정파일일반적으로 logback.xml 이라는 이름으로 만들어 src/main/resources/아래에 위치하게 된다. Spring-Boot 환경에서는 logback-spring.xml 이라는 이름으로 설정해야 하는데 logback.xml로 설정하면 스프링부트가 설정하기 전에 로그백 관련한 설정을 하기 때문에 제어할 수가 없게 된다.( 공식사이트 메뉴얼 : https://logback.qos.ch/documentation.html )12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;include resource=\"org/springframework/boot/logging/logback/defaults.xml\" /&gt; &lt;include resource=\"org/springframework/boot/logging/logback/console-appender.xml\" /&gt; &lt;!-- 변수 지정 --&gt; &lt;property name=\"LOG_DIR\" value=\"/logs\" /&gt; &lt;property name=\"LOG_PATH_NAME\" value=\"$&#123;LOG_DIR&#125;/data.log\" /&gt; &lt;!-- FILE Appender --&gt; &lt;appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;LOG_PATH_NAME&#125;&lt;/file&gt; &lt;!-- 일자별로 로그파일 적용하기 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH_NAME&#125;.%d&#123;yyyyMMdd&#125;&lt;/fileNamePattern&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;!-- 일자별 백업파일의 보관기간 --&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%F]%M\\(%L\\) : %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;layout class=\"ch.qos.logback.classic.PatternLayout\"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%F]%M\\(%L\\) : %m%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- TRACE &gt; DEBUG &gt; INFO &gt; WARN &gt; ERROR, 대소문자 구분 안함 --&gt; &lt;!-- profile 을 읽어서 appender 을 설정할수 있다.(phase별 파일을 안만들어도 되는 좋은 기능) --&gt; &lt;springProfile name=\"local\"&gt; &lt;root level=\"DEBUG\"&gt; &lt;appender-ref ref=\"FILE\" /&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;springProfile name=\"real\"&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"FILE\" /&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt; &lt;/springProfile&gt;&lt;/configuration&gt; # java 코딩에서의 로깅실제 사용은 다음과 같이 LoggerFactory를 이용해서 사용하거나 Lombok어노테이션을 활용하면 심플하게 사용이 가능하다. LoggerFactory 사용 12345678910import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class Foo &#123; static final Logger logger = LoggerFactory.getLogger(Foo.class); public void test() &#123; logger.debug(\"ID : &#123;&#125;\", \"foo\"); &#125;&#125; Lombok 어노테이션 사용 123456789import lombok.extern.slf4j.Slf4j;@Slf4jpublic class Foo &#123; public void test() &#123; log.debug(\"ID : &#123;&#125;\", \"foo\"); &#125;&#125; # 마치며일반적인 웹 어플리케이션에서는 WAS에서 로깅을 따로 관리하고 있기 때문에 file 로 로깅을 할 필요는 없을것 같다.(일반 jar 형태에서는 파일 로깅이 필요 할수도…) # 참고사이트 http://yookeun.github.io/java/2015/11/10/log4jtologback/ http://java.ihoney.pe.kr/397 https://logback.qos.ch/","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"logback","slug":"logback","permalink":"https://taetaetae.github.io/tags/logback/"}]},{"title":"자바 8 Date","slug":"java8-date","date":"2017-01-10T11:55:33.000Z","updated":"2020-04-23T04:41:36.771Z","comments":true,"path":"2017/01/10/java8-date/","link":"","permalink":"https://taetaetae.github.io/2017/01/10/java8-date/","excerpt":"이제까지 내 기억으로는 Date 관련 클래스를 아래처럼 점차 바꿔써온걸로 기억이 난다.java.util.Date &gt; java.util.Calendar &gt; org.joda.time그런데 java 8 버전에서 기존에 있었던 문제들을 개선해서 나왔다고 한다. (네이버 HellowWorld 포스팅 참고) JSR-310 이라는 표준명세로.","text":"이제까지 내 기억으로는 Date 관련 클래스를 아래처럼 점차 바꿔써온걸로 기억이 난다.java.util.Date &gt; java.util.Calendar &gt; org.joda.time그런데 java 8 버전에서 기존에 있었던 문제들을 개선해서 나왔다고 한다. (네이버 HellowWorld 포스팅 참고) JSR-310 이라는 표준명세로.지금부터는 JAVA 8 에서 제공하는 API로 날짜 연산을 어떻게 하는지에 대해 알아보고자 한다. (물론 수많은 날짜 연산 방법을이 있지만 자주 쓰이는 부분들 위주로 정리해보자.) Date &gt; String (format) 1LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); String &gt; Date (format) 1LocalDateTime.parse(\"2017-01-01 12:30:00\", DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); 날짜/시간 증감 1234567LocalDateTime localDateTime = LocalDateTime.of(2017, 1, 1, 10, 0, 0);localDateTime.plusDays(1); // 일localDateTime.plusMonths(1); // 월localDateTime.plusHours(1); // 시간localDateTime.plusWeeks(1); // 주localDateTime.minusYears(1); // 년localDateTime.minusMinutes(1); // 분 더 다양한 내용들은 아래 URL 에서 확인이 가능하다.https://docs.oracle.com/javase/tutorial/datetime/iso/overview.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"date","slug":"date","permalink":"https://taetaetae.github.io/tags/date/"}]},{"title":"Spring Transactional 설정 및 주요속성","slug":"transactional-setting-and-property","date":"2017-01-08T08:19:30.000Z","updated":"2020-04-23T04:41:36.891Z","comments":true,"path":"2017/01/08/transactional-setting-and-property/","link":"","permalink":"https://taetaetae.github.io/2017/01/08/transactional-setting-and-property/","excerpt":"지난번에는 트랜잭션의 설정값에 대해 알아본 바 있다. [ Spring Transaction 옵션 ]이번 포스팅에서는 실제로 스프링 환경에서 어떤식으로 설정해야 @Transactional 어노테이션을 사용할수 있는지, 그리고 어떤 속성들이 있는지에 대해 알아보고자 한다.","text":"지난번에는 트랜잭션의 설정값에 대해 알아본 바 있다. [ Spring Transaction 옵션 ]이번 포스팅에서는 실제로 스프링 환경에서 어떤식으로 설정해야 @Transactional 어노테이션을 사용할수 있는지, 그리고 어떤 속성들이 있는지에 대해 알아보고자 한다. # 설정기존 xml방식에서는 다음과 같이 설정을 한다.1234&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\" proxy-target-class=\"true\"/&gt; 혹, JavaConfig 방식으로 설정하기 위해서는 다음과 같이 설정한다.123456789@EnableTransactionManagementpublic class AppConfig &#123; ... @Bean public PlatformTransactionManager transactionManager() throws URISyntaxException, GeneralSecurityException, ParseException, IOException &#123; return new DataSourceTransactionManager(dataSource()); &#125;&#125; 위와같이 설정을 해주면 트랜잭션을 설정하고자 하는 곳 어디서든 @Transactional 어노테이션을 지정해서 적용이 가능하다.1234567public class UserService&#123; @Transactional public boolean insertUser(User user)&#123; ... &#125;&#125; # 주요속성@Transactional 어노테이션의 주요속성은 다음과 같다. 속성 설 명 사용 예 isolation Transaction의 isolation Level. 별도로 정의하지 않으면 DB의 Isolation Level을 따름. @Transactional(isolation=Isolation.DEFAULT) propagation 트랜잭션 전파규칙을 정의 , Default=REQURIED @Transactional(propagation=Propagation.REQUIRED) readOnly 해당 Transaction을 읽기 전용 모드로 처리 (Default = false) @Transactional(readOnly = true) rollbackFor 정의된 Exception에 대해서는 rollback을 수행 @Transactional(rollbackFor=Exception.class) noRollbackFor 정의된 Exception에 대해서는 rollback을 수행하지 않음. @Transactional(noRollbackFor=Exception.class) timeout 지정한 시간 내에 해당 메소드 수행이 완료되지 않은 경우 rollback 수행. -1일 경우 no timeout (Default = -1) @Transactional(timeout=10) # 마치며자칫 잘못했다가는 원치않는 트랜잭션으로 잘못된 결과를 초래할수 있기때문에 기본값은 숙지하는게 좋을것 같다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"Transaction","slug":"Transaction","permalink":"https://taetaetae.github.io/tags/Transaction/"},{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"}]},{"title":"spring4에서 json view 활용하기(with @ResponseBody)","slug":"spring4-json","date":"2017-01-07T06:47:59.000Z","updated":"2020-04-23T04:41:36.858Z","comments":true,"path":"2017/01/07/spring4-json/","link":"","permalink":"https://taetaetae.github.io/2017/01/07/spring4-json/","excerpt":"수많은 블로거분들의 도움을 받고자 구글링을 해서 적용을 해봤지만 너무많은 삽질을 했다.(해봤던 방식은 jsonViewResolver 를 따로 설정해보거나, @RequestMapping 옵션을 바꿔보는 수준..) 특히나 Spring설정방식이 예전 방식이였던 xml이 아닌 javaconfig였기 때문에 더욱더 자료가 없었고..","text":"수많은 블로거분들의 도움을 받고자 구글링을 해서 적용을 해봤지만 너무많은 삽질을 했다.(해봤던 방식은 jsonViewResolver 를 따로 설정해보거나, @RequestMapping 옵션을 바꿔보는 수준..) 특히나 Spring설정방식이 예전 방식이였던 xml이 아닌 javaconfig였기 때문에 더욱더 자료가 없었고.. 한참을 삽질하다 해결을 하여 포스팅하게 된다. 우선 환경은 spring 4.3.4.RELEASE, Maven, jdk8임을 밝힌다. # pom.xmljackson-mapper-asl을 이용해서 하라는 블로거들도 있었지만, 아무리해도(뭔가 Spring버전과 맞지 않는듯 했다.) 잘 안되어 아래와 같은 dependency를 주었다.12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;/dependency&gt; # Controller아래와같이 @ResponseBody 어노테이션을 설정해주고 리턴은 해당 모델을 넘기면 된다.12345678@RequestMapping(value=\"/test\")@ResponseBodypublic Map&lt;String, Object&gt; test()&#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(\"1\", \"111\"); map.put(\"2\", 222); return map; &#125; 그리고 호출을 해보면 기대했던것처럼 이쁘게 json형태로 나온다.12345&#123;\"1\": \"111\",\"2\": 222&#125;물론, list 나 array, 일반 객체도 가능하다. # 정리삽질을 끝에 알게된 사실(?)을 정리해보자.다른측면에서 분석을 해보면. @ResponseBody을 이용하여 view 에 json 형태로 나타내고자 할 경우 가능한 상황은 toString으로 했을때 json형태로 나올수 있으면 가능하다. 예로들어 아래처럼 클래스에 Lombok 어노테이션인 @Data가 붙으면 자동으로 toString을 오버라이딩 해주기 때문에 해당 클래스를 리턴하게되면 자동으로 json 처리가 된다. 123456@Datapublic Student&#123; private String id; private String name; ...&#125; @ResponseBody을 붙이고 List&lt;Student&gt;를 리턴하게 되면 에러가 나는데, 이럴경우 별도 라이브러리를 추가해줘야 자동으로 변환되어 json 형태로 나올수 있게 된다. (list.toString을 하면 json형태가 아닌 이상한 문자형태로 나오기 때문… Map같은것도 마찬가지 이유로 별도 라이브러리를 추가해줘야 정상적으로 나온다.) # 마치며단순히 @ResponseBody를 사용해서 json으로 리턴하려면 어떤 라이브러리를 추가해야한다 로 생각했던것에서, 이것저것 테스트 한 결과 toString을 할수 있어야 하고 그 값이 json형태이면 가능하다 로 결론이 지어졌다. 확실히 장님 코끼리 만지듯이 ‘그런가보다’하고 넘어가면 삽질이 진짜 불필요한 삽질이 되는것 같다. 구글링을 해보고, 테스트를 해봐서, 결론적으로 내것으로 만드는 습관을 가져야 겠다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"json","slug":"json","permalink":"https://taetaetae.github.io/tags/json/"}]},{"title":"jsp include","slug":"20170104","date":"2017-01-04T09:36:17.000Z","updated":"2020-04-23T04:41:36.633Z","comments":true,"path":"2017/01/04/20170104/","link":"","permalink":"https://taetaetae.github.io/2017/01/04/20170104/","excerpt":"1. 디렉티브방식1&lt;%@ include file=\"~~\"%&gt; 정적 include 방식, 인클루드 되는 내용이 단순하게 텍스트로 포함되어 컴파일이 된다. (복사된다는 느낌)","text":"1. 디렉티브방식1&lt;%@ include file=\"~~\"%&gt; 정적 include 방식, 인클루드 되는 내용이 단순하게 텍스트로 포함되어 컴파일이 된다. (복사된다는 느낌) 주의할점은 비록 포함되는 페이지라 하더라도 한글을 제대로 처리하기 위해서는 포함되어지는 jsp파일 상단에 인코딩 명시를 해줘야 한다. 포함되어지는 jsp 내용이 변경이 될 경우 해당 jsp를 사용하는 jsp를 강제로 변경(touch) 해줘서 다시 컴파일이 되게 해야하는 불편함이 있다. 정적 방식이기 때문에 예로들어 전역변수를 인클루드 되는 jsp에서 지정하게 되면 상위jsp에서 사용이 가능하게 된다. 2. 액션태그 방식1&lt;jsp:include page=\"~~\"/&gt; 동적 include 방식, 포함하는 문서와 상관없이 동적으로 컴파일 된다. (완전히 별도로 동작하기 때문에 변수를 동시에 사용하려면 따로 파라미터로 넘겨줘야 한다.) flush 옵션은 요청흐름이 넘어가면서 현재까지 페이지의 결과를 출력할 것인지 말것인지를 결정하는것이다. 일반적으로 false로 설정한다. &lt;jsp:param&gt;를 이용하여 파라미터를 전송할수 있다.1234&lt;jsp:include page=\"...\" flush=\"false\"&gt; &lt;jsp:param name=\"name\" value=\"이름\" /&gt; &lt;jsp:param name=\"pageName\" value=\"페이지이름\"/&gt;&lt;/jsp:include&gt; 3. JSTL 방식1&lt;c:import url=\"~~\" /&gt; JSTL(JSP Standard Tag Library) 태그중의 하나 컴파일 되고 동작하는 방식은 액션태그&lt;jsp:include page=&quot;~~&quot;/&gt;와 같음 현재 컨테이너 안에 있는 자원외에 다른 외부 자원도 포함이 가능하다. 1&lt;c:import url=\"http://www.google.com/\"/&gt; 아래와 같이 보다 더 다양한 옵션이 제공된다12345&lt;c:import! url=&quot;읽어올 URL&quot; var=&quot;읽어올 데이터를 저장할 변수명&quot; scope=&quot;변수의 공유 범위&quot; varReader=&quot;리소스의 내용을 Reader 객체로 읽어올 때 사용&quot; charEncoding=&quot;읽어온 데이터의 캐릭터셋 지정&quot; /&gt;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jsp","slug":"jsp","permalink":"https://taetaetae.github.io/tags/jsp/"}]},{"title":"2017 버킷리스트","slug":"hello-2017","date":"2017-01-01T06:49:31.000Z","updated":"2020-04-23T04:41:36.756Z","comments":true,"path":"2017/01/01/hello-2017/","link":"","permalink":"https://taetaetae.github.io/2017/01/01/hello-2017/","excerpt":"올해도 어김없이(?) 1월 1일이 되어 해맞이(해돋인지 해맞인지 햇갈리지만, 새해 첫날부터 복잡해지기 싫당 =ㅁ=)를 다녀오고 까페에서 새해 계획을 세워본다. 정말 지킬수 있는 계획들, 현실적인 부분들만 고려해서 써내려 가보자. 절반 이상만이라도 지킬수만 있다면 그나마 다행이라고 생각!","text":"올해도 어김없이(?) 1월 1일이 되어 해맞이(해돋인지 해맞인지 햇갈리지만, 새해 첫날부터 복잡해지기 싫당 =ㅁ=)를 다녀오고 까페에서 새해 계획을 세워본다. 정말 지킬수 있는 계획들, 현실적인 부분들만 고려해서 써내려 가보자. 절반 이상만이라도 지킬수만 있다면 그나마 다행이라고 생각! 기술블로그 운영하기 : 월 2회 posting 내가 아는지식이 얼만큼인지, 보여주기식이 아닌 내 머릿속에 자리잡고 있는 부분들을 정리해서 기록화 하는 이름하야 기술블로그를 작성하는거다. 2주에 최소 하나씩, 이렇게 되면 한달에 최소 2post, 1년이면 약 20post. 작다고 해도 마냥 작게만 느껴지지 않을 분량이다. 사소한거 하나라도. 이를테면 서버 설치나 스프링의 기본 설정 관련된 것들도. 글쓰는 연습도 하고 좋은 기회가 될것 같다. 4대강 종주 : 영산강, 1박2일코스 2014년에 한강(북한강, 남한강), 2015년에 금강, 2016년에는 못갔다. 4대강 종주의 목표가 갑자기 시들어진 작년이라 생각이 든다. 우여곡절 산전수전 다겪은 내 자전거 붕붕이에게 미안하지 않기위해서라도 올해 여름에는 꼭 영산강이나 낙동강 하나를 계획잡아 1박2일 코스로 다녀와야겠다. 음, 대략 5월? 아마 영산강을 가게될것같다. 이번에는 무리하지 않고 1박2일코스로.. 독서 : 월 전공1권, 전공외1권 작년에 내 입에서 나왔던 이야기들중에 한심스럽게(?)나온 멘트중 가장 많이나왔던 책좀읽자 올해에는 정말 많이는 아닐지라도 자주읽는 습관을 길러야겠다. 한달에 전공책 한권, 기타서적 한권. 얇은책+읽고싶은책 부터 읽기 시작해서 내년 1월1일때는 내 책상 한켠에 자리잡고있는 책장을 가득 메워보고싶다. 아, 물론 다 읽은 책들로만. 여행+사진 : 한달에 한번이상 여행가기 해외든 국내든, 올해는 정말 많이 다녀와야겠다. 가볍게 당일치기부터 시작해서 갈수만 있다면 해외여행도. 물론 올해도 야근과의 싸움은 계속될테지만 주말 잠깐이라도 시간을 내서 두달에?아니 한달에 한번이라도 휴가를 써서라도 가까운곳에 힐링하러 다녀오고 싶다. 가서 작년에 산 카메라로 사진도 이것저것 많이 찍고 좋은추억 많이 만들고오고 싶다. 저축+a : 근검절약의 생활화, 경제공부 나름 월급의 60%이상을 저축하는 중이다. 그치만 상황이 상황인지라 지금도 만족하지 못한다. 천장에 굴비 달아놓고 간장찍어 먹는다는게 아니라 아낄수 있는 부분들은 최대한 아끼면서 살자는거다. 아침에 택시 타지말고 조금 일찍 일어나서 버스를 탄다던지, 버스를 타지말고 조금일찍 일어나서 자전거를 탄다던지(사실 자전거를 타면 퇴근할때 더 빠르고 편하게 올수 있으니) 생활속에서 절약할수 있는 부분들을 찾고, 몸에 베도록 습관화 시켜야겠다. 그리고 주식이나 펀드 등 투자에 대해서도 이제 공부를 해봐야겠다. (독서하자는거랑 비슷한 이야기) 저금리시대 마냥 저축만 하다보면 힘든건 누구나 다 아는이야기. 일에 치여 생활에 치여 핑계대지말고 배워가면서 챙겨보자. 운동 : 자유형마스터, 몸짱 항상 하는 이야기지만 건강보다 중요한건 없다고 생각한다. 올해에도 병원가지 않는 나를 만들기 위해 헬스 + 배드민턴 + 라이딩 은 필수고 가능하면 수영도 배워서 자유형 정도는 할수있는 나를 만들고 싶다. 그래서 다들 말하는 몸짱도 되보고 싶고 자신있게 해변가에서 상의를 탈의할수있는(?!) 건강한 내가 되도록 노력해야겠다. 봉사활동하기 : 연탄배달, 자원봉사 작년에 하려다가 못한 봉사활동 올해는 꼭 해야겠다. 가깝게 할수있을법한게 연탄배달, 이건 1월달 내로 꼭! 해서 봉사라는것과 나눔이라는 행복을 느껴보고 싶다. 지금 생각나는건 자원봉사 같은것도 해보고 싶고 무보수 알바(?) 같은것도 해보고싶다. 나이들면 못할, 언제 해보겠나. 가까운, 먼 사람들 만나기 마지막으로, 잊고있었던 중요한 행동. 바로 사람들 만나기다. 바쁘다는 핑계 하나만으로 등한시한 내 소중한 사람들. 아무리 연봉을 많이 받고 일을 잘한다고 회사에서 잘나간다 할지라도, 내 곁엔 나를 생각해주는 소중한 사람들이 있기에 내가 있을수 있는것 같다. 가까운 사람들부터 시작해서 오랬동안 못봤던 사람들도 하나둘씩 연락하면서 지내는 여유를 가져야 겠다. 할수 있을까? 라는 생각보다 하나둘씩 잊지말고, 놓치지 말고 하루를, 이번주를, 이번달을 점검하고 실천해 나가보자.올 한해도 열심히 최선을 다해 살것!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"2016 회고","slug":"adieu-2016","date":"2016-12-31T07:59:43.000Z","updated":"2020-04-23T04:41:36.644Z","comments":true,"path":"2016/12/31/adieu-2016/","link":"","permalink":"https://taetaetae.github.io/2016/12/31/adieu-2016/","excerpt":"2016년, 내겐 정말 수많은 일들이 있었고 그 어느때보다 (전역 후로) 미친듯이 회사에 집중했던 시간들로 기억난다. 무작정 다가오는 새해를 맞이하는것도 좋지만 올 한해를 되돌아보는 시간을 갖고, 나를 다시 점검해보는 차원에서 일명 ‘회고’를 해볼까 한다.","text":"2016년, 내겐 정말 수많은 일들이 있었고 그 어느때보다 (전역 후로) 미친듯이 회사에 집중했던 시간들로 기억난다. 무작정 다가오는 새해를 맞이하는것도 좋지만 올 한해를 되돌아보는 시간을 갖고, 나를 다시 점검해보는 차원에서 일명 ‘회고’를 해볼까 한다. # 회사정말 열심히 했다. 잘했는지는… 잘 모르겠다. 난 잘한것 같다. 물론 내 하루중에 가장많은 시간을 쏟은것도 있지만 작년에 많이 하지 못하던것을 ‘날씨’라는 서비스를 홀로 맡으면서 정말 많은것을 배우고 결과물도 후회하지 않을만큼 나온것 같다. 지나고보면 구지 하지 않아도 월급은 똑같이 나올테고, 시키지도 않았는데 그시간에 잠을 더 잤으면 하는 생각도 들지만 후회하지 않는다. 아무도 없는 사무실 나혼자 12시넘어서 퇴근을 해도 즐거웠으니까, 그거면 됬다.모바일 개편이라는 큰 업무를 무사히(?) 해쳐내고는 사내에서 조직(서비스)을 변경할수 있는 기회가 되어 나홀로 지원, 다행스럽게도 합격을 해서 지금은 네이버페이 와 관련된 일을 하고있는 중이다. 기존 서비스운영을 하면서 느끼지 못했던, 초기 설계부터 시작하여 어떤 기술스택을 쓸것인가에 대한 선택부터 다양한 시행착오를 통해 이제 한 두달 되었는데 정말 많이 배우고 있다. 너무 힘들지만 너무 행복하다.돌이켜보면 작심삼일로 개발 관련된 공부를 등한시 한게 너무 후회가 된다. 바쁘다는 이유하나만으로 (솔직히 바쁘다는건 핑계다) 기능구현에만 신경을 써왔는데, 내년부터는 할수만 있다면 업무 외적으로 나만의 개발트리를 세워보고 싶다. # 건강일주일에 한번 이상 오전엔 배드민턴, 저녁엔 헬스장엘 가려고 노력했다. 그 결과였을까, 사랑니 뺀거 말고는 병원을 단한번도 안갔다. 감기조차 걸리지 않는게 다행이라고 생각하지만, 요즘들어(야근이 많아져서인지) 책상에 앉아있는 자세가 불량해서 거북목이 되가고 있다. 폼롤러도 구비했고 어깨 펴지라고 밴드도 구입해서 사용은 하는데 잘 실천이 안되는 중이다.작년에 자전거를 잃어버리다 되찾고는 자전거를 등한시 하게 되는것 같다. 이또한 핑계겠지. 내년엔 꼭 4대강중 하나 잡고 종주한번 해야겟다. 기필코.. 아맞다 수영도. ㅠㅠ 물에 뜨질 않으니 큰일이다… # 사람관계학교선후배동기 및 동아리 사람들, 군대 동기들 및 소대원들 과 선임 장교분들, 기타 등등… 올해 들어서인지. 연락에 너무 무색할만큼 잊고 살았던것 같다. 지나고보면 다른곳에 신경쓴다고 연락을 못했다고 핑계를 대고 있는 나이지만, 또 한편으로는 그 연락 10분 시간이 없다는건 … 역시나 핑계다. 나를 도와주고 나를 믿어주고 나를 생각해주는 사람들을 조금이라도 더 신경써서 연락하고 찾아 뵙는 시간을 내년부터서라도 가져야겠다. # 마치며일단 첫번째로 내년부터 할일은, 기술 블로그를 운영하는것이다. 솔직히 두달전 이 gitHub 를 이용해서 블로그를 만들긴 했지만 그닥 포스팅도 못했고 방치 수준이였으니… 적어도 한달에 한두개 정도는 포스팅 해보려고 노력해야겟다. 글쓰는게 힘들고 시간이 많이 들어가는 작업이지만, 돌이켜 생각하면 다 내 자산이고 나를 다시 바라볼수 있는 기회니까. 꼭 기술블로그만이 아닌, 하루를 기록하는 무언가를 해야겠다. 막상 한해를 돌이켜보니 그때는 뭐했는지 기억도 안나네..두번째로는 지킬수 있는 계획을 잡는것이다. 올 한해 목표중에 이룬건 10개중에 단 두개… (그중에 노래대회나가기, 스쿠버다이빙 하기, 자유형 마스터하기도 있다;;) 부끄럽다.. 2016년, 나라도 뒤숭숭 하고 정신없던 한해였지만 나름 의미있던 시간들을 보낸것 같아 다행이라 생각한다.음,. 10점만점에 8점??2017년! 다시한번 일어서자! 화이팅!!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"Spring Transaction 옵션","slug":"20161008","date":"2016-10-08T09:04:19.000Z","updated":"2020-04-23T04:41:36.630Z","comments":true,"path":"2016/10/08/20161008/","link":"","permalink":"https://taetaetae.github.io/2016/10/08/20161008/","excerpt":"상황스프링 환경에서 일반적으로 DAO 나 BO 레벨에서 다음과 같이 코딩을 하게 된다.","text":"상황스프링 환경에서 일반적으로 DAO 나 BO 레벨에서 다음과 같이 코딩을 하게 된다.1234@Transactional(isolation = Isolation.READ_COMMITTED, propagation = Propagation.REQUIRED, rollbackFor = Exception.class)public int method(int i) throws Exception &#123; return sqlMapClient.delete(\"~~~~\");&#125; 무분별하게 Ctrl+C,V 신공으로 트랜잭션 어노테이션을 가져다가 사용할수도 있겠으나, 각 값들이 어떤 역활을 아는지에 대해 알고 넘어갈 필요성이 있다. @Transactional우선 해당 어노테이션을 적용하면 적용된 클래스 또는 메소드에 트랜잭션이 적용된다. 따라서 로직 흐름에 맞추어 전체적으로 트랜잭션을 적용할것인지, 아니면 특정 메소드에 적용할것인지 전략을 잘 세워야 한다. isolation격리수준이라는 옵션이다. 트랜잭션에서 일관성이 없는 데이터를 허용하도록 하는 수준을 말하는데 옵션은 다음과 같다. READ_UNCOMMITTED (level 0) 트랜잭션에 처리중인 혹은 아직 커밋되지 않은 데이터를 다른 트랜잭션이 읽는 것을 허용 어떤 사용자가 A라는 데이터를 B라는 데이터로 변경하는 동안 다른 사용자는 B라는 아직 완료되지 않은(Uncommitted 혹은 Dirty) 데이터 B를 읽을 수 있다. Dirty read위와 같이 다른 트랜잭션에서 처리하는 작업이 완료되지 않았는데도 다른 트랜잭션에서 볼 수 있는 현상을 dirty read 라고 하며, READ UNCOMMITTED 격리수준에서만 일어나는 현상 READ_COMMITTED (level 1) dirty read 방지 : 트랜잭션이 커밋되어 확정된 데이터만을 읽는 것을 허용 어떠한 사용자가 A라는 데이터를 B라는 데이터로 변경하는 동안 다른 사용자는 해당 데이터에 접근할 수 없다. REPEATABLE_READ (level 2) 트랜잭션이 완료될 때까지 SELECT 문장이 사용하는 모든 데이터에 shared lock이 걸리므로 다른 사용자는 그 영역에 해당되는 데이터에 대한 수정이 불가능하다. 선행 트랜잭션이 읽은 데이터는 트랜잭션이 종료될 때까지 후행 트랜잭션이 갱신하거나 삭제하는 것을 불허함으로써 같은 데이터를 두 번 쿼리했을 때 일관성 있는 결과를 리턴함 SERIALIZABLE (level 3) 완벽한 읽기 일관성 모드를 제공 데이터의 일관성 및 동시성을 위해 MVCC(Multi Version Concurrency Control)을 사용하지 않음(MVCC는 다중 사용자 데이터베이스 성능을 위한 기술로 데이터 조회 시 LOCK을 사용하지 않고 데이터의 버전을 관리해 데이터의 일관성 및 동시성을 높이는 기술) 트랜잭션이 완료될 때까지 SELECT 문장이 사용하는 모든 데이터에 shared lock이 걸리므로 다른 사용자는 그 영역에 해당되는 데이터에 대한 수정 및 입력이 불가능하다. propagation ( 전파옵션) REQUIRED : 부모 트랜잭션 내에서 실행하며 부모 트랜잭션이 없을 경우 새로운 트랜잭션을 생성 REQUIRES_NEW : 부모 트랜잭션을 무시하고 무조건 새로운 트랜잭션이 생성 SUPPORT : 부모 트랜잭션 내에서 실행하며 부모 트랜잭션이 없을 경우 nontransactionally로 실행 MANDATORY : 부모 트랜잭션 내에서 실행되며 부모 트랜잭션이 없을 경우 예외가 발생 NOT_SUPPORT : nontransactionally로 실행하며 부모 트랜잭션 내에서 실행될 경우 일시 정지 NEVER : nontransactionally로 실행되며 부모 트랜잭션이 존재한다면 예외가 발생 NESTED : 해당 메서드가 부모 트랜잭션에서 진행될 경우 별개로 커밋되거나 롤백될 수 있음. 둘러싼 트랜잭션이 없을 경우 REQUIRED와 동일하게 작동 no-rollback-for - 예외처리 (기본값 : 없음)특정 예외가 발생하더라도 롤백되지 않도록 설정 스프링 배치에서의 트랜잭션 (내가 당했던(?) 문제)스프링 배치에서는 Tasklet 에서 기본적으로 step 단위 트랜잭션을 지원하고 있다고 한다.기본적으로 job이 하나의 tasklet 의 step 으로 실행하다보니 명시적이진 않지만 내부적으로 전체 트랜잭션으로 걸려있게 된다. 나같은 job 내 DAO delete 메소드에서 @Transactional 설정을 하고 그 DAO 메소드를 반복문에 의해 delete 하는 로직을 수행하는 부분이 있었는데 부모의 트랜잭션(tasklet에서 설정된 트랜잭션)으로 인해 dao 를 몇번 호출하던 job단위로 트렌젝션이 걸리게 되었다. (결국 트랜잭션은 반복문이 다 끝나야 적용이 된다는점.)그러다보니 가끔 DB Query Lock이 걸렸는데 DB레벨에서 undolog를 남기는게 너무 무거워져 lock이 발생 따라서 전파옵션을 수정해서 해당 문제를 해결하였다.1234567891011# 기존begin delete &lt; for 반복문commit# 전파옵션 수정 (기존 REQUIRES 에서 REQUIRES_NEW 으로 수정)for begin delete commitfor end","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"Transaction","slug":"Transaction","permalink":"https://taetaetae.github.io/tags/Transaction/"}]},{"title":"디자인패턴-싱글톤","slug":"20161006","date":"2016-10-06T08:03:48.000Z","updated":"2020-04-23T04:41:36.626Z","comments":true,"path":"2016/10/06/20161006/","link":"","permalink":"https://taetaetae.github.io/2016/10/06/20161006/","excerpt":"디자인 패턴중에 가장 잘 알려진 싱글톤 에 대해서 알아보고자 한다. 멀티 스레드 환경에서 자주 이용되는 패턴이라고만 들었는데 이번 기회를 통해 제대로 정리해보자","text":"디자인 패턴중에 가장 잘 알려진 싱글톤 에 대해서 알아보고자 한다. 멀티 스레드 환경에서 자주 이용되는 패턴이라고만 들었는데 이번 기회를 통해 제대로 정리해보자 싱글톤이 무엇인가 싱글톤(Singleton)은 정확히 하나의 인스턴스만 생성되는 클래스이다. 라고 이펙티브 자바에서 정의되어있다. 즉, 딱 하나만 생성하고 이를 여기저기서 사용하는 패턴이라 생각하면 될듯 하다. 싱글 스레드 환경에서는 당연히 인스턴스를 공유할 상황이 없겠지만 대부분 멀티 스레드 환경이기 때문에 싱글톤 패턴은 아주 중요한 부분이다. 0. 아주 고전적인 방법 (위험한 방법)123456789101112public class Singleton &#123; private static Singleton uniqueInstance; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if (uniqueInstance == null)&#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125; &#125; 위와 같은 상황에서 if절을 도달하는 시점이 각 스레드마다 다를경우 문제가 발생할 수 있다.(교묘한 시점에 객체가 1개 이상 반환될 여지가 있음) 이를 해결하기 위해서는 다음과 같이 getInstance()를 동기화 해주면 된다. 하지만 불필요하게 동기화 하는 오버헤드만 증가하게 된다.123456789101112public class Singleton &#123; private static Singleton uniqueInstance; private Singleton()&#123;&#125; public static synchronized Singleton getInstance()&#123; if (uniqueInstance == null)&#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 1. public static 인스턴스로 생성12345public static final LocalCache sharedObject = new LocalCache();private LocalCache()&#123;&#125; 이코드는 간단하다는 장점이 있는 반면에 유연하지 못한 부분이 있다. (아래 이어서 설명) 2. private static final 인스턴스로 생성12345678910private static final LocalCache sharedObject = new LocalCache(); private LocalCache() &#123; &#125; public static LocalCache getInstance() &#123; return sharedObject ; &#125; 이렇게 하면 factory 메소드를 통해 객체를 반환받고, 반환 받는 시점에 다양한 작업들을 할수 있다. 3. enum 으로 생성123456public enum LocalCacheEnum&#123; LocalCache; //etc another functions&#125; 잘 사용하지는 않지만 가장 좋은 세번째 방법인 enum으로 클래스를 만드는 방법이라고 한다. 복잡한 직렬화나 리플렉션(reflection) 상황에서도 직렬화가 자동으로 지원되고, 인스턴스가 여러개 생기지 않도록 확실하게 보장해준단다. (by effective java) 그럼 어디서 사용될까 static 으로 선언해서 공통적으로 사용되는 부분이나 환경설정 내용이 변경되면 다른 클래스에서도 그 부분이 똑같이 적용되어 실행되어야 할때 자주 사용되는 부분을 싱글톤으로 만들어 생성되는 시간을 줄일때 스프링에서의 DB커넥션 로직","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"디자인패턴","slug":"디자인패턴","permalink":"https://taetaetae.github.io/tags/디자인패턴/"},{"name":"싱글톤","slug":"싱글톤","permalink":"https://taetaetae.github.io/tags/싱글톤/"}]},{"title":"hexo 환경 구축하기","slug":"20160923","date":"2016-09-23T01:26:53.000Z","updated":"2020-04-23T04:41:36.623Z","comments":true,"path":"2016/09/23/20160923/","link":"","permalink":"https://taetaetae.github.io/2016/09/23/20160923/","excerpt":"개요이전포스팅 에서 이야기 한것과 같이 어느곳에서든지(집 또는 회사 등) 블로그 포스팅을 할수 있는 환경을 만들고 싶었다. (git을 이용해서.)","text":"개요이전포스팅 에서 이야기 한것과 같이 어느곳에서든지(집 또는 회사 등) 블로그 포스팅을 할수 있는 환경을 만들고 싶었다. (git을 이용해서.)그래서 git repository 를 두개를 만들었고, 하나는 실제 블로그서버로 이용하고 하나는 블로그를 포스팅하는 hexo 환경을 저장하게 된다. 지금부터 이야기 할 내용은 hexo환경을 git repository 에서 pull 받아서 환경구성하는 부분을 이야기 하려고 한다. 환경구성hexo설치와 git설치는 되어있다고 가정.먼저 구성할 폴더를 생성하고 이 폴더에 hexo 환경을 구성하겠다고 초기 셋팅을 한다12mkdir bloghexo init blog 그리고 hexo환경을 저장해둔 repository를 가져와야 하므로 git설정을 한다1234cd blog/git initgit remote add origin https://github.com/taetaetae/hexo.gitgit fetch 필요없는초기셋팅이 되는 파일은 지우고12rm source/_posts/hello-world.mdrm -r themes/landscape/ #해당 테마를 사용하고 있다면 지울필요가 없다. hexo환경 repository 를 pull받는다12git reset --hard origin/mastergit pull origin master hueman테마의 검색 기능을 사용한다는 가정하에 필요한 플러그인과, 나중에 deploy 할때 필요한 플러그인을 설치해준다12npm install hexo-deployer-git --savenpm install -S hexo-generator-json-content 이렇게 되면 기존처럼 환경설정이 마무리 되고, 포스팅을 할수 있게 된다. # 추가 canonical 속성npm install –save hexo-auto-canonical 사이트맵 속성npm install hexo-generator-seo-friendly-sitemap –save feed 속성npm install hexo-generator-feed –save","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"}]},{"title":"hexo + github + blog 연동하기","slug":"hexo_github_blog","date":"2016-09-18T06:38:34.000Z","updated":"2020-04-23T04:41:36.760Z","comments":true,"path":"2016/09/18/hexo_github_blog/","link":"","permalink":"https://taetaetae.github.io/2016/09/18/hexo_github_blog/","excerpt":"들어가기에 앞서예전부터 블로그를 운영해야지 하구서 tistory, naver blog 등 다양한 플랫폼으로 시작을 했었지만 이렇다할 운영이 안되었고 사실 열정이 부족했었다. 직접 홈페이지를 만들기에는 너무 많은 허들이 있다보니 (서버구축, 호스팅, 도메인 등 …) 계속 차일피일 미루고 있었다.","text":"들어가기에 앞서예전부터 블로그를 운영해야지 하구서 tistory, naver blog 등 다양한 플랫폼으로 시작을 했었지만 이렇다할 운영이 안되었고 사실 열정이 부족했었다. 직접 홈페이지를 만들기에는 너무 많은 허들이 있다보니 (서버구축, 호스팅, 도메인 등 …) 계속 차일피일 미루고 있었다.그러다 github에서 제공하는 pages라는 걸 이용해서 무료로 도메인과 웹호스팅을 할수 있다는 부분을 알게되었고, 거기에다 jekyll을 이용하면 설치형 블로그를 운영할수 있다는것에 놀라웠다. 하지만 jekyll을 적용해보려고 이것저것 하다보니 ruby라는 언어로 만들어져있고 커스터마이징이 어렵다는 부분을 확인, 좀더 알아보다 hexo 라는 걸로 해당 블로그를 만들게 되었다.필자처럼 남들과는 다른 블로그를 만들고 싶거나, git command 공부도 하면서 블로그를 운영해볼 사람들은 해당 글을 천천히 따라오면 좋을것 같다. - hexo 시작하기hexo 라는걸 시작하기 위해 몇가지 준비물이 있다. node 설치 git 설치 github에 블로그로 사용할 빈 repository 생성 github에 hexo 설정을 저장할 빈 repository 생성 위 4가지(?!)가 전부 설치 되었다고 가정을 하고 시작을 해보겠다. - hexo 설치간단하다. hexo 페이지에도 나와있는것처럼 아래 명령어를 실행해주면 된다.1$ npm install -g hexo-cli - 블로그로 운영할 폴더 hexo 초기화폴더 구조로 구성이 되기 때문에 임의의 폴더를 하나 만들고 해당 폴더를 hexo 명령어로 초기화 시켜준다.12$ mkdir &lt;디렉토리명&gt;$ hexo init &lt;디렉토리명&gt; - 로컬서버 띄워보기이제 로컬에서 서버를 띄워서 블로그가 어떻게 나오는지 확인을 해보면 된다.1$ hexo s (or server) 간혹 서버가 실행이 안되거나 오류가 발생, 수정한 부분이 반영이 안된다면 clean 명령어를 한번 해준 다음에 다시 서버를 실행해주면 되는 경우도 있다.12$ hexo clean$ hexo s (or server) http://localhost:4000 을 접속해서 정상적으로 페이지가 나오는지 확인을 해보자.페이지가 정상적으로 나온다면 성공! - 글 작성아래 명령어를 실행하면 /source/_post/ 아래에 .md 파일이 생성이 된다.1$ hexo new &lt;글 제목&gt; 해당 파일을 사용하기 편한 에디터로 열어서 마크다운 문법에 맞추어 수정을 하면 끝! - 왜 두개의 repository가 필요한가아래에서 이야기 하겠지만, 하나는 실제 블로그 내용이 올라갈 저장소이고 다른 하나는 블로그를 운영하고 있는 hexo 자체를 저장할 저장소이다. hexo 정보를 가지고 있지 않을꺼라면(필자처럼 다양한 PC에서 업로드 환경을 구축하지 않을꺼라면) 하나의 레파지토리만 필요할수도 있다. - github 셋팅 지금부터가 알짜배기다. 즉 이글을 포스팅 하는 의미. 다른 글들에서도 hexo 사용법을 친절하게 알려 주셨으나 github와의 연동, 그리고 어떤식으로 운영해야할지는 찾기 힘들었다. 필자는 감으로 그런가보다(?)하고서 터득한 바를 공유하려한다. (이게 정답은 아니지만, 나는 이렇게 사용하는게 맞겠다 싶어..) 일반적으로 github에서 블로그로 사용할 repository를 만들게 되면 http://(github아이디).github.io/(repository이름) 으로 블로그가 만들어 지는데 뭔가 조금 이상해서 간지가 안나서 찾고 찾아서 아래와 같은 방식으로 하게 되었다.필자의 github 아이디는 taetaetae 이고 도메인은 아이디 그대로를 사용하여 http://taetaetae.github.io 으로 사용하고 싶었다. 따라서 github에 repository이름을 taetaetae.github.io로 만들어야 한다. 여기까지만 하면 일단 github에 배포할수 있는 준비가 되어있는 상태 - hexo로 배포하기포스팅한 글이 정상적으로 등록이 된 것을 로컬서버에서 확인이 되었으면 이 상태를 조금전 만든 github repository으로 배포(정확히 말하면 git push)해줘야 한다. 그전에 최상위 폴더에 있는 _config.yml 파일을 열어서 github 정보를 입력해 줘야 한다.하단 영역 Deployment 부분에 다음과 같이 작성하고 저장한다.12345# Deploymentdeploy: type: git repo: https://github.com/taetaetae/taetaetae.github.io branch: master 그 다음 hexo 에서 github로 배포할수있는 플러그인을 설치해준다.1$ npm install hexo-deployer-git --save 이제 설정한 github에 배포를 하면 끝!1$ hexo deploy 1분~3분 뒤에 도메인을 접속하여 정상적으로 페이지가 나오는지 확인하고, github에 파일들이 push가 잘 되었는지를 확인한다. - 향후 관리 hexo 정보 저장나중에 다른 PC에서도 블로그를 포스팅 할 경우가 있으니 hexo를 이용하여 포스팅 한 환경 자체를 저장 해야할 필요가 생겼다. 따라서 만들었던 폴더 또한 github에 업로드를 해놓는게 좋을것 같다 (지극히 개인적인 생각)git command 설명은 따로 정리하지 않겠다.1234567891011121314git 초기화$ git init변경사항 추가(전체))$ git add .커밋$ git commit -m &quot;커밋메시지&quot;remote repository 등록$ git remote add origin https://github.com/taetaetae/hexo.gitremote repository로 push$ git push origin master 마치며아직 git command 나 markdown, hexo 등 잘 모르는 부분들이 많다. 하나씩 시행착오를 겪어가면서 정리될수 있는 부분들은 이어 정리를 할 계획이다. 그리고 hexo 기본개념이나 설정파일 수정하는 부분들은 다른 분들이 많이 올려놓으셨기 때문데 중요하다고 생각되는 지극히 개인적으로 정리해야겠다고 느끼는 부분들 중심으로 정리를 해야겠다","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"}]},{"title":"첫번째 포스팅","slug":"first","date":"2016-09-17T15:34:23.000Z","updated":"2020-04-23T04:41:36.737Z","comments":true,"path":"2016/09/18/first/","link":"","permalink":"https://taetaetae.github.io/2016/09/18/first/","excerpt":"","text":"시작은 언제든지 새롭고 떨리고 가슴벅차는 순간이다.과연 이 블로그를 잘 운영할수 있을런지..제대로 한번 관리 해보고, 나만의 공간으로 꾸며보자!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]}]}