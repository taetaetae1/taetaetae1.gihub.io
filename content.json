{"meta":{"title":"꿈꾸는 태태태의 공간","subtitle":"taetaetae","description":"개발,사진,일상 모든것에 대한 기록","author":"taetaetae","url":"https://taetaetae.github.io"},"pages":[{"title":"all-archives","date":"2018-08-03T04:23:31.334Z","updated":"2018-07-29T08:31:26.481Z","comments":false,"path":"all-archives/index.html","permalink":"https://taetaetae.github.io/all-archives/index.html","excerpt":"","text":""},{"title":"all-categories","date":"2018-07-29T08:31:26.483Z","updated":"2018-07-29T08:31:26.483Z","comments":false,"path":"all-categories/index.html","permalink":"https://taetaetae.github.io/all-categories/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2018-07-29T08:31:26.484Z","updated":"2018-07-29T08:31:26.484Z","comments":false,"path":"all-tags/index.html","permalink":"https://taetaetae.github.io/all-tags/index.html","excerpt":"","text":""}],"posts":[{"title":"기술블로그 구독서비스 개발 후기 - 3부","slug":"daily-dev-blog-3","date":"2019-02-16T16:17:27.000Z","updated":"2019-02-23T10:42:15.776Z","comments":true,"path":"2019/02/17/daily-dev-blog-3/","link":"","permalink":"https://taetaetae.github.io/2019/02/17/daily-dev-blog-3/","excerpt":"작년 7월 12일부터 시작한 필자의 첫 토이프로젝트인 기술블로그 구독서비스. 오픈할 때까지만 해도 “AWS 프리티어를 사용하고 있는 1년 안에 구독자가 설마 1,000명이 넘겠어?” 라고 생각을 했었는데","text":"작년 7월 12일부터 시작한 필자의 첫 토이프로젝트인 기술블로그 구독서비스. 오픈할 때까지만 해도 “AWS 프리티어를 사용하고 있는 1년 안에 구독자가 설마 1,000명이 넘겠어?” 라고 생각을 했었는데 오픈을 하고 220일째 되는 바로 어제 어느덧 벌써 구독자가 1,000명을 달성하게 되었다. 그 기념으로 그동안 미뤄두었던 기술블로그 구독서비스 개발 후기 시리즈의 3부를 쓰고자 한다.오예~ 1,000명이다! 땡큐! 출처 : https://gfycat.com/ko/leafytorngroundbeetle오예~ 1,000명이다! 땡큐! 출처 : https://gfycat.com/ko/leafytorngroundbeetle 혹시 전에 내용을 보고자 하면 아래 링크에서 확인할 수 있다. 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 # 그간 어떤 식으로 서비스를 운영했는가?(한마디로 정리할 순 없는 지난 220일이었지만…) 딱 한마디로 정리하자면 엄청나게 많은 것을 배우고 경험할 수 있었으나 그만큼 힘들었던 시간들이라고 말할 수 있을 것 같다. 2부에서 이야기한 문제 발생에 따른 Trouble Shooting들도 있었지만 운영을 해오다 보니 사전에 생각하지도 못한 부분에서 문제가 생기는 정말 다양한 경험을 할 수 있었기 때문이다. 블로그 포스팅을 수집하는 과정에서의 문제일부 블로그 RSS url에 접근을 할 때 요청에 대한 응답이 무한대로 멈춰버리는 현상이 간헐적으로 있었다. 이는 별도의 타임아웃을 설정하지 않았기 때문이다. 그래서 어느 정도의 타임아웃을 두고 시간 내에 응답이 없을 경우 다음 포스팅으로 넘어가도록 하였다. (타임아웃은 아주 기본적인 부분인데…) 1requests.get(rss_url, timeout=10.0) 메일 발송하는 과정에서의 문제가끔 메일이 오지 않는다고 친절하게 필자 개인 메일로 연락이 오는 경우가 있었다. 그때마다 서버의 상태를 보면 서버에 직접 접속조차 안 될 정도로 메모리 사용량이 너무 많아서 그때마다 AWS 웹 콘솔에서 강제로 서버를 재부팅을 하곤 했었다. 예전에도 이야기한 것처럼 AWS 프리티어를 사용하고 있다 보니 서버의 메모리가 1기가밖에 되지 않아서 … 제한된 시스템에서 서비스 운영을 할 수밖에 없는 상황이었다.그래서 수집/발송 상태를 로깅으로 쉽게 볼 수 있고 스케줄링을 하기 위해 띄워둔 Jenkins(tomcat)를 중단하고 crontab으로 스케줄링을 하도록 하였고, 로깅은 별도의 파일로 로깅하도록 변경하였다. 1/usr/bin/python3.6 /home/~~~/email_send.py &gt; /home/~~~/logs/job/email_send_`date +\\%Y\\%m\\%d\\%H\\%M\\%S`.log 2&gt;&amp;1 또한 기존에는 빠르게 발송하기 위해 냅다 스레드로 돌렸는데 구독자 수가 많아지다 보니 RuntimeError: can&#39;t start new thread 라고 스레드를 만들 수 없다는 에러가 발생하기도 했다. 그래서 Pool을 사용하는 방식의 multiprocessing 을 도입하여 스레드로 발송할 때보다는 엄청나게 빠른 속도는 아닐지라도 효율적인 메모리 사용으로 2분 안에 1,000명에게 안정된 메일을 보낼 수 있게 되었다. (여담이지만 메일이 안 온다고 알려주셨던 분들께 이 자리를 빌려 감사의 인사를 전하고 싶다.) from multiprocessing import Pool ... pool = Pool(20) pool.map(sendMail, email_list) Heroku 나 Netlify 같이 서버를 직접 들어가지 않고 앱 형태로 배포하는 식으로 할 수도 있다. 하지만 초기에 이 토이프로젝트를 시작할 때 실 서비스와 최대한 동일한 시스템으로 운영해보고 싶었기 때문에 라즈베리파이에 설치하는 것까지 알아보다 결국 AWS를 사용하기로 하게 되었다.그렇다면 AWS 프리티어를 사용하지 않고 별도의 서버를 구매하면 안 될까? 하는 생각도 해봤지만 최소한의 인프라로 최대한의 성능을 내보고 싶은 욕심(?) 때문에 1년간은 프리티어로 운영하고 그다음엔 (혹은 소프트웨어적으로 한계까지 도달한다면) 서버를 구매해서 운영하게 될 것 같다. (적어도 이후에도 이 서비스를 유지한다는 가정하에…) 농부의 마음으로... 출처 : http://www.iwithjesus.com/news/articleView.html?idxno=2511농부의 마음으로... 출처 : http://www.iwithjesus.com/news/articleView.html?idxno=2511 아침 10시가 되면 자동으로 메일이 잘 발송되었는지, 혹 어제 수집된 것이 아니라 예전에 수집된 내용이 중복 발송된 건 아닌지, 발송은 구독한 사람 전부에게 잘 보내졌는지… 거의 매일같이 Daily-DevBlog 서비스를 살피며 지낸 것 같다. (하루라도 문제가 생기면 밤을 새워서라도 원인을 파악하고 다음 발송에는 정상적으로 발송되도록 수정하기도 했다.) # 앞으로의 계획과 방향성여력이 되는 데까지 이 서비스를 운영할 계획이다. AWS 프리티어 기간이 끝나도 라즈베리파이나 안 쓰는 노트북을 활용해서 서버를 구성하던지 (한 달에 얼마를 지불할지는 모르겠지만) AWS에서 서버를 발급받아서라도 운영하고 싶다. 그 이유는 이 토이프로젝트를 진행하면서 얻게 된 인사이트도 상당히 많았고, python과 apache 등 기존에 알고 있던 부분 이외로 알게 되는 것 또한 많았기 때문이다. 그리고 가장 중요한 공유, 사실 이 서비스를 만들면서 필자 또한 많은 좋은 글들을 볼수있었고 그에 큰 도움도 많이 받을 수 있었다.만들고 싶은 기능도 많다. 포스팅의 내용을 분석하여 자동으로 기술과 관련되지 않는 글을 제외하는 기능도 만들고 싶고, 자동으로 주요 키워드 (태그)를 만들어 이후에도 태그 기준으로 검색을 통해 보고싶은 글을 뉴스처럼 볼수 있는 기능도 만들고 싶고… 운영을 하다 보니 만들고 싶은 기능은 많지만 기술적인 접근이 어려운 상황이다.하지만 가장 중요한 건 새로운 기능 추가보다 안정적으로 매일 아침 10시마다 바로 어제의 글들을 수집하여 구독자들에게 발송하는 것이 가장 중요한 게 아닐까 싶다. 구독자수 증가 그래프구독자수 증가 그래프 # 구독자 1,000명 기념 추가 기능 공개!예전부터 1,000명이 되는 시점에 뭔가 이벤트 성으로 새로운 기능을 공개하고 싶어서 준비를 해보았다. 아카이브위에서 이야기했듯이 기술과 관련되지 않는 글들에 대한 필터링을 기술적으로 하고 싶었으나 예로 들어 “00역 맛집리스트 자동으로 가져오기” 나 “코딩하면서 먹기에 좋은 음식” 이라는 제목이 있을 경우 과연 어떤 글이 기술에 관련된 글이고 어떤 글이 기술과는 거리가 있는 글인지 기술적으로 분석할 방법이 아직까지는 떠오르지 않는다. (물론 머신러닝이나 다른 방법이 있겠지만…)그래서 기존에 수집한 글들을 한 곳에서 보여주면서 기술과는 거리가 있어 보이는 글들에 대해서 제외하고 볼 수 있도록 아카이빙 페이지를 만들었다. 그리고 날짜를 넘겨가며 조회할 수 있고 정렬 순서는 랜덤으로 만들었다. 크롬 익스텐션 기술블로그 라고 검색해도 나온다.기술블로그 라고 검색해도 나온다. 위에서 만들었던 아카이빙 페이지를 단순하게 익스텐션 클릭 한 번으로 접속이 되도록 만들어보았다. 점심시간 또는 여유시간에 공유된 기술 블로그 포스팅을 쉬운 접근성을 통해 읽어보자는 조금이라도 챙겨보자는 느낌으로 만들게 되었고, 크롬 알림 기능을 활용하여 PC 크롬이 켜져 있는 상황에서 아침 10시가 되면 메일이 발송되는 것처럼 아래 화면과 같이 알람을 주도록 하였다. 아침 10시엔 우리 모두 Daily-DevBlog를~아침 10시엔 우리 모두 Daily-DevBlog를~ 주간 인기글구독자들이 어떤 글에 더 관심이 갖는지 궁금하였고 많이 본 글에 대해서는 한 번 더 정리하여 메일로 발송해주는 것이 좋을 것 같다는 생각이 들었다. 그래서 메일로 발송된 글에 대해 클릭수를 기준으로 매주 월요일마다 “주간 인기글”을 발행하는 기능을 추가하였다. 단체 블로그 추가 수집 (예정)지금은 어썸 데브블로그에서 제공해주는 개인 블로거들의 피드를 수집하고 있는데 단체 블로그들 또한 추가로 수집하여 메일의 상단에 배치시켜 발송할 예정이다. (단체 블로그는 아무래도 검증이 된 글일 거라 생각이 든다.) # 마치며혹시 이 서비스에 대한 아이디어가 있는 분들은 아래 댓글이나 개인 메일로 알려주시면 최대한 반영해보고자 한다. 또한 나중에는 github에 공개하여 오픈소스화한다면 필자보다 더 뛰어난 python 개발자들이 보다 좋은 코드를 만들어주어 점점 해당 서비스가 좋아지지 않을까 하는 기대를 해보며 기술블로그 구독서비스 개발후기를 마친다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[]},{"title":"누구나 할 수 있는 엑세스 로그 분석 따라 해보기 (by Elastic Stack)","slug":"access-log-to-elastic-stack","date":"2019-02-10T05:37:31.000Z","updated":"2019-02-10T15:44:14.470Z","comments":true,"path":"2019/02/10/access-log-to-elastic-stack/","link":"","permalink":"https://taetaetae.github.io/2019/02/10/access-log-to-elastic-stack/","excerpt":"필자가 Elastic Stack을 알게된건 2017년 어느 여름 동기형이 공부하고 있는것을 보고 호기심에 따라하며 시작하게 되었다. 그때까지만 해도 버전이 2.x 였는데 지금 글을 쓰고있는 2019년 2월초 최신버전이 6.6이니 정말 빠르게 변화하는것 같다.","text":"필자가 Elastic Stack을 알게된건 2017년 어느 여름 동기형이 공부하고 있는것을 보고 호기심에 따라하며 시작하게 되었다. 그때까지만 해도 버전이 2.x 였는데 지금 글을 쓰고있는 2019년 2월초 최신버전이 6.6이니 정말 빠르게 변화하는것 같다. 빠르게 변화하는 버전만큼 사람들의 관심도 (드라마틱하게는 아니지만) 꾸준히 늘어나 개인적으로, 그리고 실무에서도 활용하는 범위가 많아지고 있는것 같다. trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"elasticsearch\",\"geo\":\"KR\",\"time\":\"today 5-y\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&geo=KR&q=elasticsearch\",\"guestPath\":\"https://trends.google.co.kr:443/trends/embed/\"}); 그래서 그런지 최근들어 (아주 코딱지만큼 조금이라도 더 해본) 필자에게 Elastic Stack 사용방법에 대해 물어보는 주변 지인들이 늘어나고 있다. 그리고 예전에 한창 공부했을때의 버전보다 많이 바꼈기에 이 기회에 “그대로 따라만 하면 Elastic Stack을 구성할 수 있을만한 글”을 써보고자 한다. 사실 필자가 예전에 “도큐먼트를 보기엔 너무 어려워 보이는 느낌적인 느낌” 때문에 삽질하며 구성한 힘들었던 기억을 되살려 최대한 심플하고 처음 해보는 사람도 따라하기만 하면 “아~ 이게 Elastic Stack 이구나!”, “이런식으로 돌아가는 거구나!” 하는 도움을 주고 싶다. + 그러면서 최신버전도 살펴보고… 1석2조, 이런게 바로 블로그를 하는 이유이지 않을까?다시한번 말하지만 도큐먼트가 최고 지침서이긴 하다… Elastic 공식 홈페이지에 가면 각 제품군들에 대해 그림으로 된 자세한 설명과 도큐먼트가 있지만 이들을 어떤식으로 조합하여 사용하는지에 대한 전체적인 흐름을 볼 수 있는 곳은 없어 보인다. (지금 보면 도큐먼트가 그 어디보다 설명이 잘되어 있다고 생각되지만 사전 지식이 전혀없는 상태에서는 봐도봐도 어려워 보였다.)이번 포스팅에서는 Apache access log를 Elasticsearch에 인덱싱 하는 방법에 대해 설명해보고자 한다. # 전체적인 흐름필자는 글보다는 그림을 좋아하는 편이라 전체적인 흐름을 그림으로 먼저 보자. 외부에서의 접근이 발생하면 apache 웹서버에서 설정한 경로에 access log가 파일로 생성이 되거나 있는 파일에 추가가 된다. 해당 파일에는 한줄당 하나의 엑세스 정보가 남게 된다. fileBeat에서 해당 파일을 트래킹 하고 있다가 라인이 추가되면 이 정보를 logstash 에게 전달해준다. logastsh 는 filebeat에서 전달한 정보를 특정 port로 input 받는다. 받은 정보를 filter 과정을 통해 각 정보를 분할 및 정제한다. (ip, uri, time 등) 정리된 정보를 elasticsearch 에 ouput 으로 보낸다. (정확히 말하면 인덱싱을 한다.) elasticsearch 에 인덱싱 된 정보를 키바나를 통해 손쉽게 분석을 한다. 한번의 설치고 일련의 과정이 뚝딱 된다면 너무 편하겠지만, 각각의 레이어가 나뉘어져있는 이유는 하는 역활이 전문적으로(?) 나뉘어져 있고 각 레이어에서는 세부 설정을 통해 보다 효율적으로 데이터를 관리할 수 있기 때문이다. beats라는 레이어가 나오기 전에는 logstash에서 직접 file을 바라보곤 했었는데 beats가 logstash 보다 가벼운 shipper 목적으로 나온 agent 이다보니 통상 logstash 앞단에 filebeat를 위치시키곤 한다고 한다. 전체적인 그림은 위와 같고, 이제 이 글을 보고있는 여러분들이 따라할 차례이다. 각 레이어별로 하나씩 설치를 해보며 구성을 해보자. 설치순서는 데이터 흐름의 순서에 맞춰 다음과 같은 순서로 설치를 해야 효율적으로 볼수가 있다. (아래순서대로 하지 않을경우 설치/시작/종료 를 각각의 타이밍에 맞추어 해줘야 할것 같아 복잡할것같다.) 1elasticsearch → logstash → kibana → filebeat 이 포스팅은 CentOS 7.4에서 Java 1.8, apache 2.2가 설치되어있다는 가정하에 보면 될듯하다. 또한 각 레이어별 설명은 구글링을 하거나 Elastic 공식 홈페이지에 가보면 자세히 나와있으니 기본 설명은 안하는것으로 하고, 각 레이어의 세부 설정은 하지 않는것으로 한다. # Elasticsearch공식 홈페이지1234567891011121314151617다운받고 압축풀고 심볼릭 경로 만들고 (심볼릭 경로는 선택사항)$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.tar.gz$ tar zxvf elasticsearch-6.6.0.tar.gz$ ln -s elasticsearch-6.6.0 elasticsearch설정 파일을 열고 추가해준다.$ cd elasticsearch/conf$ vi elasticsearch.ymlpath.data: /~~~/data/elasticsearch (기본경로에서 변경할때추가)path.logs: /~~~/logs/elasticsearchnetwork.host: 0.0.0.0 # 외부에서 접근이 가능하도록 (실제 ip를 적어줘도 됨)elasticsearch 의 시작과 종료를 조금이나마 편하게 하기위해 스크립트를 작성해줌 (이것또한 선택사항)$ cd ../bin$ echo &apos;./elasticsearch -d -p es.pid&apos; &gt; start.sh$ echo &apos;kill `cat es.pid`&apos; &gt; stop.sh$ chmod 755 start.sh stop.sh 혹시 아래와 같은 에러가 발생할경우 공식문서 대로 진행해준다.1234ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]&gt; sudo /sbin/sysctl -w vm.max_map_count=262144 이렇게 하고 시작을 한뒤 브라우저에서 http://{ip}:9200 로 접속하면 다음과 같이 설치된 elasticsearch에 기본 정보가 나오게 되고 이렇게 elasticsearch의 설치 및 실행이 완료되었다. 1234567891011121314151617&#123; &quot;name&quot;: &quot;@@@&quot;, &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;@@@&quot;, &quot;version&quot;: &#123; &quot;number&quot;: &quot;6.6.0&quot;, &quot;build_flavor&quot;: &quot;default&quot;, &quot;build_type&quot;: &quot;tar&quot;, &quot;build_hash&quot;: &quot;@@@&quot;, &quot;build_date&quot;: &quot;2019-01-24T11:27:09.439740Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;7.6.0&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;5.0.0&quot; &#125;, &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; # Logstash공식 홈페이지 1234567891011121314151617181920212223242526272829303132다운을 받고 압축풀고 심볼릭 링크 설정$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.6.0.tar.gz$ tar -zxvf logstash-6.6.0.tar.gz$ ln -s logstash-6.6.0 logstashlogstash가 실행될때 설정값 파일을 만들어준다.$ cd logstash/config$ vi access_log.conf# beats 에서 5044 port 로 데이터를 input 받겠다는 의미input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;# grok 필터를 활용하여 엑세스로그 한줄을 아래처럼 파싱하겠다는 의미# 해당 필터는 apache의 로깅 설정에 의해 만들어지는 파일의 포멧에 맞추어 설정해야한다.filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [&quot;%&#123;IPORHOST:clientip&#125; (?:-|%&#123;USER:ident&#125;) (?:-|%&#123;USER:auth&#125;) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;NOTSPACE:uri&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER:bytes&#125;) (?:-|%&#123;NUMBER:bytes2&#125;)( \\&quot;%&#123;DATA:referrer&#125;\\&quot;)?( \\&quot;%&#123;DATA:user-agent&#125;\\&quot;)?&quot;] &#125; remove_field =&gt; [&quot;timestamp&quot;,&quot;@version&quot;,&quot;path&quot;,&quot;tags&quot;,&quot;httpversion&quot;,&quot;bytes2&quot;] &#125;&#125;# 정제된 데이터를 elasticsearch 에 인덱싱 하겟다는 의미# index 이름에 날짜 형태로 적어주면 인덱싱 하는 시점의 시간에 따라 인덱싱 이름이 자동으로 변경이 된다. (아래는 월별로 인덱스를 만들경우)output &#123; elasticsearch &#123; hosts =&gt; [ &quot;&#123;elasticsearch ip&#125;:9200&quot; ] index =&gt; &quot;index-%&#123;+YYYY.MM&#125;&quot; &#125;&#125; 실행은 다음과 같이 &amp;연산자를 활용하여 background로 실행하게 한다. 1$ bin/logstash -f config/access_log.conf &amp; 이렇게 해서 실행을 하고 에러없이 정상적으로 실행이 된뒤 프로세스가 올라와 있으면 (ps -ef | grep logstash) 성공된 상태라 볼수있다. # Kibana공식 홈페이지 12345678910역시 다운받고 압축풀고 심볼릭 링크 설정$ https://artifacts.elastic.co/downloads/kibana/kibana-6.6.0-linux-x86_64.tar.gz$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz$ ln -s kibana-6.6.0-linux-x86_64 kibana외부에서 접근을 하기위해 ip를 적어주고, 연결할 elasticsearch 주소또한 적어준다.$ cd kibana/config$ vi kibana.ymlserver.host: &quot;@.@.@.@&quot;elasticsearch.hosts: [&quot;http://@.@.@.@:9200&quot;] 실행은 bin 폴더로 이동후에 다음과 같이 실행시켜준다. 별다른 에러가 없으면 외부에서 접근이 가능한지 확인해보자.1234$ cd bin/$ nohup ./kibana &amp;접속 : http://@.@.@.@/5601 # Filebeat공식 홈페이지 1234567891011121314151617181920212223242526다운 → 압축해제 → 심볼릭링크$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.6.0-linux-x86_64.tar.gz$ tar -zxvf filebeat-6.6.0-linux-x86_64.tar.gz$ ln -s filebeat-6.6.0-linux-x86_64 filebeatfilebeat가 실행될때의 설정파일을 작성해준다.$ cd filebeat$ vi access_log.ymlfilebeat.prospectors:- type: log enabled: true paths: - /~~~/logs/apache/access.log.* # 실제 엑세스 파일의 경로 tail_files: true # filebeat 시작시점 기준 파일 끝에서부터 로깅을 읽기 시작 ignore_older: 1m # filebeat 시작시점 기준 1분전의 내용은 무시 close_inactive: 2m clean_inactive: 15mlogging.level: infologging.to_files: truelogging.files: # filebeat가 실행되면서 남기는 로깅파일 정보. 도큐먼트를 읽어보는것을 추천한다. path: /~~~/logs/filebeat name: test-filebeat-log keepfiles: 7 rotateeverybytes: 524288000output.logstash: # 최종적으로 output 할 logstash의 정보를 입력해준다. hosts: [&quot;@.@.@.@:5044&quot;] 위와 같이 설정파일을 작성한 다음 아래처럼 실행을 하면 엑세스 파일의 내용이 filebeat를 거치고 logstash를 거쳐 최종적으로 elasticsearch 에 도달하게 된다. 기존에 엑세스 로그가 양이 많다면 그 정보를 다 읽는 시간이 걸리므로 주의한다. (filebeat 자체적으로 해당 파일의 offset을 관리하기 때문) 1$ ./filebeat -c access_log.yml -d publish &amp; # 최종 확인왼쪽에는 해당 서버를 호출하고 오른쪽에는 키바나를 띄워논뒤 테스트를 해보면 아래처럼 access log를 확인이 가능하다. (apache만 띄워놓은 상태라 404상태로 나오긴 한다..) 불필요한 필드가 있다면 logstash 의 filter에서 remove 하면되고 키바나에서 각 정보를 가지고 다양한 유의미한 데이터를 만들어볼 수 있게 되었다. # 마치며막상 해보면 (해보기전에 느끼는 두려움보다는) 엄청나게 미친듯이 어렵지는 않는데… 맨땅에 해딩이든 뭐든 시작해보고 만들어보는게 중요하다고 다시한번 생각해본다. 필자는 elasticsearch 2.4버전에 대해 영어로된 문서를 보며 설치하고 구성하며 (왜 한글로 된 문서가 한명도 없을까…) 하는 아쉬움에 있었는데 이 글이 필자처럼 설치하는데 비슷한(?) 고충을 느낀 사람들에게 도움이 되었으면 한다.마지막으로 세부 설정값들로 인해 성능이나 기능이 다양하게 바뀔수 있으니 공식 도큐먼트를 보는것을 강력 추천하고 싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"https://taetaetae.github.io/tags/logstash/"},{"name":"kibana","slug":"kibana","permalink":"https://taetaetae.github.io/tags/kibana/"},{"name":"filebeat","slug":"filebeat","permalink":"https://taetaetae.github.io/tags/filebeat/"}]},{"title":"Spring MVC Redirect 처리중에 발생한 Out Of Memory 원인 분석하기","slug":"spring-redirect-oom","date":"2019-01-10T14:14:07.296Z","updated":"2019-01-10T17:22:24.660Z","comments":true,"path":"2019/01/10/spring-redirect-oom/","link":"","permalink":"https://taetaetae.github.io/2019/01/10/spring-redirect-oom/","excerpt":"초창기 신입시절에 배우거나 사용했던 기술적인 방법들이 있다. 시간이 지날수록 왠만해선 다른방법은 사용하지 않으려 하고 습관처럼 기존에 사용했던 방법을 고수하는 버릇이 있다. 그 이유는 과거에 사용했을때 아무 탈 없이 잘 되었기 때문에, 그리고 빠른 구현 때문이라는 핑계일 것 같다.","text":"초창기 신입시절에 배우거나 사용했던 기술적인 방법들이 있다. 시간이 지날수록 왠만해선 다른방법은 사용하지 않으려 하고 습관처럼 기존에 사용했던 방법을 고수하는 버릇이 있다. 그 이유는 과거에 사용했을때 아무 탈 없이 잘 되었기 때문에, 그리고 빠른 구현 때문이라는 핑계일 것 같다. 이러한 버릇은 비단 이 글을 적고있는 필자 뿐만이 아니라 대부분의 개발자들이 가지고 있을꺼라 조심스레 추측해본다. (아니라면…더욱 분발 해야겠다…ㅠ)최근 운영하고 있는 서비스에서 장애 상황까지 갈수있는 위험한 상황이 있었는데 팀내 코드리뷰를 통해 문제점을 파악할 수 있었다. 그 원인은 Spring MVC Controller 레벨에서 redirect 처리를 할때 return값의 Cardinality가 높을경우 다음과 같이 사용하면 안된다고… 12345@RequestMapping(value = \"/test\", method = RequestMethod.GET)public String test() &#123; String url = \"어떠한 로직에 의해 생성되는 url\"; return \"redirect:\" + url; // &lt;- 위험 포인트!&#125; 이 코드가 왜? 어디가 어때서?이제까지 Controller 레벨에서 redirect 처리를 할때 아무생각없이 위에 있는 코드 형태로 구현을 했는데 저러한 코드 때문에 OOM이 발생하여 fullGC 가 여러번 발생하고, 일시적으로 서비스가 지연되는 현상이 발생했다고 한다. 자주 사용하던 방법이였는데 장애를 유발할수 있는 위험한 방법이였다니…이번 포스팅에서는 이러한 방법이 왜 잘못되었는지 실제로 테스트를 통해 몸소(?) 체감을 해보고, 그럼 어떤 방법으로 redirect 처리를 해야 하는가와 개선을 함으로써 기존방식에 비해 어떤점이 좋아졌는지에 대해서 정리해보고자 한다. 뭔가 내것으로 만들기 시리즈물이 나올것만 같은 느낌이다… # 기존방식의 문제점 재현 및 다양한 원인분석기존방식으로 했을때 왜 OOM이 발생했을까? 우리는 개발자이기 때문에 이런저런 글들만 보고 추측 할것이 아니라 직접 재현을 해보고 다양한 시각에서 원인분석을 해보자.먼저 기본적인 Spring MVC 뼈대를 만들고 redirect 하는 return 값의 Cardinality가 높도록 random string 을 만들어 주도록 한다. 즉, /random을 호출하면 /result/ETmHfowFkU처럼 random string 이 만들어 지며 redirect 처리가 되는 매우 심플한 구조이다. 123456789101112131415// Spring 버전은 4.0.6.RELEASE@Controller@RequestMapping(\"/\")public class TestController &#123; @RequestMapping(value = \"random\", method = RequestMethod.GET) public String random() &#123; return \"redirect:result/\" + UUID.randomUUID(); &#125; @RequestMapping(value = \"result/&#123;message&#125;\", method = RequestMethod.GET) public String result(ModelMap model, @PathVariable String message) &#123; model.addAttribute(\"message\", message); return \"result\"; &#125;&#125; 또한 해당 프로젝트에서는 AOP를 사용하고 있었기 때문에 그때와 동일한 상황으로 재현을 하기 위해 AOP관련 설정도 추가해준다. 12345678910111213141516@Configuration@EnableWebMvc@EnableAspectJAutoProxy@ComponentScanpublic class HelloWorldConfiguration &#123; @Bean(name=\"HelloWorld\") public ViewResolver viewResolver() &#123; InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setViewClass(JstlView.class); viewResolver.setPrefix(\"/WEB-INF/views/\"); viewResolver.setSuffix(\".jsp\"); return viewResolver; &#125;&#125; 이렇게 한뒤 tomcat으로 최대/최소 메모리를 256m으로 설정후 해당 모듈을 띄워준다. 그다음 메모리 상태를 보기 위해 tomcat에 pinpoint를 연동하고 마지막으로 호출테스트를 위해 nGrinder을 설정해준다. 특별한 설정은 없고 위 컨트롤러의 url (/random) 을 여러번 호출하도록 하였다. nGrinder을 설정하는대에는 이 블로그 포스팅을 참고해서 설정하였다. 자, 이제 테스트를 시작해보자. (마치 수술 집도하는것 같은 기분으로…간호사~ 칼!) nGrindernGrinder의 기본 스크립트에서 url만 해당 서버로 호출되도록 바꿔주고 총 가상 사용자는 2,000으로 시간은 5분으로 설정후에 테스트 시작을 하였더니 다음과 같은 그래프를 볼수 있었다. TPS가 불안정해지다가 어느시점부터 낮아지는것을 확인할 수 있다. 이게 서비스 였다면 사용자가 접속하는데 불편을 느꼈을꺼라 추측을 해본다. 또한 아주 간단한 random string 을 리턴하는 페이지 임에도 불구하고 에러 응답이 적지 않은것을 확인할 수 있었다. pinpoint메모리 상태는 어떤지 확인하기 위해 pinpoint를 확인해보면 다음과 같은 그래프를 볼수 있었다. 보기만해도 심장이 벌렁벌렁(?) 뛸 정도로 무서운 그림이다. 실제로 서비스에 (이정도까진 아니였지만) 비슷한 상황이 발생했었다. 메모리가 테스트를 점점 하면 할수록 올라가다가 fullGC가 발생하더니 대나무 숲에 있는 대나무마냥 fullGC가 빼곡히 발생하였다. (이러니… 페이지 접근에 지연이 생긴것 같다.) Heap dump그럼 실제로 메모리는 어떤 상태였고 어디서 메모리를 많이 사용하고(점유하고) 있는지를 확인하기 위해 Heap dump를 생성해 보았다. 힙덤프 분석하는데 잘 알려진 Memory Analyzer (MAT)를 다운받고 해당 프로세스의 힙덤프를 생성한다음 분석을 해봤더니 아래와 같은 화면을 볼 수 있었다. 힙덤프 파일을 열자마자 (저 문제 있어요~ 도와주세요 하듯) 뭔가 많이 점유하고 있는것처럼 보이는 파이그래프가 Overview에 보였다. Reports 영역에 있는 Leak Suspects를 확인해보니 아래 경로에서 많이 사용하는 것을 확인할 수 있었다. java.util.concurrent.ConcurrentHashMap$Node 이 툴에서는 OQL이라고 힙덤프에 있는 데이터를 일반 SQL처럼 쿼리처럼 볼수 있었다. 그래서 아래처럼 쿼리를 작성해서 봤더니 결과만 봐도 어디서 메모리를 점유하고 있는지 한눈에 볼수 있었다 123SELECT o.key.toString() FROM java.util.concurrent.ConcurrentHashMap$Node o WHERE ((o.key != null) and (o.key.toString().indexOf(\"org.springframework.web.servlet.view.RedirectView_redirect\") = 0)) 무작위로 만들어진 url에 대한 정보를 캐시하고 있는 듯한 결과였다. # 방식 개선 및 변화 비교결국 return &quot;redirect:&quot; + url; 와 같은 처리가 문제를 야기했던 것이였다. 그럼 redirect 처리를 어떻게 하는게 좋을까?조금 검색을 해보면 RedirectView 나 ModelAndView를 사용하라고 권장하고 있다. 물론 redirect 되는 url의 Cardinality가 높지않고 고정적이라면 지금의 return &quot;redirect:&quot; + url; 이 방식을 사용해도 무방할수 있다. 하지만 컨트롤러 메소드가 String타입을 return 하게 되면 View 클래스로 변환작업을 진행하게 된다고 한다.이 작업중에 org.springframework.beans.factory.config.BeanPostProcessor구현체들도 같이 진행되고, 이중에 하나가 AnnotationAwareAspectJAutoProxyCreator 라고 있는데 해당 클래스 내부적으로 ConcurrentHashMap&lt;Object, Boolean&gt; 타입 객체에 key : viewName, value : 필요 여부(boolean) 형태로 갯수 제한 없이 저장하고 있다. 그러다보니 url의 종류가 많아질수록 메모리가 많이 사용될 수밖에 없었던 것 같다.즉, 동일한 url에 대해 View 객체를 캐싱하고 있으니 위와 같이 url의 종류가 다양할경우 (특히 로그인 같은 처리를 할때 고유값을 파라미터로 넘기는 경우) 캐싱 객체 숫자가 많아지기 마련이다. 실제로 코드를 보면 캐싱을 하고있는것을 볼수 있다. Spring-project github123456789101112131415// ... 생략protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; // ... 생략 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); // &lt;- 여기 return bean; &#125; // ... 생략 this.advisedBeans.put(cacheKey, Boolean.FALSE); // &lt;- 여기 return bean;&#125;// ... 생략 자, 그럼 개선방법을 알아봤으니 한번 비교를 해보자. 아래처럼 ModelAndView를 사용해서 redirect처리를 할수있도록 코드를 변경하고 1234567891011121314151617181920@Controller@RequestMapping(\"/\")public class TestController &#123; @RequestMapping(value = \"random\", method = RequestMethod.GET) public ModelAndView random() &#123; ModelAndView modelAndView = new ModelAndView(); RedirectView redirectView = new RedirectView(); redirectView.setUrl(\"result/\" + UUID.randomUUID()); modelAndView.setView(redirectView); return modelAndView; &#125; @RequestMapping(value = \"/result/&#123;message&#125;\", method = RequestMethod.GET) public String result(ModelMap model, @PathVariable String message) &#123; model.addAttribute(\"message\", message); return \"result\"; &#125;&#125; 기존과 동일한 방법과 환경에서 테스트를 해보자. nGrinder위해서 했던 방법과 동일한 가상 사용자 수와 동일한 시간으로 테스트를 해보니 다음과 같은 그래프를 볼수 있었다. 위에서 봤던 들쭉날쭉 그래프보다 훨씬 더 안정적인것을 볼 수 있었고, 에러도 단 한건도 없이 훨씬 높은 TPS를 끝까지 일정하게 유지하는 모습을 볼수 있었다. (로직상 에러가 나는게 이상한… 아니 안나야 정상이다.) pinpointTPS가 안정적이였기 때문에 메모리의 상태를 안봐도 되겠지만 비교의 목적이 있기 때문에 pinpoint 의 그래프를 한번 보자. nGrinder로 테스트하는 시점만 잠깐 메모리가 올라가다가 다시 내려오고 전에 있었던 fullGC도 없고 위에서 테스트 했던 그래프 보다는 안정적인 그래프라고 볼수 있었다. Heap dump메모리가 안정적이였지만 혹시 pinpoint에서 잘못 집계하거나 그래프만 보고 맹신할수 없었기 때문에 이번에도 Heap dump를 생성해 보았다. 일단 점유하고 있는 메모리의 크기가 약 10분의 1정도로 줄어든것을 확인할수 있었고, 위에서 했던 OQL을 이용한 메모리 점유를 확인해봐도 기존에 있던 RedirectView_redirect관련 데이터가 아예 없음을 확인할 수 있었다. 코드 몇줄 변경한것밖에 없는데 같은 테스트 환경에서 확연히 좋아진것을 확인할 수 있다. (뿌듯) 전체적으로 다시 비교를 해보면 아래와 같이 이쁜(?) 변화를 볼수가 있다. # 마치며평소에 자주 사용하던 방식인데 성능적으로 자칫 치명적인 결과를 가져올수 있다고 하여 재현을 안해볼수가 없었다. 만약 악의적으로 url뒤에 무작위 문자열을 더해서 ddos공격을 했더라면? 얼마 안가서 서버가 터졌을지도 모른다.지금이라도 알아서 다행이라는 생각과 재현을 안해보고 그냥 그런가보다 하며 넘어갔다면 실제 Spring 내부 코드까지 볼일이 있었을까 하는 생각을 해본다.이번에 재현을 해보면서 nGrinder 로 성능테스트에 pinpoint 모니터링, 마지막으로 힙덤프 분석까지. 꼭 이번 url redirect 문제만이 아니라 다른 성능적인 이슈가 생길때 마치 치트키처럼 활용할수 있을 나만의 무기를 얻은것 같아 다시한번 뿌듯함을 느낀다. 마지막으로, 이렇게 재현까지 하도록 자극을 주신 팀 동료분께 감사드린다고 전하고 싶다. (보실지 안보실지 모르겠지만 ^^;) 관련 참고글https://www.baeldung.com/spring-redirect-and-forwardhttps://www.slideshare.net/benelog/ss-35627826","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"redirect","slug":"redirect","permalink":"https://taetaetae.github.io/tags/redirect/"},{"name":"out of memory","slug":"out-of-memory","permalink":"https://taetaetae.github.io/tags/out-of-memory/"},{"name":"heap dump","slug":"heap-dump","permalink":"https://taetaetae.github.io/tags/heap-dump/"}]},{"title":"천만 명의 사용자에게 1분 내로 알림 보내기 (병렬프로세스의 최적화)","slug":"faster-parallel-processes","date":"2019-01-02T03:43:44.000Z","updated":"2019-01-07T04:23:55.005Z","comments":true,"path":"2019/01/02/faster-parallel-processes/","link":"","permalink":"https://taetaetae.github.io/2019/01/02/faster-parallel-processes/","excerpt":"만약 1번부터 10번까지 번호표가 있는 사람들 총 열명에게 혼자서 동일한 내용의 메일을 보낸다고 가정해보자. 그리고 메일 발송시 한번에 한명에게만 보내야 하는 제한사항이 있을때 과연 당신은 어떤식으로 보내겠는가? 이어서 읽지말고 한번 생각해보자.","text":"만약 1번부터 10번까지 번호표가 있는 사람들 총 열명에게 혼자서 동일한 내용의 메일을 보낸다고 가정해보자. 그리고 메일 발송시 한번에 한명에게만 보내야 하는 제한사항이 있을때 과연 당신은 어떤식으로 보내겠는가? 이어서 읽지말고 한번 생각해보자.아무것도 고려하지 않고 단순하게 생각한다면 1번 보내고 &gt; 2번 보내고 … 9번 보내고 &gt; 10번 보내는 방법이 먼저 떠오르게 된다. (for loop 1 to 10 … ) 하지만 보내야 할 사람들이 많아져서 백명, 천명 많게는 천만명에게 보내야 할 경우 방금과 같은 순차적인 방법을 사용하면 너무 늦게 발송된다는건 코드를 작성하지 않아도 알 수있는 문제… 그렇다면 어떤 방법으로 보내야 보다 빨리 보낼수 있을까?이번 포스팅에서는 필자가 운영하고 있는 서비스에서 기존에 있던 병렬프로세스를 어떤식으로 최적화 했는지, 그래서 결국 얼마나 빨라졌는지에 대한 과정을 정리해 보고자 한다. 비단 메일 발송이나 앱 푸시 등 특정 도메인에 국한되지는 않고 전반적인 프로세스에 대해 이해를 한다면 다른 곳에서도 비슷한 방법으로 활용할 수 있을꺼라 기대 해본다. # 상황파악 및 목표(원할한 이해를 돕기 위하여) 먼저 필자가 운영하고있는 서비스를 간략히 소개부터 해야겠다. (그렇다고 필자 혼자 다 하는건 아님^^;…)셀럽의 방송이 시작되면 구독한 사용자에게 각 모바일 기기에 설치되어있는 앱으로 알림을 보내어 예정에 없던 깜짝 라이브 방송이나 VOD 영상 오픈을 보다 빠르게 확인할 수 있도록 제공하고 있다.여기서, 알림이 늦게 발송되면 셀럽은 방송을 시작하고 팬들이 들어오기까지 기다려야 한다거나 반대로 팬들은 방송 시작하고 뒤늦게 방송을 보게되는 불편함이 생기게 된다. 그리고 중복으로 알림이 발송되거나 특정 사용자들에게 발송이 누락되면 안 되는 등 “알림” 이란 기능은 서비스에 있어서 중요한 기능 중에 하나라고 할수 있다. 여기서 “발송 시간”은 처음 발송작업 시작부터 마지막 사용자에 대해 사내 발송 플랫폼으로 발송 요청을 하기까지의 시간을 의미 그리고 “채널” 이라는 샐럽단위의 그룹이 있는데 영상과 채널의 관계는 1:N이다. 즉, 하나의 영상을 여러 채널에 연결시킬수 있어서 하나의 영상에 대해 여러 채널들에게 연결을 시켜놓으면 채널을 구독하고있는 각각의 사용자에게 모두 알림을 발송 할수가 있게 된다. 우선, 알람이 사용자에게 전달되기까지의 큰 흐름은 다음과 같다. 알림 프로세스알림 프로세스 서비스에서 보낼 대상과 보낼 정보를 조합하여 사내 푸시 발송 플랫폼인 사내 발송 플랫폼에게 전달을 하면 플랫폼에 따라 발송이 되고 최종적으로는 사용자의 모바일 기기에 노출이 됨 간단하게 “병렬로 발송하면 되지 않을까?”라는 필자의 생각이 부끄러워질 정도로 이미 redis, rabbitMQ 를 활용해서 아래 그림처럼 병렬 프로세스로 구성되어 있었다. 기존 구조기존 구조 라이브가 시작되거나 VOD가 오픈될 경우 api가 호출이 되고 다시 배치 서버에게 영상의 고유번호를 전달 전달받은 영상의 고유번호를 rabbitMQ의 수신자 조회 Queue에 produce 수신자 조회 Queue의 consumer인 수신자 조회 모듈에서 영상의 고유번호를 consume 후 아래 작업을 진행3-1. 영상:채널 은 1:N 구조이기 때문에 여러 채널의 사용자들에게 알림을 발송할 수 있고, 영상에 연결된 채널들의 user를 db에서 가져온다.3-2. 가져온 user를 (중복으로 알림이 발송되지 않기 위해) java set에 담고 모든 채널을 조회했다면 redis에 sorted set으로 담는다.3-3. 적당한 크기로 분할하고 이 분할정보를 발송 Queue에 produce 발송 모듈에서 분할 정보를 consume 하고 아래 작업을 진행 (병렬처리)4-1. redis 에서 user 모음을 가져오고4-2. 조회한 user에 해당하는 deviceId를 db에서 가져옴 deviceId와 컨텐츠 정보를 활용하여 적절한 payload를 구성 후 사내 발송 플랫폼 에게 전달 기존 구조에서 발송 시간은 서비스에서 구독자 수가 가장 많은 채널 기준으로 약 1.1천만 명에게 최종 11분 정도 소요되고 있었다. (맨 처음에 이야기 한 순차적인 방법이였다면… 훨씬더 오래 걸렸을꺼라 예상해본다…) 기존에 구성하셨던 분들도 수많은 시행착오와 고민을 하시며 구성하셨을 텐데 더 이상 어떻게 더 빠르게 보낼 수 있을까 하는 부담감과 자칫 알림이 잘못 발송되기라도 한다면(장애가 발생한다면) 그 수많은 사용자들의 불만 화살 과녁이 필자가 되어야 한다는 압박감이 개선 시작 전부터 머릿속을 휘감고 있었던 찰나에 답정너답정너 라는 불가능할 것만 같은 목표가 (답정너 마냥) 정해지며 그렇게 푸시 개선 프로젝트가 시작되었다. 결국 사내 발송 플랫폼에게 얼마나 더 빨리 보낼수 있는가 가 개선 포인트 라고 할수 있겠다. # 1차 개선 : AsyncRestTemplate 적용사내 발송 플랫폼에 요청을 한 뒤 응답의 종류(성공/실패)에 따라 발송 시간 로깅만 하기 때문에 응답을 기다리지 않고 AsyncRestTemplate를 사용해서 비동기 호출로 변경하였다. 사내 발송 플랫폼에 요청에 따른 응답은 1~2초 내외였지만 발송 대상이 많을수록 기다리는 시간을 모아보면 무시 못 할 시간이었기 때문이다. 구조가 크게 변경된 건 없었고 발송하는 부분에서 약간의 로직만 변경하였는데 나름의 큰 효과를 볼 수 있었다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 첫발송 약 6분 약 6분 마지막 발송 약 11분 약 7분 # 2차 개선 : 발송대상 구하는 즉시 병렬 발송처리, 불필요 프로세스 제거 발송대상 구하는 즉시 병렬 발송처리기존 구조에서는 발송 대상을 전부다 구한 뒤에 발송이 시작되었다. 왜냐하면 영상에 연결된 채널이 여러 개가 될 수 있다 보니 중복 사용자 제거를 해야 하기 때문이었다. 예로 들어 영상 하나에 A, B, C 채널이 연결되어있고 어느 사용자가 A, B 채널을 구독하고 있는 상황에서 중복제거를 하지 않고 보낸다면 해당 사용자는 같은 내용의 알림을 두 번 받는 상황이 된다.영상에 연결된 채널이 한 개라면 문제가 없지만 두 개 이상일 경우부터 중복알림 문제가 발생했기 때문에, 그리고 이 중복제거 프로세스가 다 되어야지만 첫 번째 발송이 되는 구조였기 때문에 어떻게든 다른 방법으로 중복 발송을 해결해야만 했다.그래서 여러 시행착오 끝에 결정된 방법은 “이 사용자는 발송이 되었다”라는 정보를 redis에 담는 식으로 중복체크하는 방법을 바꾸는 것이었다. 또한 첫 번째 채널(구독자 수가 가장 많은 채널)은 중복체크를 할 필요가 없기 때문에 db에서 조회하는 즉시 발송해서 방송 시작 1초 내에 사용자에게 알림을 발송할 수가 있었다. 불필요 프로세스 제거발송 triggering 을 배치(jenkins)에서 하고 있었다. api에서 jenkins remote api로 호출이 되면 기본적으로 약 20~30초가량의 인스턴스 구동시간이 존재하게 되는데 이 시간 또한 불필요한 프로세스라고 생각되어 api가 바로 수신자 조회 Queue에 produce 하는 식으로 구조를 변경하였다. 2차 개선2차 개선 api에서 바로 수신자 조회 분할 Queue로 produce 수신자 조회 분할 모듈에서 consume을 하고 적당한 크기로 start index, end index를 구분하여 다시 수신자 조회 Queue로 produce 수신자 조회 모듈이 병렬로 consume을 하며 아래 작업을 수행합니다.3-1. 발송 대상 user를 db에서 가져옴3-2. 첫 번째 채널일 경우(구독자 수가 가장 많은 채널) 중복제거키에 담고 발송대상 key에 담은 뒤 발송 Queue에 produce3-3. 첫 번째 채널이 아닐 경우 중복제거를 해야 하기 때문에 중복제거키에서 redis의 zscore 연산 (시간 복잡도 O(1) )을 활용하여 발송되지 않은 user만 간추려서 발송 대상 key에 담은 뒤 발송 Queue에 produce 기존과 동일 이렇게 개선한 결과 사용자들이 방송이 시작되자마자 알림을 받기 시작할 수 있었고, 발송 대상을 구하자마자 발송하기 때문에 발송 속도도 개선이 됬음을 확인할 수 있었다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 약 11,120,000명 첫발송 약 6분 약 6분 약 1초 마지막 발송 약 11분 약 7분 약 5분 30초 # 3차 개선 : 발송대상 병렬x병렬조회, redis 파티셔닝, 채널간의 발송 타이밍 해소 발송대상 병렬x병렬조회몇 차례 속도 개선을 하는 필자를 보고 팀원 분들이 짠하게(?) 느끼셨는지 아이디어를 하나 건네주셨다. 그건 바로 db에서 user를 조회할 때 병렬 조회하는 것을 다시 병렬 조회하는 것.db에 채널별 구독자 테이블에는 user가 오름차순으로 정렬되어 있다 보니 큰 단위로 나눌수가 있고, 다시 이를 작은 단위로 분할하여 조회가 가능했던 것이었다. 대신, 나누는 단위가 적당해야 하고(테스트를 통해서 찾아내야…) user가 꽉 찬(?) 그룹이 있는가 반면 비어있는 그룹이 있을 수가 있다. 그림으로 그려보면 다음과 같다. 1단계 : 첫 번째 user가 1, 마지막 user가 300만이라고 가정할 때 큰 단위(10만)로 분할합니다.예 ) 0~100,000 / 100,000~200,000 / … / 2,900,000~3,000,000 2단계 (병렬) : 1단계에서 나눈 단위를 다시 작은 단위(1,000)로 분할하여 db에서 조회를 하고 그다음 단계를 진행합니다.예) 0~1,000 / 1,000~2,000 / … / 99,000~100,000 이렇게 하고서 반영을 해보니 속도가 빨라진 대신 redis 가 부하를 많이 받게 되어 다른 모듈에서 redis 를 사용하는 곳에서 지연이 발생하게 되었습니다. 모니터링 툴인 pinpoint에 롯데타워가 뙇.. 결국 알림 속도를 빠르게 한답시고 서비스 전체가 사용하는 공용 redis에 지연이 발생하게 되어버린 것이었다. 개선을 함에 있어 서비스 영향도를 리스트업 하고, 조금이라도 문제가 생길것 같은 부분을 고려해야 하는 교훈을 얻을수 있었다. redis 파티셔닝알림 발송만을 위한 별도 redis 클러스터를 구축하기에는 장비 발급부터 간단한 작업이 아니었기에 어떻게든 로직에서 해결점을 찾아야 했다. 고민의 고민을 한 결과 redis 는 Single thread 방식으로 처리하기 때문에 key 하나에 연산이 끝날 때까지 해당 key가 속한 redis는 다른 연산을 처리할 수가 없게 되는 부분을 인지하고 중복 발송을 막기 위한 redis 키를 기존에는 하나를 사용하고 있었는데 이를 user 값 기준으로 여러 개의 키로 파티셔닝 하게 되었다.즉, user가 천만 개라고 가정했을 때 기존에는 한 개의 키에 천만 개가 들어가던 구조에서 user 값을 10,000으로 나누어 결과적으로는 하나의 키에 1,000개씩 총 10,000개의 키에 파티셔닝되어 들어가게 되는 구조로 변경하게 되었다. 그랬더니 발송 속도도 더 빨라지고 pinpoint에 응답 그래프도 전혀 문제가 없는 수치인 것을 확인할 수 있었다. 채널간의 발송 타이밍 해소여러 채널을 동시에 보내다 보니 아주 간헐적으로 중복 알림이 발생하게 되었다. 이유는 지금까지 프로세스를 보면 여러 단계의 병렬 프로세스가 있는데 각 프로세스별 순서 보장이 안되고 각자 진행되기 때문에 중복체크 키에 들어가기 전에 다른 프로세스에서 먼저 중복체크를 하고 발송을 해버리면 중복으로 발송이 되어버리던 것이었다. 간단히 그림으로 설명해보면… 위 그림에서 1,2,3,4,5 가 동시에 발송을 시작한다고 가정했을 때 그 다음은 2번이 먼저 진행될 수도 있고 5번이 먼저 진행될 수도 있게 된니다. 그렇기 때문에 매번 중복 알림이 발생되는 건 아니었지만 아주아주 간헐적으로 발생하게 되었다.이 문제는 간단히 채널별로 병렬 조회 하는 부분에서, 1초마다 발송 대상수(redis 를 활용하여 로깅 목적으로 발송 대상수를 트래킹하고 있다.)가 변하지 않을 경우 한 채널에 대해 발송이 완료되었다고 간주하고 그 다음 채널을 발송하는 방법으로 해결하였다. 이렇게 거듭된 개선을 거쳐 정리된 최종 구조는 다음과 같다. 3차 개선. a.k.a. 최종 구조3차 개선. a.k.a. 최종 구조 채널별로 큰 단위로 index를 파티셔닝 하여 병렬 조회할 수 있도록 한다.1-1. 첫 consumer는 채널을 채널 간의 알림 발송 진행을 담당해주고,1-2. redis에 발송 대상수가 변함이 없을 경우 다음 채널을 발송하도록 한다. 1에서보다 더 작은 단위로 파티셔닝하여 아래 작업을 수행한다.2-1. db에서 user를 조회하고2-2. user를 중복제거 key에 10만단위로 파티셔닝 하여 담는다. ex ) user가 105872 인경우 push:overlapCheck:100000 에, user가 3409572 인 경우 push:overlapCheck:34000002-3. 2-1에서 가져온 user를 임의 redis key에 담는다. 중복체크 작업을 수행합니다.3-1. 첫 번째 채널일 경우 중복체크를 하지 않고 바로 발송을 한다.3-2. 첫 번째 채널이 아닐 경우 2-3에서 저장한 redis key의 값을 조회하여 2-2에서 저장한 중복제거 key에 있는지 확인 후 발송 여부를 결정한다. ▶ 개선 결과 항목 기존 1차 2차 3차 발송 대상수 약 10,530,000명 약 10,570,000명 약 11,120,000명 약 11,240,000명 첫발송 약 6분 약 6분 약 1초 약 1초 마지막 발송 약 11분 약 7분 약 5분 30초 51초 # 마치며결국 처음에 개선 프로젝트 시작 시 정해졌던 목표에 도달할 수 있었다.(발송 대상이 약 100만 명이 더 늘었지만 1분내로 발송 성공)또한 무조건 좋다고 사용하다간 오히려 독이 될 수 있고, 반대로 돌아가는 원리를 잘 알아보고 사용한다면 본인이 원하는 가장 이상적인 결과를 만들 수 있다는 좋은 경험을 얻을 수 있었다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"parallel precess","slug":"parallel-precess","permalink":"https://taetaetae.github.io/tags/parallel-precess/"},{"name":"redis","slug":"redis","permalink":"https://taetaetae.github.io/tags/redis/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"https://taetaetae.github.io/tags/rabbitMQ/"}]},{"title":"2018 회고 - Coder가 아닌 Programmer로","slug":"review-2018","date":"2018-12-31T12:33:29.000Z","updated":"2018-12-31T17:17:54.820Z","comments":true,"path":"2018/12/31/review-2018/","link":"","permalink":"https://taetaetae.github.io/2018/12/31/review-2018/","excerpt":"매사에 행동하는 모든것들의 끝자락에서는 그동안 잘한것과 못한것을 다시 생각하며 잘한것은 보다 더 잘할수 있도록 하고 못한것은 왜 못했는지 그리고 어떻게 하면 못한 부분을 고칠수 있을지에 대한 시간을 갖으려고 노력해왔다. 그게 개발이 되었든 게임이 되었든 연인과의 데이트가 되었든 뭐든지.","text":"매사에 행동하는 모든것들의 끝자락에서는 그동안 잘한것과 못한것을 다시 생각하며 잘한것은 보다 더 잘할수 있도록 하고 못한것은 왜 못했는지 그리고 어떻게 하면 못한 부분을 고칠수 있을지에 대한 시간을 갖으려고 노력해왔다. 그게 개발이 되었든 게임이 되었든 연인과의 데이트가 되었든 뭐든지. 이러한 시간들은 필자에게 큰 인사이트를 얻을 수 있게 되었고 지난 한해를 돌이켜 보자면 개인적으로 계획한 전부를 다 이뤄내지는 못했지만 나름의 많은 경험과 성과를 달성했다고 생각해본다.이제 몇시간 뒤면 올해가 끝나고 새로운 한 해가 시작되는 이 시점에 개발자로써의 회고를 해보며 2018년 정리 및 2019년 목표를 다짐해보자. # 글쓰는 개발자가 되자. 개인 블로그 운영아주 오래전, 동기 형을 통해 개발자가 글을 써야하는 중요성에 대해 절실하게 배우게 되었고 그때부터 블로그를 운영하기 시작하였다. 그 동기형의 말에 조금 더 내 생각을 첨가하자면 글을 쓰다보면 누군가 내 글을 본다는 마음에 내가 알고있는 지식을 보다 더 깊게 공부하게 되고 그것들이 모여 내 개발 히스토리가 만들어 지며 포트폴리오 등 다양하게 활용할 수 있기에 블로그를 운영하는건 정말 좋은 선택지 였던것 같다. 실제로 그냥 구글링 해서 알게된 것과는 또 다른 배움이 있었기 때문이다.회사 일 그리고 개인 공부를 하면서 적어도 한달에 한가지 이상은 배우게 되기 때문에 올해 초 한달에 한개 이상의 글을 쓰기로 결심하였다.(그 달의 글이 없다면 뭔가 놀았거나(?) 미친듯이 바빴거나 아니면 게을렀거나…) 블로그에 글을 쓴 내역을 그래프로 시각화 해보면 아래처럼 총 23개의 글을 작성하였고 월 평균 1.9개의 글을 작성하게 된것을 볼수 있다. 9월달엔 팀 옮기자마자 엄청 바빴고, 11월엔 그 바쁜게 결실을 맺는 시간… 이라 핑계를… (나중에 블로깅 예정, 병렬 프로그래밍 관련) 월별 글 작성수월별 글 작성수 위 결과만을 두고 봤을땐 많으면 많고 적으면 적다고 할 수 있는 결과지만 개인적으로는 자투리 시간을 활용해서 그간 배웠던것, 그리고 경험했지만 내것으로 만들지 못하고 보기만 하며 넘어간것들에 대해 귀찮지만 시간을 투자하고 정리했더라면 더 많은 글을 썼을것 같다는 조금 아쉬운 결과라고 생각이 든다. 주 단위 PV, 누군가 내 글을 보고 있다는것에 뿌듯함주 단위 PV, 누군가 내 글을 보고 있다는것에 뿌듯함 나름 열심히 글을 쓴 결과일까, GA를 통해 본 필자의 블로그에 유입량이 점점 늘어나는것을 보며 하나를 쓰더라도 좀더 자세히 독자의 입장에서 써야겠다고 다시한번 다짐하게 된다. 다만 글을 “많이” 쓰는것보다 하나를 작성하더라도 원인과 근거를 들어가며 문제를 정확히 파악하는데 집중을 해야하고, 단순 사용법 나열이 아닌 실제로 경험을 해가면서 “내것”으로 만드는 과정이 필요하겠다. # 회사 팀 변경 그리고 토이 프로젝트기존에 아무것도 없던 환경에서 서버 발급부터 이런 저런 서비스에 도움이 되는 다양한 모니터링 툴을 개발하며 무사히 서비스를 오픈을 하였고, 약간의 매너리즘이 생겨날 즈음 좋은 기회가 생겨 성격이 전혀 다른 서비스를 하는 팀을 옮기게 되었다. 약간 이직과도 비슷한 느낌으로 팀을 옮기게 되었는데 처음엔 새로운 지식을 습득해야 하는 두려움도 있었고 기존 서비스에 애정이 많아서 고민이 많았지만 벌써 옮긴지 5개월이 지나고 돌이켜보면 올해 가장 잘한 일 중 하나가 아닐까하는 생각이 든다. 전 팀에선 서비스를 운영하는데 그쳤지만 지금 내가 있는 곳은 대용량 서비스를 성능측면에서, 그리고 아키텍쳐 측면에서 보다 효율적으로 개발하는데 집중을 하려는 모습들이 보이기 때문이다. 더불어 팀에 투입되자마자 필자 홀로 기존에 있던 병렬 프로세스를 개선하여 서비스적으로 약 90%의 개선효과를 볼수있었는데 이 부분은 추후 포스팅 할 예정이다.그리고 팀을 옮기기 한두달 전 개인적인 여유시간이 많이 있었고, 다른사람들의 블로그를 보며 챙겨보고 싶은 마음에 토이 프로젝트를 만들게 되었다. 7월 중순부터 시작했으니 이것도 어느덧 반년이 지나고 있는데 운영을 해가면서 기능을 추가하기 위해 종종 밤을 새는 등 올 한해있어 꽤 많은것을 얻을수 있었던 시간이였다. 간혹 버그가 생겨 메일이 발송 안되면 지인 또는 모르는 분들이 메일로 제보도 해주시고 … 색다른 경험이였다. 자세한 내용 및 후기는 개발후기-1 과 개발후기-2에서 확인 가능하다. (어서 3편을 쓰고 마무리를 지어야 할텐데…) 그리고 최근에는 아카이빙 기능을 만들어 과거 글을 조회할수 있도록 만들었는데 2% 부족한 느낌이다… (맘같아서는 형태소 분석을 해서 자동 필터링도 해보고 싶은데…) 점점 늘어가는 구독자수, AWS 프리티어가 끝나기 전에 뭔가 방법을 찾아야 하는데 ...점점 늘어가는 구독자수, AWS 프리티어가 끝나기 전에 뭔가 방법을 찾아야 하는데 ... # 공유 및 발표3월 즈음 POP it 관리자분께서 회사까지 직접 찾아와 주셔서 만남을 갖고 POP it 저자활동을 시작하게 된다. 그리고 비슷한 시점 D2 Hello World 담당자의 제안으로 이전 팀에서 활용했었던 기술에 대해 기고를 하는 영광을 얻게되고 (내 서버에는 누가 들어오는 걸까 - Apache 액세스 로그를 Elastic Stack으로 분석하기, 여러차례 각종 개발 관련 행사에 참여하며 “난 언제쯤 저런 발표를 할수 있을까?” 하는 부러움이 무엇때문인지 “나도 할수있다”는 자신감으로 변화되어 2018 Pycon 행사에서 짧은 5분이였지만 급작스럽게 필자가 만들었던 토이 프로젝트에 대해 약 100~200여명 앞에서 간단히 소개하는 발표를 하게된다. (Pycon 라이트닝토크) 작년까지만 해도 전혀 생각하지도 못할 외부활동을 정말 다양하게, 그리고 두려움을 떨쳐내고 회사라는 울타리를 벗어나 바깥세상을 바라볼수 있는 눈을 얻는 좋은 기회였다고 생각이 든다. # 결론, 그래서 내년엔?숨가쁘게 달려온 2018년. 올해는 무엇보다 외부활동을 많이 하면서 기존에 갖고있던 주니어로써의 개발 마인드를 조금이나마 벗어나며 주니어와 시니어 사이의 포지션으로 한발자국 올라선 기분이다. 예전에는 회사내에 팀장님이나 선배 개발자분들이 시킨일을 하며 감을 받아 먹었다면, 지금은 그 감을 어떻게 따먹는지, 어떤 감이 더 맛있고 어떻게 따먹어야 더 효율적인지 스스로 일어서는 방법에 첫 단추를 낀것 같아 한편으로는 마음이 무겁지만 한편으로는 새로움을 경험하고 배운다는 것에 벅차오르기까지 한다. 또한 기존에는 일반적인 Web Framework인 Spring 만을 가지고 CRUD에 고심했다면 ElasticStack, kafka, RabbitMQ, Redis 등 새로운 기술들을 배우기 시작하면서 새로운것에 대한 두려움 보다는 호기심이 더 커서 스펀지마냥 습득할수 있었던것 같다.새해계획이라면 거창하게 들릴지 모르겠지만 다가오는 2019년엔 Coder 가 아닌 Programmer 가 되고 싶다. (관련 좋은 글) 막연하게 들릴지 모르겠지만 회사원이 아닌 개발자로써 나를 발전시키고 공유하며 서로 성장해 가는, 골을 직접 넣진 않지만 그 과정을 빌드업 하는 미드필더같은 역활을 할수있는 개발자가 되고 싶다. 이러한 계획을 달성하기 위해서는 내년에도 올 2018년을 계속 회고해가면서 잘못된 점을 고쳐나가고 잘한점을 상기하며 개발에 임해야 하지 않을까 싶다.2018년, 고생했다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"}]},{"title":"엘라스틱서치 12월 서울 밋업 후기","slug":"elastic-meetup-201812","date":"2018-12-13T13:27:02.000Z","updated":"2018-12-13T16:43:14.303Z","comments":true,"path":"2018/12/13/elastic-meetup-201812/","link":"","permalink":"https://taetaetae.github.io/2018/12/13/elastic-meetup-201812/","excerpt":"엘라스틱을 처음 접하게 된 건 2017년 여름 facebook 피드에 “Elastic Stack을 이용한 서울시 지하철 대시보드” 라는 링크를 보게 된 것부터인 것 같다. 그 당시 데이터 분석 및 자동화에 관심이 커지고 있던 찰나였는데","text":"엘라스틱을 처음 접하게 된 건 2017년 여름 facebook 피드에 “Elastic Stack을 이용한 서울시 지하철 대시보드” 라는 링크를 보게 된 것부터인 것 같다. 그 당시 데이터 분석 및 자동화에 관심이 커지고 있던 찰나였는데 키바나로 간단하면서도 아주 멋진 대시보드를 그릴 수 있다는 게 너무 흥미롭게 다가왔고 거기다 실시간으로 볼수 있다는 점에 공부를 시작하지 않을 수 없었다. 그렇게 이것저것 만들어 보기도 하고 한국 엘라스틱서치 커뮤니티 활동을 해오던 찰나 (최근들어 눈팅만 하고 있지만…) 올해 마지막 밋업을 한다고 하여 참여하게 되었다. # 여기어때 본사 방문강남에 위치한 여기어때 본사에서 밋업을 하게 되어 덕분에 다른 회사 구경을 할 수 있게 되었다. 예전 다른 IT 스타트업 밋업 행사에서도 느꼈던 부분인데 엄청나게 큰 시설은 아니지만 아기자기하게 회사의 색깔과 특징을 잘 살려놓은 인테리어가 인상적이었다. 그런데 생각보다 사람이 너무~ 많이 와서 약간 집중이 안 될것 같았지만 다행히도 자리를 잘 잡아서 세션을 듣는 데는 무리가 없었다. (정확하진 않지만 참석하신 분들 중의 절반 정도만 강의장에 들어오고 나머지는 밖에서 듣는 걸 보고 이런 IT 행사의 인기를 다시 한번 실감할 수 있었다.)여기어때 본사건물에서 엘라스틱 밋업을!여기어때 본사건물에서 엘라스틱 밋업을! # 엘라스틱서치 6.5 최신버전 소개 및 커뮤니티 회고행사 처음 세션으로 김종민 커뮤니티 엔지니어 분께서 엘라스틱의 최근 업데이트 정보와 커뮤니티 활동에 대해서 회고해주셨다. 내가 처음 엘라스틱서치를 접한 버전이 2.4였는데 벌써 6.5라니… 빨라도 너무 빠르다. 이번 버전에서는 한 클러스터에서 다른 클러스터로의 인덱스를 복제하는 방법인 Cross-cluster replication (클러스터 복제) 기능이 추가되었고 ODBC Client 추가, 자바 11지원 등 여러 가지 기능이 추가되었다고 한다.특히 키바나에서는 파일을 업로드하면 자동으로 분석해서 인덱싱을 해주는 기능도 생겼고 (파일 크기가 100메가 제한이라는게 살짝 아쉽긴 했다.) 캔버스, 스페이스 등 역시 키바나 라는 생각이 들 정도로 비주얼라이징을 한번더 업그레이드 한듯 하다. (다 사용할 수 있을까 하는 정도로… 엘라스틱 스택을 들어보기만 하던 함께 참석한 동기 녀석도 당장 해보겠다고 할 정도로…)다른 자세한 내용은 여기서 확인이 가능하다.너무나 빠른 버전업과 너무나 발빠르게 움직이는 사람들너무나 빠른 버전업과 너무나 발빠르게 움직이는 사람들 # 엘라스틱서치 활용사례스마일게이트 및 여기어때 에서 엘라스틱 서치를 활용한 사례를 발표해 주셨다. 하지만 아쉽게도 필자는 5.6 버전까지밖에 사용한 게 전부여서인지(그것도 일부 기능만) 전체 발표 내용을 다 이해를 하진 못했지만 구축하면서 생긴 문제나 삽질 경험담을 공유해주셔서 간접적으로라도 그때의 현장감(?)을 느낄 수 있어 좋았고, 한편으로 여태까지 나름 엘라스틱서치를 만져봤다고 약간의 자신감 반 자만심 반으로 생각했었는데 역시 세상엔 고수가 많구나 하며 다시 분발해야겠다고 다짐했다.스마일게이트 + 여기어때스마일게이트 + 여기어때 # 마치며커뮤니티 활동 회고 시간에 누가 페이스북 커뮤니티에서 “공유”라는 단어를 사용해서 게시글을 작성했는지 키바나로 보여주고 밋업에 온 사람이 있다면 5만원 여기어때 쿠폰을 준다고 했었다. 마침 키바나 대시보드 한쪽 구석에 필자의 이름이 보였지만 (예전에 나름 활발하게 질문도 하고 공유도 했던 적이 있어서…) 쿠폰을 받는구나 하며 기대를 하고 있었지만 아쉽게도 최근에 작성한 몇 분에게만 선물이 돌아갔다… 하지만 그 아쉬움도 잠시, 무작위로 추첨하여 또 쿠폰을 준다고 했는데 당첨이 되어서ㅎㅎ 감사하게도 쿠폰을 받는 기쁨을 누릴 수 있었다!! 역시 밋업의 마무리는 굿즈모음이지(?)역시 밋업의 마무리는 굿즈모음이지(?) 매번 이런 IT밋업에 참가 신청을 하고 참석하기 전에는 “아 귀찮다. 취소할까. 날도 추운데. 피곤한데” 하며 가기 싫었지만 막상 와보면 생각보다 많은 것을 배워가고 얻어 간다. (쿠폰을 받아서가 아니라…) 세션에 발표하시는 분들, 그리고 그 발표를 듣는 참석하신 분들의 눈동자에서 배움에는 끝이 없고 배워야 살아남는다는 걸 (특히 IT직군은 더…) 다시 한번 느끼고 생각할 수 있었던 좋은 시간이었다. 내년엔 엘라스틱으로 뭘 만들어 볼까! 새로워진 기능들 + 삽질 경험담을 내 것으로 만들어 보자!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"}]},{"title":"Jenkins 설치 치트키","slug":"jenkins-install","date":"2018-12-01T19:37:59.000Z","updated":"2018-12-02T21:07:55.951Z","comments":true,"path":"2018/12/02/jenkins-install/","link":"","permalink":"https://taetaetae.github.io/2018/12/02/jenkins-install/","excerpt":"“show me the money”, “black sheep wall”.어렸을적 스타크래프트라는 게임이 나오고서 입에 달고 살았던 치트키. 게임이 시작되고 해당 치트키를 입력하면 돈이 들어오거나 맵이 훤하게 보여 컴퓨터를 이기는데 도움을 주곤 했었다.","text":"“show me the money”, “black sheep wall”.어렸을적 스타크래프트라는 게임이 나오고서 입에 달고 살았던 치트키. 게임이 시작되고 해당 치트키를 입력하면 돈이 들어오거나 맵이 훤하게 보여 컴퓨터를 이기는데 도움을 주곤 했었다. 개발을 하면서 Jenkins는 나 대신 어떤 업무를 수행하는데 강력한 툴 중에 하나이다. (물론 만능이라는 소리는 아니지만…) 새로운 프로젝트가 시작되거나 개발도중 무언가 자동화를 하고 싶을 경우엔 Jenkins를 찾게 되는데 그럴때마다 설치를 하고 이런저런 설정이 필요하다.눈치를 챘을수도 있지만 이 포스트는 오로지 젠킨스 설치하는 방법을 아주 간단하고 핵심만 정리하고자 한다. 마치 치트키처럼.나중에 다시 보기위해 + 누군가 해당 포스트를 보고 도움이 되었으면 하는 바람으로. (물론 이 방법밖에 있는건 아니지만 필자는 아래와 방법을 사용하고 있다.) 우선 CentOS 환경에 Java가 설치되어 있는 상황이라 가정한다. 적당한 위치에 tomcat 다운 ( https://tomcat.apache.org/download-80.cgi ) 1wget http://apache.mirror.cdnetworks.com/tomcat/tomcat-8/v8.5.35/bin/apache-tomcat-8.5.35.tar.gz 압축 해제후 하위 폴더중 webapps로 이동 12tar -zxvf apache-tomcat-8.5.35.tar.gzcd apache-tomcat-8.5.35/webapps Jenkins 다운 ( https://jenkins.io/download/ ) 1wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war tomcat 하위폴더중 conf 폴더로 이동 1cd ../conf server.xml 수정 및 http port 확인 1234567vi server.xml&lt;Host&gt; 하위에 추가&lt;Context path=&quot;/jenkins&quot; debug=&quot;0&quot; privileged=&quot;true&quot; docBase=&quot;jenkins.war&quot; /&gt;port 확인&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;/&gt; 해당 서버의 ip와 위 port에 맞춰 url 입력후 jenkins 설치 1http://ip:8080/jenkins","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"Jenkins에서 파이썬 출력을 실시간으로 보고싶다면?","slug":"python-buffer","date":"2018-12-01T16:40:11.000Z","updated":"2018-12-02T21:08:50.974Z","comments":true,"path":"2018/12/02/python-buffer/","link":"","permalink":"https://taetaetae.github.io/2018/12/02/python-buffer/","excerpt":"필자가 운영하고 있는 Daily Dev Blog 라는 서비스는 매일 동일한 시간에 주기적으로 데이터를 크롤링 하고 사용자에게 메일을 발송하는 일련의 작업을 수행하고 있다. 헌데 예상하지 못한 부분에서 예외가 발생하게 되면 어떤경우는 메일 발송을 못한다거나 기존에 발송했던 데이터를 다시 보내는 등 정상적이지 못한 상황을 맞이하게 된다.","text":"필자가 운영하고 있는 Daily Dev Blog 라는 서비스는 매일 동일한 시간에 주기적으로 데이터를 크롤링 하고 사용자에게 메일을 발송하는 일련의 작업을 수행하고 있다. 헌데 예상하지 못한 부분에서 예외가 발생하게 되면 어떤경우는 메일 발송을 못한다거나 기존에 발송했던 데이터를 다시 보내는 등 정상적이지 못한 상황을 맞이하게 된다.메일이 하루라도 잘못오면 여기저기서 연락이 온다. 감사한 분들...메일이 하루라도 잘못오면 여기저기서 연락이 온다. 감사한 분들...이런저런 바쁜일들로 차일피일 미루다 마침 여유가 생겨 기존에는 Crontab 스케쥴로 파이썬 스크립트를 실행하던 것에서 Jenkins로 옮기는 작업을 했다. 젠킨스가 스케쥴링을 해주고 실행이력을 보여주며, 실시간으로 스크립트가 돌아가는걸 볼수 있을것 같다는 기대감에서이다. 위에서 이야기 했던 예외상황을 보다 빠르고 편하게 실시간으로 디버깅을 하기 위해서가 가장 컸다. # 당연히 될거라고 생각했으나…작업은 간단할꺼라 생각했다. 우선 Jenkins를 설치하고 기존에 스크립트 파일을 Jenkins Job으로 옮긴후에 적당한 코드 중간중간에 디버깅이 용이하도록 로그를 출력하게 해둔다음 스케쥴링만 걸어두면 끝이라고 생각했다. 하지만, 이렇게 간단하게 끝날것만 같았던 작업이 은근 귀찮은 작업이 될줄이야. 디버깅을 위해 로그를 출력하도록 해놨는데 모든 스크립트가 끝이 나서야 해당 로그가 출력되는 것이였다. 로그를 실시간으로 볼수 없다면 Crontab에서 Jenkins로 옮기는 이유가 크게 없게 된다. 실제로 아래처럼 코드를 작성하고 Jenkins Job을 실행시켜보면 다 끝나고서야 출력이 되는걸 볼수 있었다. (1초에 한번씩 5초동안 로그를 찍는 간단한 코드다.) 123456789import timeprint('start')for second in range(0,5) : print(second) time.sleep(second)print('end') 스크립트가 다 끝나서야 출력을 볼수 있다ㅠ 실시간으로 디버깅이 어렵다.스크립트가 다 끝나서야 출력을 볼수 있다ㅠ 실시간으로 디버깅이 어렵다. # 그럼 어떻게 해야할까?개발을 하면서 만나는 대부분의 문제들은 누군가 과거에 경험했던 문제였고, 이미 해결된 문제일 확률이 상당히 높은것들이 많다. 이번에도 역시, 갓 스택 오버플로우 : https://stackoverflow.com/questions/107705/disable-output-buffering 위 링크에서 알려준것처럼 해보면 다음과 같이 로그가 출력되는대로 젠킨스에서 볼수 있게 된다.콘솔환경에서의 디버깅은 로깅이 최고!콘솔환경에서의 디버깅은 로깅이 최고! 정리해보면 다음과 같은 방법이 있겠다. Execute Python script 을 활용하여 Jenkins 에 직접 코드를 작성하는 경우 print의 flush옵션을 활용 ( https://docs.python.org/3/library/functions.html?highlight=print#print ) 1print('hello', flush=True) 매번 print 가 될때마다 flush가 되도록 재정의 123456789101112131415import sysclass Unbuffered(object): def __init__(self, stream): self.stream = stream def write(self, data): self.stream.write(data) self.stream.flush() def writelines(self, datas): self.stream.writelines(datas) self.stream.flush() def __getattr__(self, attr): return getattr(self.stream, attr)sys.stdout=Unbuffered(sys.stdout) Execute shell을 활용하여 특정경로의 Python 파일을 실행할 경우 -u 옵션을 줘서 실행시킨다. ( python -u python_module.py ) 이렇게 두고보면 너무 간단한 작업인데 이런 방법을 모르는 상황에서는 작성된 Python Script를 Shell Script로 다시 감싸보거나 Python 코드를 쓰지 말까 까지 생각했었다… 삽질의 연속들… (Shell Script로 작성하면 바로바로 보였기 때문…) 다시한번 모르면 몸이 고생한다(?)라는걸 몸소 체험한 좋은…시간이였다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"Deview 2018 리뷰 (Day 1, Day2)","slug":"deview-2018","date":"2018-10-14T09:26:26.000Z","updated":"2018-10-14T16:21:54.824Z","comments":true,"path":"2018/10/14/deview-2018/","link":"","permalink":"https://taetaetae.github.io/2018/10/14/deview-2018/","excerpt":"회사 내에서도 대학시절 수강신청마냥 1분도 안되서 마감될 정도로 관심이 많았던 DEVIEW 2018. 다행히 클릭신공으로 운좋게 신청에 성공하였고 팀에서도 바쁜 시기였지만 감사하게도 보내주셔서 올해는 이틀 모두 다녀올수 있게 되었다.","text":"회사 내에서도 대학시절 수강신청마냥 1분도 안되서 마감될 정도로 관심이 많았던 DEVIEW 2018. 다행히 클릭신공으로 운좋게 신청에 성공하였고 팀에서도 바쁜 시기였지만 감사하게도 보내주셔서 올해는 이틀 모두 다녀올수 있게 되었다. 예전에는 연차가 올라가면 DEVIEW행사는 참여 안하겠지~라는 생각이 있었는데 그때는 단순 호기심에 참석을 하고 싶었다면 이번에는 뭐라도 배워오자라는 마음으로 신입 시절보다 조금더 성숙한 마음가짐과 자세를 가지고 참석을 하게 되었다. 다시 생각해보면 호기심만으로 세션들을 듣고 부스에서 나눠주는 굿즈를 조금이라도 더 받아와야지 하고 생각했던 신입시절의 생각이 틀린건 아니였지만, 말 그대로 기술행사이니만큼 가급적이면 세션에서 발표하는 내용을 내것으로 만들고 실무에서 또는 다른곳에서 활용할수는 없을까 하는 생각을 갖는게 보다 성장하려는 개발자로서의 자세가 아닐까 생각이 든다. (라고 멋드러지게 말하지만 세션내용의 절반이라도 이해하면 다행이겠지…) # 행사 시작 그리고 키노트10초만에 마감되었다는 소리가 있을정도로 올해도 여전히 관심이 많았던 DEVIEW 2018. 코엑스에 도착하고 등록을 한뒤 이곳저곳 부스들을 구경하기 바빴다. 이번에는 지난번과 달리 거의 네이버 서비스가 60~70%를 자리잡고 있었고(파파고, 지도, 클로바, 글로벌 광고 등등) 일반 기업에서는 얼마 오지 않았다.(내 기억으로 5~6개?) 개인적으로 여러 다양한 회사들이 함께하는 기술행사가 되었으면 하는 바램이 있었지만 회사를 선정하는데, 그리고 기타 사정들이 있을꺼라는 아쉬움을 뒤로하고 CTO님이 발표하시는 키노트를 들으러 메인강의장에 들어갔다. (자칫… 이것도 네이버 독과점(?) 이러면 할말이 없는데…ㅠㅠ)송창현 네이버 CTO님의 keynote송창현 네이버 CTO님의 keynote작년에는 거의 로봇잔치로 느껴졌는데 올해는 그 기술들의 융합(?)잔치 로 받아들여졌다. Ambient Intelligence 를 강조하시며 기술의 진정한 가치는 기술이 생활속으로 사라졌을 때 나온다라는 명언같은 말씀도 해주셨다. 연결 : 사물, 상황, 위치인식, 이해 발견 : 적시에 답, 추천, 액션제공 그리고 그와 관련된 네이버 서비스를 공개 하셨는데, 네이버 지도 Map API를 무제한/무료로 사용할수 있게 된다고 한다. (박수 유도하심 ㅎㅎ) 또한 이번에 가장 크게 바뀌는 네이버 모바일 홈 페이지인 그린닷, 지도 기술들의 종합 플랫폼인 xDM Platform(측위, 지도, 내비), 그리고 자율주행과 로봇에 대해 연구결과 그리고 앞으로의 방향성에 대해 정리해주셨다. 집에 돌아와서 검색좀 하다보니 테크수다에서 벌써(?) 영상을 하나 올린게 있어 공유해본다. 키노트를 다 듣고 작년에는 그런가보다 하고 별생각이 안들었는데 올해는 저런 기술들이 서비스 레벨까지 가는데 이렇다할 허들없이 사용자들에게 보여질수만 있다면 개발자로서 보다 더 큰 자부심을 가지고 기술개발에 정진할텐데… 하는 씁슬한 생각을 해보게 되었다. (물론 이런 부분들도 다 사정이 있을꺼라 생각이 들지만 안타까운건 감출수가 없을것 같다.) 이틀에 걸쳐 이런저런 다양한 세션들을 들을수 있어 좋았는데 몇몇 세션들은 기본지식이 없어 (AI, 머신러닝 등…ㅠ) 이해하기 힘들었다. 내년엔 이해할수 있도록 준비를 해서 오자며 또다짐을 하고… 그나마 조금이라도 이해할수 있었던 세션들 몇개만 정리해본다. # React Native: 웹 개발자가 한 달 만에 앱 출시하기React Native: 웹 개발자가 한 달 만에 앱 출시하기React Native: 웹 개발자가 한 달 만에 앱 출시하기 지난팀에서 아주 잠깐 React를 경험해보긴 했지만 거의 hello world 수준이였기 때문에 이 세션 역시 이해가 잘 되지 못했다. 하지만 필자처럼 이해를 잘 못하는 사람도 발표자가 전달하려는 목적이 무엇인지 알수 있을 정도로 전체적인 흐름은 조금이나마 이해를 할수 있었고 특히 개발하면서 좋았던 것이나 경험담을 알려주며 삽질공유를 해주는게 듣기 좋았다. React Native 는 빠른개발을 할수있고 코드공유가 쉬우며 개선이 쉽다는 장점이 있다고 한다. 또한 단기간에 크로스 플랫폼을 만들어야 할때 사용한다고 하니 나중에 참고해봐도 좋을듯 싶다. 발표자료 [121]React Native: 웹 개발자가 한 달 만에 앱 출시하기 from NAVER D2 # LINE x NAVER 개발 보안 취약점 이야기LINE x NAVER 개발 보안 취약점 이야기LINE x NAVER 개발 보안 취약점 이야기 버그바운티라는 신기한(?)프로그램에 대한 소개와 운영에 대한 내용을 발표해 주셨다. 가끔 사내에서도 버그를 잡으면 포상을 드려요 라는 글이 올라왔었는데 그때마다 손안데고 코풀려나 하는 비뚤어진(?)생각을 갖곤 했었다. 하지만 듣고보니 해커를 고용하는 가장 좋은 방법이면서도 회사의 보안을 지키는 가장 좋은 방법이라고 한다. 또한 잘 알려진 왠만한 기업들은 버그바운티 프로그램을 운영하고 있다고 한다. 비뚤어진 생각을 다시 고쳐 생각해보면, 해커에게는 취약점을 찾으며 해커 본연의 업무를 더욱더 발전시킬수 있고, 회사로써는 수만가지의 취약점을 관리하는 별도의 팀을 운영하는 비용보다 이러한 버그바운티라는 프로그램을 운영하면 선택과 집중을 하며 보다 효율적일것 같다는 생각이 들었다. 그러면서 어떠한 내용들로 보상을 해줬는가에 대해 소개를 해줬는데 가장 접하기 쉬운 XSS 공격이나 서버설정 미스로 인한 정보노출 등 아주 다양한 사례를 공유해 주셨다. 발표자료 [113]LINExNAVER 개발 보안 취약점 이야기 from NAVER D2 # 쿠팡 서비스 Cloud Migration을 통해 배운 것들쿠팡 서비스 Cloud Migration을 통해 배운 것들쿠팡 서비스 Cloud Migration을 통해 배운 것들 사내에서도 발표가 있었는데 그때 제대로 못들어서 다시 듣게 되었다. 지난 2년동안 서비스를 클라우드로의 이전을 하면서 마주쳤던 문제들과 해결책, 그리고 클라우드의 마이크로서비스가 만나면서 마주친 새로운 문제들과 정리했던 생각들에 대해 공유하는 발표였다. 클라우드 이전원칙 : 확장성 확보하기 위함, 무중단 이전, 고객에게 만족도에 영향이 없어야 함 공통배포 파이프라인 유지, 만든사람이 운영하는 문화, 장애관리 문화 특히 안정상태 찾기라는 제목으로 서비스의 건강도를 측정하는 대시보드 형태의 페이지를 만들었다는 부분에서 내가 하고있는 서비스에서도 API Status 페이지처럼 서비스 전반에 대한 건강도(?)를 대시보드 형태로 충분히 만들수 있지 않을까 하는 생각을 해보았다. 발표자료 [115]쿠팡 서비스 클라우드 마이그레이션 통해 배운것들 from NAVER D2 # Druid로 쉽고 빠르게 빅데이터 분석하기Druid로 쉽고 빠르게 빅데이터 분석하기Druid로 쉽고 빠르게 빅데이터 분석하기 여태 참석한 DEVIEW 세션들 중에 시작전에 줄이 가장 길었던 세션(행사장 전체 반바퀴를 줄서야 했던 ;;) 그만큼 사람들도 빅데이터 분석에 대해 관심이 많이 있다는걸 증명하였고 나역시 Day1, Day2 전 세션들 중에 이 세션이 가장 기대가 되었다. Druid는 분석용도로 만든 플랫폼이고 아파치 인큐베이터에 들어가 있을정도로 각광받고 있는 플랫폼 이라고 한다. 일반적으로 빅데이터를 분석하기 위해서는 Druid와 Elasticsearch, Apache kudu 가 비교대상이 되는데 발표자분은 각 플랫폼을 아주 다양한 각도에서 비교 분석하며 현 상황에서 가장 적합한 플랫폼을 찾기위한 노력한 부분을 보여주셨다. 필자는 기존에 Elastic Stack을 백지부터 홀로 터득한 경험이 있어 무슨차이가 있는지 궁금했는데 가장 큰 다른점은 join과 group by가 된다는 장점이 있다고 한다. (join기능은 자체 제공하지는 않지만 spark와 연동해서 해결했다고 한다.)또한 가장 관심갖고 봤던 비쥬얼라이징툴에 대해 소개해주셨는데 Imply UI소개를 듣고 감탄사가 절로 나왔지만 유료… 그라파나는 플러그인을 통한 다양한 그래프를 쉽게 표현이 가능하다고 하였고 Superset 은 표현할수있는 그래프가 가장 많고 권한관리도 입맛에 맞게 가능하다고 한다. 다만 너무 많아서 복잡할수도 있다고 … Metabase는 어플리케이션 방식이고 깔끔한 UI를 제공하지만 세부적인 설정은 불가능 하다는 단점이 있다고 한다.솔루션 선택을 할때 마냥 좋다고 사용하는것이 아니라 실제로 다양한 관점에서의 테스트를 해보고 서비스에 적절한 솔루션을 선택하는 좋은 발표를 들을수 있어서 너무 좋았다. 발표자료 [215] Druid로 쉽고 빠르게 데이터 분석하기 from NAVER D2 # 마치며나도 언젠간 스피커가 될수 있겠지...?나도 언젠간 스피커가 될수 있겠지...? 이번 Deview에서는 Facebook에서 알게된 POPit 저자분을 실제로 만나뵙기도 했고 지금은 S사에 계시는 우연히 만나게된 반가운 예전팀 형, 그리고 군 장교시절 필자의 소대원이 AI 스타트업 소속으로 부스를 운영을 하며 서비스 소개를 하고 있는 모습도 보고… 참 다양한 이벤트들이 많았던 행사였다. 사실 올해 Deview 발표자를 모집할때 주니어 개발자가 회사밖에서 성장하는 방법 같은 내용으로 발표를 해볼까도 생각했지만 지금생각해보면 좀더 준비를 철저히 그리고 깊게 해야겠다고 느꼈다.그리고 내가 하고있는 업무 즉, 서비스 운영/개발은 아무리 바쁘고 힘들어도 기본으로 해야하고 회사를 벗어나 신기술 또는 기존기술의 고도화 방법을 찾아서 공부하고 내것으로 만들어 나가야 하는건 예전이나 지금이나 변함이 없다는걸 다시한번 느낄수 있었던 좋은 행사였다고 생각한다. 이번에도 뒤통수 세게 맞고 간다…","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"deview","slug":"deview","permalink":"https://taetaetae.github.io/tags/deview/"}]},{"title":"2018 Pycon. 그리고 첫 발표를 하다.","slug":"pycon-2018","date":"2018-08-28T14:43:16.000Z","updated":"2018-08-28T17:30:18.324Z","comments":true,"path":"2018/08/28/pycon-2018/","link":"","permalink":"https://taetaetae.github.io/2018/08/28/pycon-2018/","excerpt":"IT관련 행사에 참여하면 여러가지 정보를 얻을수 있다. 개인적으로는 사실 정보를 얻기 위함보다 그곳의 분위기를 현장에서 몸소 느끼고 참여한 사람들의 눈빛을 보며 해이해진 마음가짐을 다시 다잡을수 있음이 가장 큰 목적이다. 그에 올해 Pycon도 하나의 전환점이 되길 바라는 마음으로 신청을 하게 되었다.","text":"IT관련 행사에 참여하면 여러가지 정보를 얻을수 있다. 개인적으로는 사실 정보를 얻기 위함보다 그곳의 분위기를 현장에서 몸소 느끼고 참여한 사람들의 눈빛을 보며 해이해진 마음가짐을 다시 다잡을수 있음이 가장 큰 목적이다. 그에 올해 Pycon도 하나의 전환점이 되길 바라는 마음으로 신청을 하게 되었다. # 등록https://www.pycon.kr얼리버드 등록을 한다고 Facebook에서 홍보를 하길래 그런가보다 했는데 잠깐 회사일에 집중하고 다시 보니 이미 매진이 되어있었다. 사실 Pycon 은 올해가 처음 가보는거라 인기를 실감할수 없었는데 이정도일줄은 상상도 못했다. (나중에 알게 된 사실이지만 올해가 가장 인원이 많았다고…) 그래서 나중에 진행되었던 일반표 등록은 휴대폰 알람까지 걸어두며 늦지않게 등록할수 있었다. 세부 일정들이 업데이트가 되고 어떤 세션을 들을까 고민하면서 간략 소개를 하나둘씩 보게 되었는데 Python을 만지며 평소에 궁금했던거나 재밌어 보이는 세션들이 너무많아 고민을 많이 했다. 한가지 아쉬운건 로그인 기반이 아니다 보니 (임시 로그인기반?) 내 시간표 설정하는게 없었다. 나는 별도로 적어서 갔지만 나중엔 그런 기능이 생겼으면 좋겠다. 2019년 Pycon엔 크롬 익스텐션으로 기능을 만들어 로그인 여부와 상관없이 몇시에 내가 어떤 세션을 들을건지에 대한 설정을 하고 이를 이미지로 캡쳐해서 출력/다운 받을수 있는 걸 만들어 보고 싶다. (그전에 미뤄뒀던 크롬 익스텐션 개발하는 방법부터 공부하자…) # 첫째날개인적으로 아침잠이 너무 많은데 알림이 울리기도 전에 눈이 떠졌고 행사장에 도착해보니 후원사 부스는 아직 텅텅 비어있었고, 밤새가면서 준비를 하셨는지 자원봉사자 분들은 여기저기 빈백에 누워(쓰러져) 자고 있었다. 그만큼 Pycon에 대한 기대가 컸나보다. 시간이 지나니 하나둘씩 사람들이 등록을 하며 오기 시작하였고 역시나 행사에 꽃중에 꽃인 후원사 부스에서 나눠주는 이벤트 상품들을 받기 바빴다.DIVE INTO DIVERSITY !!DIVE INTO DIVERSITY !!키노트를 시작으로 사람들은 각자 듣고싶은 세션에 참가하며 행사는 시작이 되었다. 전체적으로 기술의 난이도는 초급 수준의 발표였던걸로 느껴졌다. (물론 나는 초초초급도 안되는 꼬꼬마 수준이지만…) 대부분 Python으로 어떤걸 해봤고, 어떤 어려움이 있었고, 이러저러한 상황들을 만났으며, 요런 경우에서는 어떻게 하며 해결을 하였다는 등 기술을 활용한 “경험기”에 대한 내용들을 들을수 있었다.Pycon의 슬로건인 DIVE INTO DIVERSITY에 걸맞게 아주 다양한 주제로 흥미있는 발표내용들이였다. 기억나는 것들중에 인상깊었던 부분들을 정리해본다. 파이썬 문화(?)중의 하나는 몰라서 물어보는 사람에게 구글링을 하라기보다 직접 알려주라는 것이다. 배우고 싶다면 다른사람들을 가르치는것부터(알려주는것부터) 시작하라. 여성 개발자, 여성 발표자들도 점점 늘어나고 있다. 파이썬을 개발 현장(?)이 아닌 다른곳에서 사용한다면 작업 속도도 빠르고 얻어내는 가치또한 훨씬 더 방대하다. 엑셀로 할수 있는 작업을 파이썬으로 할수 있다. 파이썬의 다양한 라이브러리는 일상의 도움을 준다. 행사를 들으며 꼭 질문을 해야지 하는 마음을 갖고 있었는데 (그래야 오래 기억에 남으니) 마침 어떤 세션에서 궁금한게 있어 질문을 할수 있었다. (질문을 하니 파이썬 관련 책 선물도 받았다.^^)그리고 마지막 라이트닝 톡이라는 세션이 있었는데 여러 발표자들이 짤막하게 5분동안 하고싶은 이야기를 하는 세션이였다. 5분이라는 제한이 있기에 다들 쉽고 편하게 발표하는듯 보였으나 발표 자료나 발표내용을 보면 꼭 그렇게 간단하게 발표하는건 아니였다. 본 세션에서 말하기엔 다소 분량이 작은 알차고 깨알같은 발표도 있었고, 매년 Pycon 라이트닝톡에 발표하는게 목표이신 분도 있었다.발표를 들으면서 난 언제 저런자리에 가서 발표를 할수 있을까 하는 마음이 스쳐 지나갈때 쯤. “왜못하지? 나 파이썬으로 만든거 있잖아?” 라고 혼잣말로 궁시렁거리며 둘째날에 있는 라이트닝톡에서 발표하기로 마음을 먹고 서둘러서 참가 신청을 보냈다. 그러고서는 저녁을 먹고 집에 늦게 돌아와 새벽 3시넘어서야 발표자료를 완성하였지만 “발표” 라는 부담감때문에 어렵게 잠에 들었다. # 둘째날어제와는 달리 오늘은 잠을 많이 못자서 인지 늦게 일어나 첫 세션이 시작하고서 거의 끝날 즈음에 행사장에 도착하게 되었다. “괜히 발표 한다고 한걸까” 라는 생각이 들며 진행위 본부에 가서 발표 순서를 확인해보니 첫번째… 슬슬 머리가 아파오기 시작했다. 그래도 듣기로한 세션은 들어보고 싶어서 집중해서 세션들을 돌아가며 들었지만 머릿속에는 온통 “발표 발표 발표”라는 생각때문에 오히려 다른분께서 하시는 발표를 집중해서 듣지 못하였다. # 첫 발표라이트닝톡이 시작이 되고, 첫 타자로 단상위에 올라섰다. 어림잡아 200여명 정도 였던것 같다. (기분탓으로 많아 보였을수도…) 두근두근. 드디어 발표 시작. 마치 잡스 마냥 멋지게 발표를 하고 싶었지만 막상 올라가서 스피커를 통해 들려오는 내 목소리를 듣고 있자니 너무 떨렸다. (이럴줄 알았으면 청심환이라도 먹고 올라가는건데…) 잠깐 말을 안하고 있었는데 적막이 흐르고… 빨리 발표를 진행해야겠다 싶어 준비한 스크립트를 그대로 보고 읽기 시작한다… 마치 국어책 읽는것마냥…저땐 왜그렇게 떨렸는지...저땐 왜그렇게 떨렸는지...발표의 주제는 “파이썬으로 토이프로젝트 만들기” (a.k.a 기술블로그 구독서비스 홍보) 였다. 필자도 파이썬을 공식적(?)으로 배워본적도 없고 주어 듣고 구글링 한게 전부인 상태에서 왜 만들게 되었고 어떤식으로 만들었는지에 대해 발표를 했는데 사실 이 발표의 주된 목적중에 하나는 기술블로그 구독서비스 홍보였다. 조금이라도 구독자수를 늘리고 싶어서… 적어도 한 100명은 가입 하겠지 라는 생각으로…하지만 발표가 끝나고 구독자수를 보기위해 홈페이지에 접속해보니 서버가 다운;;; ssh 접속도 안되는 상황이였다. 부랴부랴 AWS콘솔에서 서버 재시작을 하고 겨우 살렸는데 아뿔싸! 동접에 대한 테스트 즉, TPS 측정같은 성능테스트를 전혀 하지 않은것이다. 발표도 너무 못하고, 홍보도 제대로 못하고ㅠㅠ 이렇게 발표가 끝나서 너무 아쉽다. (물론 준비 시간이 부족했던게 핑계지만 가장 크다.)발표자료유튜브 녹화영상 # 번외 (nGrinder를 성능테스트편)필자는 기술에 있어서 “개념만 아는 사람”과 “그 개념을 토대로 실제로 해본사람”은 하늘과 땅차이라고 생각한다. 예전 포스팅 한것중에 Apache 의 mpm 방식에 대해 정리한게 있었는데 (링크) 정리할때 한번 각 방식에 대해서 프로토타이핑이라도 해봤으면… 이런 사태(?)는 면했을것 같다. Apache 설치시 Default방식인 Prefork방식으로 설치되어 있었고 AWS Free tier 의 메모리는 1기가. 사용중인 메모리는 약 450메가였고 mpm방식에 대해 아무 설정도 하지 않았으니 당연히 사용자가 늘어나면 늘어날수록 메모리는 올라가고 KeepAlive 또한 기본값인 on으로 되어있어서… 예정된 서버 다운이였다. (이것도 포스팅을 했었다…ㅠㅠ링크)Apache 를 다시 Worker 방식으로 바꿔 설치하고 KeepAlive 또한 Off로 설정하고 nGrinder를 통해 성능테스트를 해보니 다음과 같은 결과를 얻을수 있었다.Prefork방식일때 테스트를 해보니 다운이 되었다.Prefork방식일때 테스트를 해보니 다운이 되었다.만약, 이걸 예상하고 Worker방식에 KeepAlive Off 가 되어있었다면? 홍보가 더 잘되었을것 같다는 생각을 뒤늦게 땅을 쳐가며 후회해본다. (이래서 개발 마지막 단계는 다양한 QA가 필요하다며…) # 마치며이틀간의 걸쳐 진행된 Pycon은 필자에게 많은 선물을 안겨 주었다. (이벤트 상품들을 많이 받아서가 아니라..ㅎㅎ;;)이번 파이콘에서 받은 선물들이번 파이콘에서 받은 선물들물론 주된 목적은 파이썬이라는 언어의 사용법과 가치, 확장성 등 다양한 세션을 통해 얻은 정보였지만 아무래도 첫 발표를 했던지라 필자에겐 라이트닝톡이 가장 기억이 남는다. 발표를 막상 해보니 별거 없다는 생각과 자신감이 생겼다. (물론 청중이 많아지고 시간이 길어진다면 또 다른 문제일수 있겠지만)내년엔 좀더 준비를 탄탄히 하고 발표도 여유롭고 부드럽게 해서 꼭 본 세션에 참가해 보고 싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"pycon","slug":"pycon","permalink":"https://taetaetae.github.io/tags/pycon/"}]},{"title":"자바 객체 복사하기 ( feat. how to use CloneUtils? )","slug":"how-to-use-cloneUtils","date":"2018-08-21T09:02:47.000Z","updated":"2018-08-21T14:53:28.968Z","comments":true,"path":"2018/08/21/how-to-use-cloneUtils/","link":"","permalink":"https://taetaetae.github.io/2018/08/21/how-to-use-cloneUtils/","excerpt":"자바(Java)로 개발을 하다보면 한번쯤 객체를 복사하는 로직을 작성할때가 있다. 그때마다 나오는 이야기인 Shalldow Copy 와 Deep Copy. 한국어로 표현하면 얕은 복사와 깊은 복사라고 이야기를 하는데 이 두 개념의 차이는 아주 간단하다. 객체의 주소값을 복사하는지, 아니면 객체의 실제 값(value)를 복사하는지.","text":"자바(Java)로 개발을 하다보면 한번쯤 객체를 복사하는 로직을 작성할때가 있다. 그때마다 나오는 이야기인 Shalldow Copy 와 Deep Copy. 한국어로 표현하면 얕은 복사와 깊은 복사라고 이야기를 하는데 이 두 개념의 차이는 아주 간단하다. 객체의 주소값을 복사하는지, 아니면 객체의 실제 값(value)를 복사하는지. 이 둘의 차이점을 소개하는 글들은 워낙 많으니 패스하도록 하고 이번 포스팅에서는 Deep Copy를 할때 org.apache.http.client.utils 하위에 있는 CloneUtils 사용법에 대해 정리 하고자 한다. 그냥 쓰면 되는거 아닌가? 라고 생각했지만 (별거 아니라고 생각했지만) 해보고 안해보고의 차이는 엄청컸고 사용할때 주의점이 몇가지 있어 정리 하려고 한다. 예제에 앞서 본 포스팅에서 사용할 객체를 간단히 정리하면 다음과 같다. (학교에서 학생 신상정보를 관리한다고 가정해보자.) 12345678910111213141516public class Student &#123; String name; // 이름 int age; // 나이 Family family; // 가족&#125;public class Family &#123; String name; // 이름 int age; // 나이 boolean isOfficeWorkers; // 직장인 여부&#125;public class PhysicalInformation &#123; int height; // 키 int weight; // 몸무게&#125; # 객체는 Cloneable interface 를 implement 해야하고 clone 메소드를 public 으로 override 해야한다.당연한 이야기가 될수도 있으나 CloneUtils를 사용하기 위해서는 해당 객체는 Cloneable interface 를 implement 해야한다. 그리고 나서 clone 메소드를 override 해야되는데 여기서 가장 중요한점은 외부에서도 호출이 가능해야하기 때문에 public 으로 override를 해야한다. (기본은 protected 로 되어있다.) 우선 간단히 객체를 생성하고 출력부터 해보자. (출력을 이쁘게 하기 위해 ToStringBuilder.reflectionToString을 사용하였다.) 12345PhysicalInformation physicalInformation = new PhysicalInformation();physicalInformation.height = 180;physicalInformation.weight = 70;System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE)); 결과는 당연히 1PhysicalInformation@5d6f64b1[height=180,weight=70] 이제 Cloneable interface 를 implement 하고 clone 메소드를 public 으로 override 한뒤, CloneUtils를 사용해서 객체를 복사해보자. 테스트를 하면서 Shalldow Copy도 해보자. 1234567891011121314151617181920212223242526272829303132333435363738394041// class settingpublic class PhysicalInformation implements Cloneable&#123; int height; int weight; @Override public Object clone() throws CloneNotSupportedException &#123; // public 으로 바꿔주자. return super.clone(); &#125;&#125;// test codePhysicalInformation physicalInformation = new PhysicalInformation();physicalInformation.height = 180;physicalInformation.weight = 70;PhysicalInformation physicalInformationShalldowCopy = physicalInformation;PhysicalInformation physicalInformationDeepCopy = null;try &#123; physicalInformationDeepCopy = (PhysicalInformation)CloneUtils.clone(physicalInformation);&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;// 원본System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE));// 얕은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationShalldowCopy, ToStringStyle.DEFAULT_STYLE));// 깊은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationDeepCopy, ToStringStyle.DEFAULT_STYLE));// 값 변경physicalInformation.weight = 80;physicalInformation.height = 170;// 원본System.out.println(ToStringBuilder.reflectionToString(physicalInformation, ToStringStyle.DEFAULT_STYLE));// 얕은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationShalldowCopy, ToStringStyle.DEFAULT_STYLE));// 깊은 복사System.out.println(ToStringBuilder.reflectionToString(physicalInformationDeepCopy, ToStringStyle.DEFAULT_STYLE)); 결과는 원본과 얕은 복사를 한것은 메모리 주소(?)가 같으나 깊은 복사를 한것은 데이터는 같지만 주소가 다르고 값을 변경해도 영향을 주지 않는다. (완전히 서로다른 객체인것을 증명) 1234567PhysicalInformation@1376c05c[height=180,weight=70]PhysicalInformation@1376c05c[height=180,weight=70]PhysicalInformation@1b4fb997[height=180,weight=70]PhysicalInformation@1376c05c[height=170,weight=80]PhysicalInformation@1376c05c[height=170,weight=80]PhysicalInformation@1b4fb997[height=180,weight=70] 만약 위에서 clone을 기본값인 protected로 override를 하게 되면 어떤 결과를 가져올까? 1234Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.PhysicalInformation.clone() at org.apache.http.client.utils.CloneUtils.cloneObject(CloneUtils.java:55) at org.apache.http.client.utils.CloneUtils.clone(CloneUtils.java:77) at com.Test.main(Test.java:16) 접근제한자에서 눈치를 챌수도 있었겠지만 접근을 할수없어 CloneUtils 이 리플렉션을 하는 과정에서 Exception을 발생한다. 꼭! public 으로 override를 해주자. # 객체 내에 clone이 안되는 변수는 별도 처리가 필요하다.객체 내에 있는 멤버 변수는 원시 변수(int, char, float 등) , Immutable Class (String, Boolean, Integer 등) 또는 Enum 형식일 때는 원본의 값을 바로 대입해도 되지만, 그렇지 않을 때는 멤버변수의 clone을 호출하여 복사해야 한다. 말로만 보면 무슨이야기 인지 모르니 예제를 보자. 12345678910public class Student implements Cloneable &#123; String name; int age; Family family; @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; Student 클래스에서 Cloneable 를 implements 하고 clone 메소드를 override 하였다. (여기서 구멍이 있다!!) 그다음 Family 클래스는 초기 그대로 두고 CloneUtils을 사용해서 객체를 복사하는 코드를 작성해보자. 12345678910111213141516171819202122Student student = new Student();student.name = \"taetaetae\";student.age = 20;Family family = new Family();family.age = 25;family.isOfficeWorkers = true;family.name = \"son\";student.family = family;Student studentDeepCopy = null;try &#123; studentDeepCopy = (Student)CloneUtils.clone(student);&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;// student 객체 복사여부 확인System.out.println(ToStringBuilder.reflectionToString(student, ToStringStyle.DEFAULT_STYLE));System.out.println(ToStringBuilder.reflectionToString(studentDeepCopy, ToStringStyle.DEFAULT_STYLE));// student 내 family 객체 복사 여부 확인System.out.println(ToStringBuilder.reflectionToString(student.family, ToStringStyle.DEFAULT_STYLE));System.out.println(ToStringBuilder.reflectionToString(studentDeepCopy.family, ToStringStyle.DEFAULT_STYLE)); Student 객체 안에 int, String, 그리고 별도로 만든 객체인 Family 가 있는 상황에서 복사를 해보자. 결과는 어떻게 나왔을까? 12345com.Student@1376c05c[name=taetaetae,age=20,family=com.Family@51521cc1]com.Student@deb6432[name=taetaetae,age=20,family=com.Family@51521cc1]com.Family@51521cc1[name=son,age=25,isOfficeWorkers=true]com.Family@51521cc1[name=son,age=25,isOfficeWorkers=true] 위에서 말했던 구멍의 결과를 볼수 있다. Student 객체는 주소값이 다른걸 보니 깊은 복사가 되었지만 그 안에 있는 Family 형 변수는 얕은 복사가 된것을 확인할수 있다. 위에서 말한것과 같이 clone이 안되는 경우는 (다시 말하면 원시변수나 Immutable Class, enum 등 clone을 지원하는 객체가 아닐경우) 별도로 clone이 되도록 설정이 필요하다. 그래서 Family 도 Cloneable 를 implements 하고 clone 메소드를 override 해준다음 최상위 객체였던 Student의 clone 메소드를 아래처럼 조금 수정해주고 나서 테스트를 해보면 이쁘게 깊은 복사가 된것을 확인할수 있다. 12345678910111213141516171819202122232425// 복사가 될수있도록 설정public class Family implements Cloneable&#123; String name; int age; boolean isOfficeWorkers; @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125;// 일단 복사를 하고, 멤버 객체 자체를 복사한 다음 대입 public class Student implements Cloneable &#123; String name; int age; Family family; @Override public Object clone() throws CloneNotSupportedException &#123; Student student = (Student)super.clone(); student.family = (Family)CloneUtils.clone(student.family); return student; &#125;&#125; 그러고 테스트를 해보면 내부 멤버 변수도 복사가 된것을 확인할수 있다. 1234com.Student@51521cc1[name=taetaetae,age=20,family=com.Family@1b4fb997]com.Student@28ba21f3[name=taetaetae,age=20,family=com.Family@694f9431]com.Family@1b4fb997[name=son,age=25,isOfficeWorkers=true]com.Family@694f9431[name=son,age=25,isOfficeWorkers=true] # 마치며멤버변수가 List, Map, Set 등 여러 유형으로 복잡하게 만들어졌을경우 각각 복사를 해주는 등 다양한 케이스에 유연하게 대응할수 있어야 하겠다.객체복사? 그거 그냥 하면 되는거 아냐? 라고 볼수도 있으나 입개발 하는 사람과 직접 구현해보고 내부 코드까지 들여다보는 수고를 하는 사람의 차이는 언젠간 분명히 드러날꺼라 생각한다. (필자가 그랬으니ㅠㅠ)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"cloneUtils","slug":"cloneUtils","permalink":"https://taetaetae.github.io/tags/cloneUtils/"},{"name":"java deep copy","slug":"java-deep-copy","permalink":"https://taetaetae.github.io/tags/java-deep-copy/"}]},{"title":"기술블로그 구독서비스 개발 후기 - 2부","slug":"daily-dev-blog-2","date":"2018-08-09T11:37:26.000Z","updated":"2019-02-17T13:23:21.686Z","comments":true,"path":"2018/08/09/daily-dev-blog-2/","link":"","permalink":"https://taetaetae.github.io/2018/08/09/daily-dev-blog-2/","excerpt":"1부에서는 기술블로그 구독서비스(이하 서비스)를 왜 만들게 되었고 어떤구조로 만들가에 대해 이야기를 해보았다면, 이번 포스팅에서는 만들면서 만나게 된 각종 트러블슈팅 종합세트(?)를 하나씩 풀어보고자 한다.","text":"1부에서는 기술블로그 구독서비스(이하 서비스)를 왜 만들게 되었고 어떤구조로 만들가에 대해 이야기를 해보았다면, 이번 포스팅에서는 만들면서 만나게 된 각종 트러블슈팅 종합세트(?)를 하나씩 풀어보고자 한다. 물론 개발을 하면서 아무 문제 없이 잘 되면 당연히 좋겠으나 잘되도 이상한게 개발이라는 세계가 아니던가.잘 안되면 문제, 잘 되도 문제 ㅠㅠ출처 : https://www.clien.net/service/board/park/9111495잘 안되면 문제, 잘 되도 문제 ㅠㅠ출처 : https://www.clien.net/service/board/park/9111495 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 지난 1부에서 이야기 했던것처럼 문제 - 해결, 문제 - 해결 식으로 나열해보고자 한다. 다소 글의 전개가 뒤죽박죽일수도 있겠지만 말 그대로 트러블슈팅 종합세트이니 독자들의 양해를 미리 구한다. - 트러블 슈팅 리스트 10시에 로직이 실행되었지만 메일을 11시 넘어서 받게 된다. 제목없는 글? 블로그 RSS파싱 오류? 간헐적으로 오류가 생긴다. 메일 내에 class를 적용하여 CSS 처리가 불가능하다. 메일을 보냈으나 스팸으로 처리된다. Elastic Stack을 사용할수 없다. 메일 보내는 발송속도가 너무 느리다. 구독해제가 아닌 자체 수신거부는 어찌 처리할까? # 10시에 로직이 실행되었지만 메일을 11시 넘어서 받게 된다.👉 해결방안 : Divide and Conquer본 서비스의 요구사항중 하나는 매일 오전 10시, 구독자들에게 어제 등록된 글을 수집하여 메일로 보내주는게 있다. 우선 로직은 다음과 같은 순서로 진행되게 개발하였고, jenkins 등 별도의 스케쥴러 관리 어플리케이션에 의해 할수도 있었으나 이 또한 심플하게 crontab 에 등록하여 매일 오전 10시에 실행되도록 하였다.12341. awesome-devblog 에서 블로거들의 RSS 피드를 조회한다.2. 어제 등록된 글이 있다면 리스트에 담는다.3. 조회가 끝나면 메일형식에 맞추어 html 문자열을 만든다.4. 만들어진 문자열을 가지고 등록된 구독자들에게 메일을 보낸다. 로직은 아주 간단했다. 데이터를 파싱하는 방법이나 메일형식에 맞추어 html문자열을 만드는 등 별도의 라이브러리를 사용하는 다소 복잡한 부분만 빼면 단순히 for문과 if문을 조합해서 로직을 구성할수 있었다. 헌데, 10시에 해당로직이 실행되었지만 최대 1시간이 지나고서야 메일을 받는 경우도 있었다. 이게 무슨일일까!?눈치를 챘을수도 있지만 RSS 피드를 조회하는 곳에서 오래걸린 것이다. 티스토리나 네이버등 다른 블로그들은 RSS를 읽고 파싱하는 속도가 그렇게 오래 걸리지 않았는데 (1초 이내) 유독 이글루스 블로그의 RSS파싱이 오래걸리는건 1분까지도 걸리던 것이였다. ( 참고로 RSS 파싱모듈, yaml 파싱모듈 을 사용했다. ) 아마 RSS의 형식이 약간 달라서 그런것 같긴 한데 그렇다고 이글루스 일 경우에 파싱을 다르게 하는건 좀 그렇고… 추후 이글루스가 아닌 또다른 파싱속도가 느린 블로그의 RSS를 만날수도 있기에 RSS 타입별로 예외처리를 하는건 좀 아닌것 같았다.이런저런 고민끝에 아주 간단하게도 임무(?)를 나누는식으로 해결 하였다. 즉, RSS를 읽고 메일에 보낼 데이터를 만드는 job 하나와 만들어진 데이터를 가지고 이메일을 보내는 job 으로 나눈뒤 RSS를 분석하는 job은 9시에, 메일보내는 job은 10시에 보내도록 해서 생각보다 아주 심플하게 문제를 해결할수 있었다.복잡하고 어려운 문제를 꼭 복잡하고 어렵게만 해결해야하는 법은 없는것 같다. 모로 가도 서울만 가면 된다라는 속담이 있지 않는가. # 제목없는 글? 블로그 RSS파싱 오류? 간헐적으로 오류가 생긴다.👉 해결방안 : 언제나 신경써야 하는 예외처리(try-catch)내가 만든 코드는 언제나 내 생각대로만 돌아갔으면 하는건 모든 개발자의 마음과 같다.흔한 IT 종사자들.deploy출처 : https://9gag.com/gag/a0Yxw4B/operations-team-before-leaving-for-holidays흔한 IT 종사자들.deploy출처 : https://9gag.com/gag/a0Yxw4B/operations-team-before-leaving-for-holidays하지만 그생각도 잠시 언제나 예외는 발생하기 마련. ( 물론 전혀 예외가 발생 안할수도 있으나 만약 발생하지 않았다 할지라도 발생할수 있는 가능성은 염두해둬야 한다. ) 파싱하는 과정에서 제목이 없는글로 온다거나, 가끔 RSS url 응답이 404 또는 503 인 경우가 있었다. 참고로 필자는 출근이 늦은편이라 아침마다 늦잠을 자곤 했는데 이 서비스를 만들면서 덕분에(?) 9시 에는 메일에 보낼 데이터가 잘 만들어 졌는지, 10시에는 메일이 잘 갔는지 확인을 하다보니 일찍 일어나는 습관이 길러졌다 -ㅁ- 이러한 경우는 예외처리를 해서 제목이 없는 경우엔 임의로 ‘제목없음’ 이라는 제목으로 만들어주고, RSS url 응답이 정상적이지 않는 경우엔 해당 블로그 RSS를 무시할수 있도록 try-catch (python에서는 try-except) 구문을 사용하여 로직 도중에 어떠한 이유라도 중단되는 일이 없도록 하였다.언제나 예외처리는 항상 신경쓰자. 물론 내가 생각한 예외가 전혀 발생하지 않더라도 예외처리를 하는 습관을 기르다 보면 호미로 막을것을 가래로 막는 불상사는 만나지 않을것 같다. # 메일 내에 class를 적용하여 CSS 처리가 불가능하다.👉 해결방안 : Inline Style 처리개인적으로 디자인을 신경쓰는 개발자(?)인지라 메일보낼때도 그냥 텍스트보다는 그럴싸한 형식으로 메일을 보내고 싶었다. 그래서 Bootstrap을 활용하여 이쁘장하게 만들었는데 막상 보내보니 설정한 class는 다 날라가고 날것의(?) html으로만 보내지는 것이였다. 좀 찾아보니 이메일 내에는 여러 부분에서 html요소들이 제한된다고 한다. ( 관련링크: 이메일 클라이언트 CSS지원 )그래서 찾다보니 다행히도 class로 설정된 html을 이쁘게 inline style 로 바꿔주는 서비스(Emogrifier)가 있어서 이를 활용하여 전부 inline style로 바꾸게 되었다. 정말 세상엔 능력자들이 너무 많다… # 메일을 보냈으나 스팸으로 처리된다.👉 해결방안 : 도메인 구입과 DKIM 설정메일 발송은 본 서비스에서 가장 중요한 부분이다. 이런저런 과정을 통해 파싱해서 메일 보낼 데이터를 만들었는데 메일을 못보내면 말짱 꽝이기 때문. 그래서 메일보내는 방법을 다양하게 생각해봤는데 얕은 지식과 더 얕은 경험으로 두가지 방법을 생각할수 있었다. 자체 SMTP 구축 Google SMTP 서버 활용 우선 처음엔 자체 SMTP 구축을 해서 메일을 보내게 되었다. 메일보낼때 시스템 부하가 발생하면 어쩌지 하는 걱정을 하였지만 그 걱정에 앞서 수신자의 이메일이 naver.com 인 경우는 메일이 잘 보내지는데 google.com 인 경우에는 스팸으로 메일이 가게 된것이다. 스팸이라… 전혀 생각하지 못한 예외가 발생하였다!이 서비스를 만들면서 가장 큰 위기였던것 같다. 우선 자체 SMTP를 구축해서 메일을 보내고 있었기에 어떻게든 설정을 통해 ‘난 스팸메일을 보내지 않는다’를 알리고 싶었다. 그에 찾아본 키워드로는 DKIM(DomainKeys Identified Mail)과 SPF(Sender Policy Framework)이 있었으나 … 귀차니즘인건지 열정의 부족인건지 서버에 셋팅하다 포기, 다시 셋팅하다 포기를 몇번을 한지 모른다.가장 큰 위기, 내가 이 서비스를 만들수 있을까? (정치적 메세지는 아닙니다.)출처 : https://news.sbs.co.kr/news/endPage.do?news_id=N1003101192가장 큰 위기, 내가 이 서비스를 만들수 있을까? (정치적 메세지는 아닙니다.)출처 : https://news.sbs.co.kr/news/endPage.do?news_id=N1003101192서버설정으로 스팸을 회피하기는 내 실력으론 부족한걸 인지하고, 다른방법으로 Google SMTP서버를 사용해보았더니 놀랍게도 너무 간단하게 스팸으로 발송이 안되고 정상적으로 메일을 발송할수 있었다. 그러나 그 행복(?)도 잠시 무료라는 키워드엔 언제나 제한이 있기 마련. 계정당 하루 최대 500개밖에 보낼수 없었다. 그렇다면 계정을 여러개 만들면 되는게 아닐까하고 서비스에 적용하기 앞서 계정을 여러개 만들어 테스트를 해보니 실 user당 하루 500개를 넘지 못하는걸 확인할수 있었다. (구글 SMTP를 사용하기 위해서는 항상 본인인증을 하는데 이게 제한 포인트인것같다.) 그럼 결국 Google SMTP도 정답은 아니고…처음 서비스를 만들때 어떠케든 비용이 들지 않는 선에서 만들고 싶었으나 투자하는 시간에 비해 퍼포먼스가 나오지 않아 결국 돈을 지불하고서라도 해야겠다는 마음으로 SMTP 호스팅 업체를 찾다 AWS에 SES라는 서비스를 발견하게 된다. 비용은 1,000건당 0.1달라이니 구독자가 1,000명이고 30일을 발송한다고 가정하면 한달에 약 3달라가 드는셈. 원화로 따지면 한달에 3천원인데 나중에는 이 비용조차 줄여볼수 있는 방안을 고려해봐야 겠다. 이는 3부에서 정리해본다.AWS의 SES를 활용하면 앞서 그토록 어려웠던 스팸메일 우회를 아주 간단하게 할수 있었고, 그러다 보니 도메인도 구매하게 되고 깔끔하게 스팸메일 우회를 처리하면서 도메인도 이쁘게(?) 만들수 있었다. ( 물론 도메인을 구입하면서 부가세 포함 13.2달라가 발생하였지만… 열정페이로 이또한 수익을 얻는 구조로 생각을 해봐야 겠다. 1년만 서비스 할게 아니였으니… )AWS의 도메인을 구입하고 AWS-SES를 통해 스팸을 우회하여 메일을 보내는 과정은 아래 포스팅을 참조하였다. (워낙 정리가 잘되어있어 별도로 정리하지는 않고, 링크를 공유하고자 한다.) http://jojoldu.tistory.com/246 https://insidestory.kr/11477 http://blog.naver.com/my0biho/220937119874 # Elastic Stack을 사용할수 없다.👉 해결방안 : 직접 만들어버리자!로깅은 서비스를 운영하는데 있어 하나의 무기가 될수 있다. 기본 서버에서는 아파치와 Elasticsearch, kibana를 동시에 돌릴만한 메모리가 안되었기에 (1G….) 서버 한대를 더 받고 Elasticsearch와 Kibana를 설치하였고 가입자수, 포스팅수 등 다양한 로깅을 보려했는데 몇일이 지나고 AWS에서 메일이 온다.너님, 이대로 가다가는 돈낼수 있으니 참고해!너님, 이대로 가다가는 돈낼수 있으니 참고해!한달에 사용할수 있는 서버운영 제한시간이 곧 다가온다는 이야기. 이것저것 검색해보니 Free Tier 일 경우 한달간 서버 한대를 24시간 사용하는데만 무료이고 그 이상은 과금이 나간다고 한다. 덕분에 한 3달라 정도 과금이 발생해버렸다. (정리하면, AWS Free Tier를 사용할경우 24시간 서버 운영시 한달에 한대밖에 사용하지 못한다. 서버를 중지했다가 다시 살리는 수고를 하면 여러대를 사용할수도 있긴 하겠다만 구지…)해서 몇일간 Elasticsearch에 로깅해둔 데이터는 눈물을 머금고 기존 회원들의 이메일 리스트를 담고있는 sqlite3으로 백업 한 뒤 추가로 받았던 Elasticsearch용 서버는 다시 반납을 하게 되고, 이 데이터를 어찌 볼수 있을까 고민끝에 tui.chart를 사용하여 뷰를 만들게 된다. (아 물론, billboard.js도 훌륭하다. 개인 취향차이 같다. … )오픈소스인 kibana오픈소스인 kibana직접 만든 뷰, 직접 만들어서인지 더 이뻐보이는건 기분탓이겠지직접 만든 뷰, 직접 만들어서인지 더 이뻐보이는건 기분탓이겠지로깅 뷰는 http://daily-devblog.com/log/view 에서 확인이 가능하다. (최근 일주일의 데이터) 이번에 로깅 뷰를 만들면서 느낀거지만 kibana는 정말 좋은 오픈소스 툴인것 같다. 데이터 양이 많으면 자동으로 군집화(?)해서 보여주고… 이런게 스크립트단에서 처리를 하려면 엄청 복잡… 다시한번 ElasticStack을 찬양하며… # 메일 보내는 발송속도가 너무 느리다.👉 해결방안 : Thread 활용앞서 메일 보낼 데이터를 만드는 job과 메일을 보내는 job을 나눠서 메일을 보다 빠르게 발송할수 있었는데, 보내야 하는 사람이 많아지다 보니 그만큼 속도가 늦게 보내지는것이였다. 즉, 메일 보낼 사람의 메일이 구독 신청을 빨리해서 메일링 리스트 앞에 있다면 10시 부근에 받는데 최근에 구독 신청을 해서(늦게 구독 신청을 해서) 메일링 리스트의 뒷부분에 있다면 앞사람 다 보내고 받아야 하는 이슈가 있었다. 다시말하면 순차적으로 메일을 보내다보니 사람이 많아질수록 그만큼 늦게 받는 사람이 발생할수 있다는 것.이 문제는 간단히 스레드를 활용해서 해결할수 있었다. 테스트하면서 캡쳐해둔게 있는데 첫번째 스크린샷은 한사람에게 메일을 그냥 10번 반복해서 보냈고 두번째 스크린샷은 스레드를 활용해서 보내보았다. 확실히 스레드를 활용하니 속도가 빠른걸 확인할수 있었다.순차적으로 발송했을 경우 : 약 30초 소요순차적으로 발송했을 경우 : 약 30초 소요스레드를 활용했을 경우 : 거의 1초 내외스레드를 활용했을 경우 : 거의 1초 내외물론 AWS-SES에서 처리하는 메일발송 시간에 따라 달라지겠지만 실제로 스레드를 활용한 메일을 전부 열어보면 거의 동시에 온것을 확인할수 있었다. (구독자가 500명이 되었을때 확인해본 결과 1초 이내에 AWS-SES로 메일을 보낼수 있도록 처리한것을 확인할수 있었다. ) # 구독해제가 아닌 자체 수신거부는 어찌 처리할까?👉 해결방안 : AWS 반송/거부 확인 메일미약하지만 메일 발송도 AWS-SES를 사용하다보니 이제는 비용이 발생하게 되었다. 따라서 비용을 최소화 하기 위해서는 쓸떼없는 메일(?)을 발송해선 안된다. 그래서 매일 보내는 메일 하단에 구독 취소룰 할수 있도록 링크를 만들어 뒀는데 이렇게 정상적인 구독취소가 아닌 메일 자체를 사용하고 있는 메일 시스템에서 거부를 할 경우. 이를 어떻게 알고 처리할수 있을까?(역시 AWS~ AWS~)이러한 부분도 AWS에서 가이드를 해주고 있다. (무려 한글 ㅠㅠ 감사…)https://aws.amazon.com/ko/blogs/korea/automatic-managing-bounced-emails-by-amazon-ses/ 물론 또다른 문제들도 있었으나 메이저급(?) 문제들만 다뤄봤다. 가장 컸던건 앞서 말한 메일 스팸처리 ㅠㅠ 도메인도 구입하고 메일보내는데 비용도 들지만 그간 고생하고 삽질했던걸 생각하면 하나도 아깝지 않다. (어쩔수 없는 자본주의일까?)마지막으로 3부에서는 이러한 서비스를 이제 어떤식으로 운영할꺼고, 어떻게 발전시켜 나갈것이며, 수익 모델은 어떻게 생각하고 있는지에 대해 이야기 해보고자 한다. (수익모델이라 하지만… 그저 이 서비스를 운영하는 정도?;;)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[]},{"title":"기술블로그 구독서비스 개발 후기 - 1부","slug":"daily-dev-blog-1","date":"2018-08-05T01:27:21.000Z","updated":"2019-02-17T13:23:26.182Z","comments":true,"path":"2018/08/05/daily-dev-blog-1/","link":"","permalink":"https://taetaetae.github.io/2018/08/05/daily-dev-blog-1/","excerpt":"이번 포스팅은 약간의 자투리 시간을 활용하여 이것저것 만져보다 만들게 된 Daily DevBlog(기술블로그 구독서비스)에 대해 이야기 하려고 한다.","text":"이번 포스팅은 약간의 자투리 시간을 활용하여 이것저것 만져보다 만들게 된 Daily DevBlog(기술블로그 구독서비스)에 대해 이야기 하려고 한다. 하나의 글에 관련 내용을 모두 담기에는 양이 많아서 읽는사람도 지루하고, 글을 쓰는 필자 또한 어불성설 할것같아 크게 3개의 시리즈로 나눠서 최대한 자세하고 현장감(?)있게 글을 써보려고 노력했다. 1부 : 왜 만들게 되었는가 그리고 어떤 구조로 만들었는가 2부 : 문제발생 및 Trouble Shooting 3부 : 앞으로의 계획과 방향성 글에 들어가기 앞서 최종 결과는 http://daily-devblog.com 에서 확인할수 있다. # 무엇이 나를 움직이게 했는가얼마전까지 오픈소스는 정말 실력있는 개발자나 유명한 사람들 말고는 금기의 영역(?)이라고 생각했었지만 최근 오픈소스 개발자 이야기 세미나를 다녀온뒤 마음속에 있었던 벽이 사라지는듯 했다. 세미나를 들으면서 ‘나도 무언가를 만들어 볼수는 없을까?’, ‘회사라는 명찰을 떼면 난 어느 수준에서 개발을 하고 있는 것일까?’ 등 여러 생각들이 머리를 멤돌다 개발자를 위한 글쓰기라는 글에서 기술블로그들을 모아놓은 awesome-devblog를 소개하는 글을 보게 되었고 내 머릿속에 정리안되던 그 생각들은 “이 데이터를 활용해서 무언가를 만들어보자!”로 귀결되었다. 다른 이야기 이지만, awesome-devblog 을 보고 당장 내 블로그도 등록해야지 했었는데 이미 등록이 되어 있었다;; 등록해주신 분께 감사하다는 생각이 들기전에 내 블로그가 누군가에게 보여지고 있구나 하며 새삼 놀라움이 더 컸다. # 요구사항과 도구 그리고 설계만들려고 생각해봤던 요구사항은 다음과 같다. 마치 회사에서 개발전 스펙을 정리하듯… 웹페이지를 활용해서 구독하고자 하는 사람들의 이메일을 수집할수 있어야 한다. 매일 전날 작성된 글을 수집하고 조합하여 구독하고자 하는 사람들에게 메일을 보낼수 있어야 한다. 위 두가지만 보면 너무 간단했다. 또한 기존에 사용하지 않았던 기술들을 사용해보면서 최대한 심플하게 개발하는것을 첫 개인 프로젝트의 목표로 하고 싶었다. 하여 생각한 아키텍처는 다음과 같다. 최대한 심플하게 설계해보자.최대한 심플하게 설계해보자. 데이터는 해당 github에 있길래 그냥 가져다 쓰려고 했으나 그래도 데이터를 관리하시는 분께 허락을 받고 사용하는게 상도덕(?)인것 같아 수소문끝에 연락을 해서 허락받는데 성공하였다. 데이터 사용을 허락해주신 천사같으신분...데이터 사용을 허락해주신 천사같으신분... 이 자리를 빌어 데이터를 사용할수 있도록 허락해주신분 께 감사인사를 표합니다. 홈페이지를 만들기 위해서는 이제껏 삼겹살에 소주처럼(응?) Java에 Spring을 사용해 왔었지만 이번엔 좀 다른 방식을 사용하고 싶었다. 물론 삼겹살에 맥주, 치킨에 소주를 먹어도 되긴 하지만...물론 삼겹살에 맥주, 치킨에 소주를 먹어도 되긴 하지만... 최근에 Flask라는 python기반 웹 프레임워크를 만져본 경험이 있어서 이렇다할 고민없이 빠른 결정을 할수 있었다. 또한 DB는 mysql 이나 기타 memory DB를 사용할까 했지만 이또한 심플하게 파일을 활용하는 sqlite3 을 사용하고자 하였다. # 웹서버_최종_수정_파이널_진짜_확정Flask를 활용하기 위해서는 당연히 웹서버가 필요했다. 처음엔 awesome-devblog에서도 사용하고 있던 https://www.heroku.com/ 를 이용해서 해보려 했으나 매일 구독자들에게 메일을 보내는 등 스케쥴러 기능같은건 구현하기 힘들었고 인스턴트 어플리케이션을 등록하는 형태라 사용자의 메일을 입력받고 저장하는 로직을 만들기는 어려워 보였다. (필자가 heroku를 너무 수박 겉핥기식으로 봐서 일수도 있다…)좀더 찾아보니 https://www.pythonanywhere.com/ 라는 제한적이지만 무료 서비스가 있었는데 웹콘솔도 지원하고 상당히 매력있어 보여서 이거다! 하며 개발을 시작을 했으나 (나름 도메인까지 그럴싸하게 만들었지만… http://dailydevblog.pythonanywhere.com/ ) 세상에 공짜는 없다는 말을 실감하며 앞서 말했던 요구사항을 완벽하게 구현할 수 없는 상황이였다.(request 제한, 스케쥴러 등록 개수 제한 등 보다 여러기능을 사용하기 위해서는 돈을 내고 써야…)마지막 희망으로 언제샀는지 서랍속 깊이 자고있던 라즈베리 파이를 꺼내서 공유기 DDNS설정을 하고 라즈베리안을 설치하며 웹서버를 위한 셋팅을 시도해보았으나 언제나 그렇듯 (시험공부 하기전에 책상 정리하고 괜히 방청소까지 하다가 피곤해서 자버리는듯한 느낌) 배보다 배꼽이 클것같아 이또한 진행하다가 중단하게 된다.결국 AWS에서 1년동안은 무료로 사용할수 있는 Free Tier 라는걸 발견하고 이참에 나도한번 사용해보자라는 마음을 가지고 과금되지 않게 조심조심 셋팅을 할수 있었다. 물론 뒤에서 이야기 하겠지만 약간의 과금은 필요했다ㅠ (나름 심도깊었던 고민을 한방에 해결해버리는 AWS 짱;; 이래서 AWS~ AWS~ 하는가 싶었다.) 돌고 돌다 킹갓엠페러제네럴 AWS를 만나게 된다.돌고 돌다 킹갓엠페러제네럴 AWS를 만나게 된다. # AWS와 DNS, 그리고 Elastic Stack까지AWS Free Tier 에서는 참 고맙게도 다른사람들이 접속할수있도록 공인 IP를 제공해 줬다. 그래서 아파치에 Flask를 연동까지 하고 최대한 심플하게 웹페이지를 작성하여 접속이 가능하도록 했는데 문제는 IP주소로 서비스를 하기에는 뭔가 2% 부족해보였다. 그래서 무료 도메인을 찾아다니던 도중 https://freedns.afraid.org/ 이라는 서비스를 찾고 결국 http://daily-devblog.mooo.com 라는 도메인으로 AWS에 설정된 공인IP를 연동시킬수 있었다. (지금은 redirect 시켜둔 상태, 2부에서 왜 도메인을 구입했는지에 설명할 예정이다.)또한 예전 조직장님의 말씀이 떠올라 서비스에서의 또다른 인사이트를 찾기 위해 EC2 서버 한대 더 발급받아 (한대에 전부 올리기에는 메모리가 부족했다;;) Elastic Stack 을 셋팅하여 로깅을 할수 있었고 이를 아래 그림처럼 키바나로 시각화 할수 있었다. (이 얼마나 아름다운가… 다시한번 Elastic Stack 에 무한 감동을;;) 각종 로깅을 통해 만들어낸 우아한 대시보드각종 로깅을 통해 만들어낸 우아한 대시보드 하지만 탄탄대로일것만 같았던 첫번째 개인 프로젝트는 여러가지 문제점들이 발생하였고 한 일주일동안 두세시간 자며 트러블 슈팅을 해야만 했었다. (업무시간을 피해가며 하다보니 시간이 나질 않았다…) 우선 여기까지, 서비스를 만들게 된 계기와 전체 구조에 대해 이야기를 해보았고 다음 2부에서 하게될 이야기는 이보다 훨씬더 복잡하고 피곤한(?) 이야기로 포스팅 될것같다. 이야기의 흐름이 문제 - 해결, 문제 - 해결 식의 java.util.Map 구조라고 해야하나… 2부를 기대해 본다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"aws","slug":"aws","permalink":"https://taetaetae.github.io/tags/aws/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"2018 오픈소스개발자이야기 후기","slug":"open-source-software-develpoer-story-review","date":"2018-07-01T01:00:00.000Z","updated":"2018-07-29T08:31:26.241Z","comments":true,"path":"2018/07/01/open-source-software-develpoer-story-review/","link":"","permalink":"https://taetaetae.github.io/2018/07/01/open-source-software-develpoer-story-review/","excerpt":"Facebook그룹들을 눈팅하다(?) OSS개발자 포럼에서 오픈소스 개발자이야기라는 주제로 세미나를 주최한다는 공지를 보게되었다. 언제부턴가 트랜드에 뒤쳐지지 않으려는 몸부림중 세미나같은 외부 개발 행사에 참여해보자는 마음으로 공지를 보자마자 홀린듯이 신청을 하게 되었고","text":"Facebook그룹들을 눈팅하다(?) OSS개발자 포럼에서 오픈소스 개발자이야기라는 주제로 세미나를 주최한다는 공지를 보게되었다. 언제부턴가 트랜드에 뒤쳐지지 않으려는 몸부림중 세미나같은 외부 개발 행사에 참여해보자는 마음으로 공지를 보자마자 홀린듯이 신청을 하게 되었고 세미나를 듣고 감흥이 가시기 전에 후기를 적고자 한다.(시간이 지나면 잊어버릴것만같은, 보고 들은 생생한 그 무언가를 얻었기에…) 비가오는 주말이였지만 많이 배우고 오자는 설레임을 갖고 서울 광화문 근처에 있는 한국마이크로소프트로 가게되었다. 말로만 듣던 MS사 로고를 보고 사람들이 하나둘씩 모이는걸 보니 뭔가 배울수 있겠구나 하는 기대감이 생겼다. 사실 오픈소스를 사용만 해본 입장이라 실제 오픈소스에 기여하시는 분들은 어떤 생각들을 갖고 계시는지가 가장 궁금했고 개발자인 나도 언젠간 오픈소스에 기여할수있지 않을까 하는 생각을 하며 발표를 들었다. # 회색지대 : 이상과 현실 - 오픈소스 저작권 / 신정규 님 오픈소스는 아무리 말그대로 Open이지만 오픈소스마다 다양한 저작권을 갖고있고 서로 쟁취하려는 싸움이 많이 발생한다고 한다. 그러다보니 어떠한 프로그램을 만들고 오픈소스화 시킬때도 라이센스의 종류를 잘 결정해야 추후 불이익을 당하는 상황을 모면할수 있다고 한다. 특허와 라이센스는 같은 말이면서도 다른데 아래 표처럼 각 상황에 따라 다른 부분을 확인할수 있었다. 특허 라이센스 권리발생 출원, 심사, 등록 창작과 동시 발생 권리내용 독점배타적 실시권 인격권/재산권 효력범위 아이디어의 동일성 표현의 실질적 유사성 첫 시간이기도 하였고 아무래도 주제가 끝장토론을 해도 안끝날 주제였던지라 정해진 시간을 넘길정도로 이야기를 많이 해주셨다. 특히 오픈소스 관련된 이야기는 사례를 이야기 해야 재밌다고 하셨는데 시간관계상 몇가지만 말씀해주셨다. 링크가 맞는지는 모르겠으나 첨부해본다. 엘림넷 vs 하이온넷 사건 EFM Networks 오라클 VS 구글 오픈소스 개발에 대해 단순하게 누구나 수정할수 있는 환경 이라고만 생각을 하고있다가 이런 복잡한 라이센스 문제가 나오니 약간 어려웠지만, 오픈소스의 생태계를 알고 발을 들이기 위해서는 어느정도의 히스토리는 알아야 겠다고 느끼게 되었다. # Elastic 에서 Remote 로 일하기 / 김종민 님 그전부터 Elastic 제품들을 실무에서도 사용해 왔었기에 개인적으로 오늘 발표중에 가장 궁금했었고, 관심이 있던 시간이였다. 발표에 앞서 어떻게 Elastic에 들어가게 되셨고 회사 소개를 간단히 해주셨는데 생각보다 어마어마한 회사다고 느낄수 있었다.(800여명중 한국엔 9명 / 네덜란드 출신 스타트업 인데 본사는 캘리포니아 마운틴뷰에 있고 등등)원격근무는 편하고 비용이 절약되는 장점이 있으나 동료들간의 유대감형성이 힘들거나 회의시 집중이 힘든 단점도 있다는 점을 말씀해 주셨다.시간관계상 몇가지 링크들을 소개해주셨는데 나중에 봐볼 생각이다. 침대에서 회사까지 1분 [마소 392호] 리모트 워크의 중심에 서보다 원격 툴로는 다음과 같이 사용한다고 한다. github : 슬랙연동, 개발뿐 아니라 운영/기획/이벤트 공유시 활용 Google Apps Slack : 봇 활용 (다양한 종류의 봇, 상황마다 특정 알림을 준다.) salesforce : CRM 툴 zoom : 화상회의 200명 동시콜 가능, 회의가 끝나면 녹음/녹화/스크립팅까지 가능하다고 한다 (wow) pinboard : 근태/인사 관리용 앱 jira는 사용 잘 안함 나중에 팀 내 Slack 봇으로 여러가지 다양한 자동화를 구성할수 있을것 같다는 생각이 들었다. # 오픈소스 생태계 일원으로서의 개발자 / 변정훈 님 사회자분이 “아웃사이더님”이라고 하시길래 설마 했다. 뭐가 잘 안되면 구글링을 하게되는데 내가 자주 보던 블로그를 운영하셨던 분이 내눈앞에 ㄷㄷ…언제부터 해야지~가 아니고 개발하다보니 어느새 오픈소스에 참여하고 있었다고 한다. 또한 참여하는게 아니고 이미 오픈소스 생태계속에서 살고있는 우리들이라 말씀하시고, 오픈소스 Contribution 방법으로는 사용/홍보/번역/리포팅/문서화/코드제출 등 다양하게 있으니 어렵게 생각하지 말자 라고 하셨다. 오픈소스에서 배울수 있는점은 커뮤니케이션의 방법, 협업의 방법과 중요성, 테스트코드의 중요성, 지속적 통합/지속적 배포, 코드의 품질관리 라고 한다.점점 발표를 들으면서 오픈소스에 대한 생각이 바뀌고 있는 내 자신을 느낄수 있었다. 너무 어렵게만 생각해 온것 같다는. # 해외 오픈소스 컨퍼런스 발표와 참여 / 송태웅 님 리눅스 파운데이션 이벤트에서 밸표했던 경험을 공유해주셨다. 발표에 앞서 나이가 어려보였는데 저렇게 세계적인 곳에 가서 발표를 할수있다는 용기와 대범함에 발표를 듣는 내내 놀라움을 감출수 없었다.특히 영어에 대해 이야기를 해주셨는데 영어지만 IT전문용어가 있기 때문에 어느정도 질문에 답변이 가능할수 있었고, 영어를 잘하는것보다 기술을 더 깊이 이해하는게 좋다고 한다.(물론 기본적인 영어실력은 필수) 그리고 인상깊었던 말중에 “세미나 같은곳에 가면 질문 하나는 꼭 하자, 그냥 듣는것보다 질문에 대한 내용은 오래 기억이 남으니까” 라는 말이 뒤통수를 쌔게 후려 쳤다. 하지만 오늘 질문은… 못했다. 다음엔 꼭 하리라 다짐을 하며… # 파이썬, 파이콘, 파이썬소프트웨어재단 / 김영근 님 PSF(Python Software Foundation)에서 하는 일과 간략한 파이썬에 대한 이야기를 해주셨다.여담으로, 파이썬 2.x는 파이썬 커뮤니티에서는 전전전 여친 수준으로 잊고 있으니 3.x를 사용하라는 말씀도 해주셨다. 존중과 다양성을 가치로 두고있고 예컨데, 필리핀에서는 어린아이도 발표를 하고 여성 발표자도 점점 늘어나는 추세라고 한다. 파이썬은 문제 해결 하는데 있어 적어도 두번째로 좋은 언어다 라고 자부할수 있고, 애니메이션ㆍ영화 에서도 파이썬을 사용하고 있다고 한다. 또한 스프린트 라는 행사를 개최하여 외부에서 사람들이 직접 Contribution을 할수 있도록 도와주면서 누구나 파이썬에 Contribution을 할수 있는 순환구조를 만든다고 한다.가장 인상깊었던 문구가 있었는데 사진은 못찍었고 대신 텍스트로 남겨본다.1234567동료 의식에서 시작되는 존중과 배려- 를 바탕으로한 소속감과 참여감- 에서 비롯되는 기여- 를 통해 발전하는 생태계- 에 모여드는 사람들- 사이에서 싹트는 동료의식(반복) 딱! 내가 바라는(?) 개발 문화다. 이번 파이콘 행사가 기대가 더욱더 기대가 된다! # 아파치 제플린, 프로젝트 시작부터 아파치 탑레벨 프로젝트가 되기까지 / 이문수 님 아파치 제플린은 실무에서도 자주 사용하고 있는 오픈소스이다. 그런데 이 발표를 하기위해 저멀리 미국에서 오셨다는 아파치 제플린의 CTO님. 개인적으로 정말 대단해 보였다. 특히 제플린을 만든 계기부터 아파치 탑레벨 프로젝트로 되기까지의 과정이 정말 매력적으로 보였는데 2013년 10월경 하둡관련 화면단의 분석시스템이 없어 불편해서 만들기 시작하였고, 이를 여러 커뮤니티에 직접 홍보를 해가며 인큐베이션을 거쳐 결국 아파치 프로젝트로 되었다는 이야기.왜 나는 못할까 라는 자괴감도 살짝 들었으나 저분의 끈기와 실행력은 높이 평가하고 싶고 또한 배우고 싶은 부분이였다. (외모는 여느 CTO처럼 거만하지 않았는데 말은 조리있게 잘하시고^^;) # 오픈소스 개발자에게 듣다(대담)질의응답 시간이였다.Q. 오픈소스에 쉽게 기여할수 있는 방법? Github에 가보면 초보자들을 위한 이슈를 남겨놓은 경우도 있다. 조그마한 문서 수정부터 시작하자. 기여는 코드만 있는게 아니다. (e.g. 버그리포트 등) Q. 오픈소스 홍수시대에 살고있는데 어떤것을 파야하나요? 유명한 코드는 너무 방대해서 보기 힘들다. 그걸 만든 사람의 토이 프로젝트를 보며 감을 익혀보자. Q. 처음으로 기여한 오픈소스는 무엇이며 어떻게 기여하였나? 문서수정, 간단한 텍스트를 수정하는것부터 시작 내가 사용하는 프로젝트를 관심있게 보다가 인사이트를 찾아 수정 번역 또한 오픈소스에 기여 Q. 오픈소스에서 활동하는것과 회사 업무와 비교시 장단점, 입사시 유리한가? 회사업무보단 재밌다. Github계정을 기준으로 평가하기도 한다. 오픈소스 컨튜리뷰션은 내 개발 역사라고 볼수있다. Q. 개발자라면 꼭 읽어봐야할 책이 있다면 추천 개발보단 사람과의 커뮤니케이션 관련된 책 기본서는 여러번 읽을수록 새롭다. 마치며전망좋은 MS, 사은품으로 받은 MS 티셔츠전망좋은 MS, 사은품으로 받은 MS 티셔츠 일명 네임드 개발자분들을 만나고 나니 한편으론 부럽기도 하였지만 한편으론 동기부여를 얻어간다. 첫불에 배부르랴, 조그마한것부터 시작하다보면 나도 언젠간 네임드 소리를 들으며 지금보다 조금이라도 발전된 내가 될수 있다는 자신감을 얻을수 있었던, 직장인으로써 황금같은 토요일이 아깝지 않을 만큼 정말 좋았던 시간이였다! (행사에 자주 와야겠다!)","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"oss","slug":"oss","permalink":"https://taetaetae.github.io/tags/oss/"},{"name":"open source software","slug":"open-source-software","permalink":"https://taetaetae.github.io/tags/open-source-software/"},{"name":"review","slug":"review","permalink":"https://taetaetae.github.io/tags/review/"}]},{"title":"초간단 API서버 만들기 - 2부 (Python + Flask + Nginx)","slug":"simple-web-server-flask-nginx","date":"2018-06-30T17:00:00.000Z","updated":"2018-07-29T08:31:26.442Z","comments":true,"path":"2018/07/01/simple-web-server-flask-nginx/","link":"","permalink":"https://taetaetae.github.io/2018/07/01/simple-web-server-flask-nginx/","excerpt":"지난포스팅에 이어 이번엔 Flask와 Nginx를 연동하는 방법을 정리해보고자 한다. Apache로 연동했는데 왜 또 Nginx로 연동하는걸 정리하지(?)하며 의문이 들수 있는데 다른 포스팅을 봐도","text":"지난포스팅에 이어 이번엔 Flask와 Nginx를 연동하는 방법을 정리해보고자 한다. Apache로 연동했는데 왜 또 Nginx로 연동하는걸 정리하지(?)하며 의문이 들수 있는데 다른 포스팅을 봐도 Apache + Flask 조합보다 Nginx + Flask 조합이 더 많고 지난 포스팅에서도 알수있었듯이 (Apache VS Nginx) 둘중 어느것이 좋다고 할수도 없고 각 상황에서 연동하는 방법을 알고 있다면 이 또한 나만의 무기가 될것같아 Nginx를 연동하는 방법을 정리해보려 한다. 1부에서 왜 Flask인가, Flask의 장점에 대해 정리를 했으니 이번 포스팅에서는 별도로 작성하진 않는다. # Nginx 설치 ( https://nginx.org/en/ )역시 소스설치를 한다.123456789101112- 다운을 받고$ https://nginx.org/download/nginx-1.14.0.tar.gz- 압축을 푼 다음$ tar -zxvf nginx-1.14.0.tar.gz- 폴더로 이동해서 $ cd nginx-1.14.0- 설치할 디렉토리를 설정하고$ ./configure --prefix=/~~~/apps/nginx- make 파일을 만들고$ make- 설치를 진행한다.$ make install 이렇게 하면 일단 Nginx는 설치가 되었다. # uWSGI 설치 ( https://uwsgi-docs.readthedocs.io/ )앞서 Apache와 연동할때는 별도의 모듈을 Apache에게 등록하는 형태였다면 Nginx는 WSGI프로토콜을 활용하는 WSGI 어플리케이션을 실행하는 어플리케이션 서버를 활용해야 한다.12345678- 다운을 받고$ wget https://projects.unbit.it/downloads/uwsgi-latest.tar.gz- 압축을 풀고$ tar zxvf uwsgi-latest.tar.gz- 폴더로 이동하여$ cd uwsgi-2.0.17- make 명령어를 호출하면 &apos;uwsgi&apos;이라는 실행파일이 생성된다.$ make # Nginx 설정Apache와 비슷하게 uWSGI 관련 설정을 해준다.123456789server &#123; listen 80; server_name localhost; location / &#123; # ( / ) 경로로 들어올 경우 include uwsgi_params; # GET/POST 등 기본적으로 필요한 환경변수를 include 해준다. uwsgi_pass 127.0.0.1:3031; # 요청을 IP:PORT로 전달한다. &#125;&#125; 별도의 모듈을 사용하지 않기때문에 전달해주는 (proxy느낌) 설정을 해준다. # uWSGI 실행 및 Nginx 재시작앞서 설치한 uwsgi를 아래처럼 IP:port 를 명시적으로 적어주고 (위에서 전달받은 IP:PORT와 동일하게) Apache 연동시 활용했던 wsgi파일을 이번에도 동일하게 사용하도록 해서 실행한다.1$ ./uwsgi -s 127.0.0.1:3031 --wsgi-file /~~~/python_app/hello_world.wsgi 이렇게 하면 background로 실행되는게 아닌 foreground로 실행되기 때문에 &amp;을 사용한다던지 해서 background로 실행되도록 해준다. 그후 Nginx를 재시작 해주면 원하는 그토록 원했던 Hello World!를 만날수가 있게 된다. Apache연동과 조금 다른점은 모듈을 사용하지않고 별도의 전달 어플리케이션(?)이 필요하다는점이다. 간단히 Apache처럼 모듈만 넣으면 되는게 아니라서 불편할수도 있을것 같지만 한편으로는 관리할수있는 포인트가 더 늘어난 셈이라 어떤 측면에서는 활용할수 있는 방법이 하나 늘어난것으로 볼수도 있다. # 마치며막상 정리하고 나면 아무것도 아닌데 알기 위해서 몸부림을 쳐가며 책이며 구글링을 하는 과정을 통해 점점 성장을 하는것 같다. (성장통이라고나 할까) 이렇게 단순히 Flask를 할수있다 가 아닌 웹서버를 연동할수있다. 그것도 Apache와 Nginx 두개나. 이것도 언젠간 나만의 무기가 되지 않을까?","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"https://taetaetae.github.io/tags/nginx/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"초간단 API서버 만들기 - 1부 (Python + Flask + Apache)","slug":"simple-web-server-flask-apache","date":"2018-06-29T14:00:00.000Z","updated":"2018-07-29T08:31:26.435Z","comments":true,"path":"2018/06/29/simple-web-server-flask-apache/","link":"","permalink":"https://taetaetae.github.io/2018/06/29/simple-web-server-flask-apache/","excerpt":"Static한 HTML이 아닌 로직이 필요한 API서버를 구성한다고 가정해보자. (이제까지 지식으로)처음 머릿속에 떠오르는건 Java를 사용하고 스프링으로 어플리케이션을 만들고 apache에 tomcat을 연동한 다음 …","text":"Static한 HTML이 아닌 로직이 필요한 API서버를 구성한다고 가정해보자. (이제까지 지식으로)처음 머릿속에 떠오르는건 Java를 사용하고 스프링으로 어플리케이션을 만들고 apache에 tomcat을 연동한 다음 … 이러한 방법으로 API서버를 구성할수 있겠지만 프로토타이핑 또는 테스트 목적으로 만들기 위해서는 설정하는 시간이 은근 많이 소요된다. (물론 Java Config, Spring Boot 등 간소해졌지만…)얼마전부터 Python에 대한 매력을 뼈저리게 느끼고 있다보니 Python으로 API서버를 구성할순 없을까 알아봤고 (모바일 게임 듀랑고 서버가 python이라고 하기도 하고…) Flask와 Django가 있어서 둘다 써본 결과 필자는 Flask가 맞겠다고 생각해서 정리를 해볼까 한다. ‘장고’라고도 불리는 Django에는 모든것들이 다 들어가 있어서 사용하기 너무 편리하다. (DB, 어드민 등 ) 하지만 Flask는 내가 사용할 것들만 import해서 사용하는 방식이라 어떤 측면에서는 아무것도 없다 할수 있겠으나 커스터마이징에 용이하다고 볼수 있었기에 Flask를 선택하게 되었다. (Django가 Flask보다 안좋다는 말은 아니니 오해는 하지 마시길…) 글쓰기에 앞서 본 포스팅은 2개의 포스팅에 걸쳐 시리즈(?)형식으로 작성할 예정이다. 1부에서는 Flask가 무엇이고 이를 어떻게 사용하며 Apache와 연동하는 방법을 소개하고, 2부에서는 Nginx와 연동하는 방법을 소개한다.환경은 다음과 같다. CentOS 7.4 Python 3.6 (기본은 2.7이였으나 추가로 설치) # Flask ( http://flask.pocoo.org/ )공식 홈페이지에서도 보면 알수 있듯이 너~무 간단하다. 단지 아래 코드 몇줄만 작성하면 우리가 모든 프로그램 초기 작성시 항상 만나는 “Hello World”를 볼수 있다.12345678#hello_world.pyfrom flask import Flaskapp = Flask(__name__)@app.route(\"/\")def hello(): return \"Hello World!\" 위와같이 작성하고 python hello.py로 실행해두고 브라우저에서 http://127.0.0.1:5000 을 요청하면 반가운 Hello World를 만날수 있다. (너무 간단;;) 자세한 문법은 도큐먼트를 참조하면 될듯하고 이 Flask를 잘만 활용한다면 보다 빠르고 간단하게 API서버를 구성할수 있을거라 생각한다. Hello World를 찍었으면 된거 아닌가 라고 질문할수도 있겠으나 실제 서비스에서 사용하기 위해서는 앞단에 웹서버를 두는게 여러 측면에서 효율적이다. 주로 사용하는 웹서버는 Apache 와 Nginx가 있는데 여기서는 Apache와 연동하는 방법을 정리 해보고자 한다. # Apache 설치 ( http://archive.apache.org/ )우선 필자는 yum 이나 apt-get처럼 패키지 관리자로 설치하는것을 그렇게 좋아하지 않는다. 이유는 커스터마이징을 할 경우 시스템 어느곳에 설치되어있는지를 한눈에 파악하기 어렵고 윈도우경우 Program Files처럼 내가 추가로 설치하고 관리하는 프로그램들을 한곳에서 관리하고 싶기에 왠만하면 소스를 직접 컴파일하여 설치하곤 한다. 이번 역시 아파치도 소스로 설치하려고 한다.현재 아파치는 2.4버전이 Stable버전으로 되어있지만 보다 레퍼런스가 많은 2.2버전으로 설치하기 위해 어렵게 아카이빙된 경로를 통해 다운을 받고 설치를 한다.123456789101112- 다운을 받고$ wget http://archive.apache.org/dist/httpd/httpd-2.2.29.tar.gz - 압축을 푼 다음$ tar xvzf httpd-2.2.29.tar.gz- 해당 폴더로 들어가$ cd httpd-2.2.29- 컴파일 후 설치 경로를 정해주고$ ./configure --prefix=/~~~/apps/apache- make 파일을 만든다음$ make- 설치를 해준다.$ make install 이렇게 되면 /~~~/apps/apache/ 하위에 필요한 파일들이 설치가 되는데 root계정이 아닌 일반계정으로 실행하기 위해서는 /bin하위에 있는 httpd에 대한 실행/소유권한을 변경해줘야 한다. (아니면 그냥 root권한으로 시작/종료. 왜? Apache는 80port를 사용하는데 일반적으로 리눅스에서는 1024 아래 port를 컨트롤 하기 위해서는 root권한이 있어야 사용이 가능, 그게 아니라면 이처럼 별도의 설정이 필요하다.) 12$ sudo chown root:계정명 httpd$ sudo chmod +s httpd # mod_wsgi 설치 ( https://code.google.com/archive/p/modwsgi/ )웹 서버 게이트웨이 인터페이스(WSGI, Web Server Gateway Interface)는 웹서버와 웹 애플리케이션의 인터페이스를 위한 파이선 프레임워크다. 라고 정의되어있다. 즉, 웹서버(Apache)와 위에서 만든 Flask 어플리케이션을 연동해주기 위한 프레임워크이다. 이또한 소스로 설치해보자. (위와 같은 이유로~)12345678910111213- 다운을 받고$ wget https://github.com/GrahamDumpleton/mod_wsgi/archive/3.5.tar.gz- 압축을 푼 다음$ tar -zxvf 3.5.tar.gz- 폴더에 들어가서$ cd mod_wsgi-3.5- 아파치의 빌드툴인 apxs의 경로를 설정해주고- 필자와 같이 기본 python 버전을 사용하지 않을꺼라면 꼭 python경로를 설정해줘야 한다! (중요)$ ./configure --with-apxs=/~~~/apps/apache/bin/apxs --with-python=/usr/bin/python3.6- make 파일을 만들고$ make- 설치~$ make install 이렇게 설치를 하면 자동으로 아파치 하위 /modules 폴더안에 mod_wsgi.so 파일이 생긴다. (필자는 이것도 모르고 mod_wsgi.so파일을 다운 받으려고 구글링을 몇일째 했던 기억이 ㅠ) # Apache httpd.conf 설정Aapche에 mod_wsgi 모듈이 생겼고, 이를 적용하기 위해서 httd.conf 아파치 기본설정 파일을 수정해야 한다.123456789101112- 모듈을 사용하겠다고 정의LoadModule wsgi_module modules/mod_wsgi.so- mod_wsgi로 실행한 WSGI 파이썬 어플리케이션을 특정 URL( / )로 설정하기 위해 다음과 같이 등록해준다.WSGIScriptAlias / /~~~/python_app/hello_world.wsgi- 등록된 파이썬 어플리케이션을 데몬 프로세스로 실행하기위해 다음과 같이 설정해준다.WSGIDaemonProcess hello_world(어플리케이션 명) user=계정명 group=계정명 threads=5(스레드 개수)&lt;Directory /~~~/python_app&gt; WSGIApplicationGroup %&#123;GLOBAL&#125; # 해당 어플리케이션을 처리하는 프로세스에서 첫번째로 생성된 파이썬 인터프리터를 사용 Order deny,allow Allow from all&lt;/Directory&gt; 자세한 내용은 공식 홈페이지를 참고하는걸 추천한다. # WSGI 파일 작성Apache 설정에서 등록한 wsgi파일은 다음과 같이 작성해준다. 물론 이 내용도 공식 홈페이지를 참조하는게 좋다.123import syssys.path.insert(0, '/~~~/python_app')from hello_world import app as application 필자는 여기서 한참을 해맸던 부분이, sys.path.insert구문의 두번째 인자는 실제 실행될 Falsk 어플리케이션이 있는 경로를 적어줘야 하고, hello_world는 Flask 어플리케이션 파일의 확장자를 제외한 이름을 적어주면 된다. 이렇게 한 후 아파치를 시작해주면 앞서 만든 Flask 어플리케이션을 별도로 실행해 주지 않아도 Apache가 알아서 설정한 URL로 요청이 들어올경우 Flask 어플리케이션으로 전달해준다. # 마치며사실 이렇게 Apache를 연동하면서 까지 하게 된 계기는, Flask 어플리케이션 구동시 80 port를 받도록 구현하고 root권한을 가진 계정으로 실행하도록 해뒀는데 가끔 종료가 되는 부분을 해결하고자 시작하게 되었다.이 밖에도 Flask의 다양한 기능들과 Flask만 제외하면 일반 Python 이기 때문에 활용성은 무궁무진할것으로 보인다. 2부로는 또다른 웹서버인 Nginx를 연동하는 방법을 알아보고자 한다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"flask","slug":"flask","permalink":"https://taetaetae.github.io/tags/flask/"}]},{"title":"Apache냐 Nginx냐, 그것이 알고싶다.","slug":"apache-vs-nginx","date":"2018-06-27T08:17:33.000Z","updated":"2018-07-29T08:31:26.079Z","comments":true,"path":"2018/06/27/apache-vs-nginx/","link":"","permalink":"https://taetaetae.github.io/2018/06/27/apache-vs-nginx/","excerpt":"웹서버는 HTTP 프로토콜을 통해 읽힐수 있는 문서를 처리를 하며 일반적으로 웹 어플리케이션의 앞단에 배치되곤 한다. 동적인 리소스는 WAS에게 처리하도록 하고 정적인 리소스를 보다 효율적으로 처리하기 위한 방법일수도 있다. 크게 Apache와 Nginx가 사용되곤 하는데 이 둘의 차이는 무엇일까?","text":"웹서버는 HTTP 프로토콜을 통해 읽힐수 있는 문서를 처리를 하며 일반적으로 웹 어플리케이션의 앞단에 배치되곤 한다. 동적인 리소스는 WAS에게 처리하도록 하고 정적인 리소스를 보다 효율적으로 처리하기 위한 방법일수도 있다. 크게 Apache와 Nginx가 사용되곤 하는데 이 둘의 차이는 무엇일까? 사실 필자는 사내에서 주로 Apache만 사용하다보니 Nginx는 그저 Apache와는 다른 방식의 웹서버다 또는 보다 경량화 되었다 정도로만 알고있었는데 이번기회를 통해 제대로 알고 비교를 해보면서 결국 어떤게 좋은지 알아보고자 한다. 구글링을 조금만 해보면 Apache와 Nginx를 비교하는 포스팅이 많이 나온다. 이번 포스팅의 목적 이러한 정보들을 단순히 요약/종합 하려는게 아니고, 최대한 실무 서비스를 운영하는 시각으로 정리하고자 함을 밝힌다. # Apache ( https://httpd.apache.org/ )우리나라에서 웹어플리케이션을 개발하는 사람들은 한번쯤은 들어봤을 Apache. 국내 일반적인 기업에서 웹서버의 표준으로 자리잡았다고 해도 과언이 아닐것 같다. Client에서 요청을 받으면 MPM (Multi Processing Module : 다중처리모듈) 이라는 방식으로 처리를 하는데 대표적으로는 Prefork와 Worker방식이 있다. 간단하게 어떤식으로 처리하는지 알고 넘어가자. Prefork MPMWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_htmlWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_html 실행중인 프로세스를 복제되어 처리가 된다. 각 프로세스는 한번에 한 연결만 처리하고 요청량이 많아질수록 프로세스는 증가하지만 복제시 메모리영역까지 복제되어 동작하므로 프로세스간 메모리 공유가 없어 안정적이라 볼수 있다. Worker MPMWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_htmlWorker MPM, http://old.zope.org/Members/ike/Apache2/osx/configure_html Prefork 동작방식이 1개의 프로세스가 1개의 스레드로 처리가 되었다면 Worker 동작방식은 1개의 프로세스가 각각 여러 쓰레드를 사용하게 된다. 쓰레드간의 메모리를 공유하며 PreFork방식보다 메모리를 덜 사용하는 장점이 있다. 참고로 WAS로 tomcat을 연동하는 경우라면 mod_jk, mod_proxy, mod_proxy_ajp 방식을 Apache 자체적으로 지원해주기 때문에 다양하고 효율적으로 tomcat을 연동할수 있다. 참고링크 # Nginx ( https://nginx.org/en/ )Nginx에 대해 살펴보기 전에 구글 트랜드를 활용하여 Nginx에 대한 관심이 어느정도인지를 보고 넘어가자.최근 5년간 구글트랜드, 파란색이 Apache이고 빨간색이 Nginx최근 5년간 구글트랜드, 파란색이 Apache이고 빨간색이 Nginx전세계는 Nginx보다는 Apache에 대한 관심이 많은것으로 보이는데 국내는 아주 조금씩 Nginx에 대한 관심이 오르는것을 볼수있었다. (그래도 아직은 Apache가 월등히 우세한 편이다.)그럼 Nginx는 어떤식으로 돌아가는 것일까? 가장 유명한(?) 특징이라면 Event Driven 방식을 꼽을수 있을것 같다. Event Driven 방식에 대해 잠깐 언급을 하고 넘어가면 요청이 들어오면 어떤 동작을 해야하는지만 알려주고 다른요청을 처리하는 방식이다. (Producer Consumer Pattern과 유사하다.) 그러다보니 프로세스를 fork하거나 쓰레드를 사용하는 아파치와는 달리 CPU와 관계없이 모든 IO들을 전부 Event Listener로 미루기 때문에 흐름이 끊기지 않고 응답이 빠르게 진행이 되어 1개의 프로세스로 더 빠른 작업이 가능하게 될수 있다. 이때문에 메모리적인 측면에서 Nginx가 System Resource를 적게 처리한다는 장점이 있다고 한다.Nginx Process Model (https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale)Nginx Process Model (https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale) # 그래서 뭐가 좋은가?그것이 알고싶다.그것이 알고싶다. 이 포스팅을 적으면서 마지막엔 Apache가 더좋다 또는 Nginx가 더좋다로 마무리를 짓고 싶었는데 어느 시사/교양 프로그램처럼 어쩔수 없는 열린결말로 마무리를 지을수밖에 없을것 같다. (어찌보면 이게 정답일수도?) 기술의 선택에 있어서 정답은 없는것 같다.(물론 Spring 을 사용하느냐 서블릿을 직접 구현하는냐 와는 좀 다른 성격의 이야기;;) 운영하고 있는 서비스의 상황을 잘 알고 튜닝을 해가면서 가장 효율적인것을 선택하는게 정답이라고 말할수 밖에… 커뮤니티 파워를 무시 못하기 때문에 Apache를 선택할수도 있을테고, 점점 관심도가 올라간다는건 그만큼의 장점이 있고 또한 메모리 측면에서 동접자 처리시 효율적인 Nginx를 사용할수 있을것 같다. 정리하면. 내가 사용하기에 어려움이 없는 도구를 잘 활용하는것, 그렇다고 오래된 기술이 편한다고 집착해서는 안되며, 새로운 기술을 두려워 하지말고 경험을 해본 뒤에 결정을 할것 이라고 내릴수 있을것 같다. (어렵다…ㅠㅠ) 참고 포스팅http://www.mukgee.com/?p=293http://knot.tistory.com/88http://tmondev.blog.me/220737182315http://tmondev.blog.me/220731906490http://urin79.com/blog/20654191http://jaweb.tistory.com/entry/apache-%EC%99%80-Nginx-%EB%AD%90%EA%B0%80-%EC%A2%8B%EC%9D%80%EA%B1%B0%EC%95%BC","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"nginx","slug":"nginx","permalink":"https://taetaetae.github.io/tags/nginx/"},{"name":"web server","slug":"web-server","permalink":"https://taetaetae.github.io/tags/web-server/"},{"name":"Event Driven","slug":"Event-Driven","permalink":"https://taetaetae.github.io/tags/Event-Driven/"},{"name":"Prefork MPM","slug":"Prefork-MPM","permalink":"https://taetaetae.github.io/tags/Prefork-MPM/"},{"name":"Worker MPM","slug":"Worker-MPM","permalink":"https://taetaetae.github.io/tags/Worker-MPM/"}]},{"title":"시계열 데이터를 분석하여 미래 예측 하기(Anomaly Detection)","slug":"anomaly-detection","date":"2018-05-31T08:03:41.000Z","updated":"2018-10-17T03:00:52.456Z","comments":true,"path":"2018/05/31/anomaly-detection/","link":"","permalink":"https://taetaetae.github.io/2018/05/31/anomaly-detection/","excerpt":"급변하는 날씨를 예측하려면 어떠한 정보가 있어야 할까?또는 마트를 운영하는 담당자인 경우 매장 운영시간을 정해야 한다면 어떠한 기준으로?뜨거운 감자인 비트코인 시장에서 수익을 얻으려면 어떤 정보들이 있어야 물리지(?) 않을수 있을까?","text":"급변하는 날씨를 예측하려면 어떠한 정보가 있어야 할까?또는 마트를 운영하는 담당자인 경우 매장 운영시간을 정해야 한다면 어떠한 기준으로?뜨거운 감자인 비트코인 시장에서 수익을 얻으려면 어떤 정보들이 있어야 물리지(?) 않을수 있을까? 위 질문에 공통된 정답은 예전 기록들인것 같다. 날씨예측은 기상청에서 과거 기록들을 보고 비가 올지 말지를 결정하고 ( 과거 날씨 서비스를 담당해봤지만 단순히 과거 기록들로 예측한다는건 불가능에 가깝긴 하다. ) 매장 운영시간은 예전에 손님들이 언제왔는지에 대한 데이터를 보고. 비트코인이나 주식은 차트를 보고 어느정도는 상승장일지 하락장일지 추측이 가능하다고 한다. ( 물론 호재/악재에 따라 흔들리지만..ㅠㅠ..?? )이처럼 시간의 흐름에 따라 만들어진 데이터를 분석하는것을 시계열 데이터 분석이라 부르고 있다. 필자가 운영하는 서비스에서 시계열 데이터 분석을 통해 장애를 사전에 방지하는 사례를 공유 해보고자 한다. # 상황파악부터손자병법에는 지피지기 백전불태 라는 말이 있다. 그만큼 현 상황을 잘 알아야 대응을 잘할수 있다는것. 필자가 운영하는 서비스는 PG(Payment Gateway) 서비스로 쇼핑몰같은 온/오프라인 사업자와 실제 카드사와의 중간 역활을 해주고 있다. 이를테면 사용자가 생수를 10,000원에 XX카드로 구매해줘 라고 요청이 오면 그 정보를 다시 형식에 맞춰 카드사로 전달하여 사용자가 물건을 구매할수 있도록 해준다.PG서비스 : 쇼핑몰과 카드사의 중간에서 릴레이 해주는 역활이라 보면된다.PG서비스 : 쇼핑몰과 카드사의 중간에서 릴레이 해주는 역활이라 보면된다. # 요구사항 및 과거 데이터 분석서비스를 운영해보니 감지하기 어려운 상황들이 있었다. 연동하는 쇼핑몰에서 문제가 발생하거나 네트워크 문제가 발생할경우 즉, 트래픽이 평소보다 적게 들어올 경우 정상적인 에러(e.g. 잔액부족) 가 갑자기 많이 발생할 경우 이를 분석하기위해 기존의 트래픽/데이터를 분석해봐야 했다.결제건수 Kibana Visualize, 기영이 패턴결제건수 Kibana Visualize, 기영이 패턴위 그래프는 결제데이터 카운트 인데 어느정도 패턴을 찾을수 있다.에러건수 Kibana Visualize, 악어 패턴..(무리수..)에러건수 Kibana Visualize, 악어 패턴..(무리수..)위 그래프는 에러카운트 인데 일정한 패턴 속에서 어느 지점에서는 튀는것을 확인할수 있다. (빨간색 영역) 그렇다면 어떤 방법으로 장애상황보다 앞서서 감지를 할수 있을까? ( 장애 : 어떠한 내/외부 요인으로 인해 정상적인 서비스가 되지 않는 상태 ) # 장애발생 전에 먼저 찾아보자!가장 간단하게는 기존 데이터를 보고 수동으로 설정하는 방법이 있을수 있다. 예로들어 자정 즈음에는 결제량이 가장 많기때문에 약 xx건으로 설정해두고, 새벽에는 결제량이 가장 적기 때문에 약 yy건으로 설정해둔 후 에러 건수나 결제건수에 대해 실시간으로 검사를 해가면서 설정한 값보다 벗어날 경우 알림을 주는 방법이다.하지만 아무리 과거 데이터를 완벽하게 분석했다 할지라도 24시간 모든 시점에서 예측은 벗어날 수밖에 없다. (예로들어 쇼핑 이벤트를 갑작스럽게 하게되면 결제량은 예측하지 못할정도로 늘어날테고…) 또한 설정한 예측값을 벗어날 경우 수동으로 다시 예측값을 조정해줘야 하는데, 이럴꺼면 24시간 종합 상황실에서 사람이 직접 눈으로 보는것 보다 못할것 같다. (인력 리소스가 충분하다면 뭐… 그렇게 해도 된다.) # 지난 데이터와 비교하기일주일 기준으로 지난 일주일과의 데이터를 비교해보는 방법또한 있다. 간단하게 설명하면 이번주 월요일 10시의 데이터와 지난주 월요일 10시의 데이터의 차이를 비교해보는 방법이다. 키바나에서 클릭 몇번만으로 시각화를 도와주는 Visualize 기능을 통해 지난 일주일과 이번주를 비교해보면 아래 그래프처럼 표현이 가능하다.일주일 전 데이터와 단순 비교일주일 전 데이터와 단순 비교이 경우도 지난주 상황과 이번주 상황이 다른 경우에는 원하는 비교 항목 외에 다른 요인이 추가되기 때문에 원하는 비교를 할수가 없고 위에서 수동으로 설정하는 방법과 별반 다를바 없을것으로 생각된다. # 조금더 우아하게! (언제부턴가 우아하단 말을 좋아하는것 같다..)개발자는 문제에 대해서 언제나 분석을 토대로 접근을 하는것을 목표로 해야한다. 언제부턴가 Hot한 머신러닝을 도입해 보고 싶었으나 아직 그런 실력이 되질 못하고… 폭풍 구글링을 통해 알게된 Facebook에서 만든 Prophet이라는 모듈을 활용해보고자 한다.https://opensource.fb.com/#artificial 이곳에 가보면 여러 Artificial Intelligence 관련된 오픈소스들중에 Prophet 모듈을 찾을수 있다. 다행히도 BSD License라서 실무에서도 다양하게 활용할수 있을것으로 보인다. 친절하게도 Quick Start을 통해 어떤식으로 예측을 하는지 보여준다. 참고로 Python 과 R 을 지원한다. (python 의 대단함을 다시금 느끼며…)구성은 CentOS 7 + python3.6 + jenkins 를 활용한다. (python 경험이 부족하므로 코드가 허접할수도 있으니 양해바란다.)데이터 분석시 가장 많이 사용된다는 Pandas와 대규모 다차원 배열을 쉽게 처리 할 수 있게 해주는 numpy, 그리고 bprophet를 비롯한 필요한 모듈들을 pip로 설치해준다.123pip install pandaspip install fbprophetpip install numpy 필요한 모듈들을 import를 한다.123456import matplotlibmatplotlib.use('Agg') // centos 서버 환경에서 돌릴경우 예측결과를 그래프로 보는 과정에서 오류가 발생한다. 이를 방직하기 위해 해당 코드를 적어준다.import pandas as pdimport requests, datetimefrom fbprophet import Prophet 기존에 데이터들은 모두 실시간으로 elasticserach에 인덱싱 중이기 때문에 rest api 를 활용하면 쉽게 데이터를 얻을수가 있다. (RDB로 관리를 했더라면… 배보다 배꼽이 더 큰 상황이였지 않았을까 하는 생각이 든다.)123456789101112131415161718192021# 현재 시 기준 2주 (24 x 14 시간) 전의 데이터를 가져온다.day_gap = 14now_datetime = datetime.datetime.now()end_time = datetime.datetime(now_datetime.year, now_datetime.month, now_datetime.day, now_datetime.hour, 0,0)end_time_stamp = str(int(end_time.timestamp() * 1000))start_time = end_time - datetime.timedelta(days=day_gap)start_time_stamp = str(int(start_time.timestamp() * 1000))url = 'http://elasticsearch:9200/_msearch'headers = &#123; \"Content-Type\" : \"application/x-ndjson\"&#125;es_data = '&#123;\"index\":[\"index_name\"]&#125;\\n&#123;\"size\":0,\"query\":&#123;\"bool\":&#123;\"must\":[&#123;\"range\":&#123;\"log_time\":&#123;\"gte\":' + start_time_stamp + ',\"lte\":' + end_time_stamp + '&#125;&#125;&#125;]&#125;&#125;,\"aggs\":&#123;\"2\":&#123;\"date_histogram\":&#123;\"field\":\"log_time\",\"interval\":\"1h\", \"time_zone\":\"Asia/Tokyo\"&#125;&#125;&#125;&#125;\\n'response_list = requests.post(url=url, headers=headers, data=es_data.encode('utf-8')).json()['responses'][0]['aggregations']['2']['buckets']print('기준 : ' + str(start_time) + ' ~ ' + str(end_time))# es 데이터 중 시간값을 형식에 바꾸면서 key-value 형태로 정제한다.query_result = &#123;&#125;for data in response_list : query_result[data['key_as_string'].replace('T',' ').replace('.000+09:00', '')] = data['doc_count'] elasticsearch에서 aggregation을 활용하여 데이터를 1시간 단위로 가져올경우 데이터가 없는 경우를 대비해 비어있는 시간에 해당하는 카운트를 0으로 맞춰주기 위하여 loop를 돌며 데이터를 정제해준다. (이 방법보다 더 좋은 방법이 분명 있을것이다…) 123456789anomaly_detection_base_data = &#123;&#125;for data in range(24 * day_gap) : key_date_time = start_time.strftime('%Y-%m-%d %H:00:00') if key_date_time in query_result : anomaly_detection_base_data[key_date_time] = query_result[key_date_time] else : anomaly_detection_base_data[key_date_time] = 0 start_time = start_time + datetime.timedelta(hours=1) pandas 를 활용하여 데이터를 DataFrame 형식에 맞춰준 다음 시간기준으로 다음 24시간의 데이터를 예측하도록 한다.12345df = pd.DataFrame(list(anomaly_detection_base_data.items()), columns=['ds', 'y'])m = Prophet()m.fit(df)future = m.make_future_dataframe(periods=24 , freq='H')forecast = m.predict(future) 실제 집계한 값과 예측값을 비교하여 알림발생 유무를 결정한다.123456789# 현재시간 -1시간 전 실제 집계check_datetime = now_datetime - datetime.timedelta(hours=1)check_datetime = check_datetime.strftime('%Y-%m-%d %H:00:00')check_datetime_count = query_result[check_datetime]print('실제 집계수 : ' + check_datetime + ' → ' + str(check_datetime_count))# 예측 최대치forecast_max_count = forecast[forecast.ds == check_datetime]['yhat_upper'].values[0]print('예측 최대치 : ' + check_datetime + ' → ' + str(forecast_max_count)) 여기서는 에러 카운트가 평소와는 다르게 발생할 경우에 알림을 발송을 하려 하기때문에 아래처럼 구성을 해준다.12345if check_datetime_count &gt; forecast_max_count : # 예측한 결과를 이미지로 저장후 알림에 포함시켜준다. forecast_graph = m.plot(forecast, plot_cap=False, xlabel='time', ylabel='count'); file_name = 'forecast_graph.png' forecast_graph.savefig(file_name) 이런 python script를 jenkins 를 활용하여 1시간에 한번씩 실행될수있도록 구성해 두고 문제가 발생할때는 아래처럼 메일로 알림을 받는다.예측결과예측결과위 그래프를 분석하면 다음과 같다. 검정색 점 : 실제 데이터 파란 실선 : 트랜드 그래프 하늘색 음영 : 예측한 최대/최소치의 영역빨간색으로 표시한 점 두개를 보면 예측을 벗어난것을 확인할수 있고 실제로 저 시간대에 내부적 이슈로 인해 에러가 많이 발생한 경우이다. (장애 발생 시점) 이런식으로 구성을 해두면 앞서 수동으로 최대/최소 수치를 정해두고 이탈했는지에 대해서 모니터링 하는것보다 훨씬더 우아하게 모니터링을 할수있다.물론 해당 모듈이 완벽하다는 소리는 아니다. 필자도 해당 모듈을 사용하다보니 어떤 경우에서는 예측수치와 실제 데이터가 비슷한 경우가 있어 알람을 받은 경우도 있다. 예전 포스팅에서도 이야기 했듯 정답은 없는것 같다. 상황에 맞춰서 지속적으로 커스터마이징을 해야하는게 개발자의 숙명 아닐까 싶다. # 마치며사실 이렇게 시계열 데이터라는 단어나 Prophet같은 오픈소스를 검색할수 있었던건 작년 Elastic{ON}Tour 에서 봤던 Elastic X-pack의 머신러닝 기능 때문이다. 그때 받은 충격(?)이 아직까지 있는건지 이렇게 오픈소스로 비슷하게나마 구현할수 있었던 원동력이 된것 같다. 기회가 되면 X-pack을 구매해서 운영하는 서비스에 머신러닝을 활용해서 이러한 시계열 데이터에 대해 분석해보고싶다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"anomaly detection","slug":"anomaly-detection","permalink":"https://taetaetae.github.io/tags/anomaly-detection/"},{"name":"prophet","slug":"prophet","permalink":"https://taetaetae.github.io/tags/prophet/"},{"name":"facebook","slug":"facebook","permalink":"https://taetaetae.github.io/tags/facebook/"}]},{"title":"아파치 엑세스 로그에 408코드가?","slug":"apache-408-response-code","date":"2018-04-29T08:39:36.000Z","updated":"2018-07-29T08:31:26.042Z","comments":true,"path":"2018/04/29/apache-408-response-code/","link":"","permalink":"https://taetaetae.github.io/2018/04/29/apache-408-response-code/","excerpt":"예전에 아파치 로그를 엘라스틱 스택을 활용하여 내 서버에 누가 들어오는지를 확인할수 있도록 구성을 해두고 몇일간 지켜보니 다음과 같은 엑세스 로그가 발생하고 있었다.","text":"예전에 아파치 로그를 엘라스틱 스택을 활용하여 내 서버에 누가 들어오는지를 확인할수 있도록 구성을 해두고 몇일간 지켜보니 다음과 같은 엑세스 로그가 발생하고 있었다.1234567891011121314151617181.2.3.4 - - [26/Apr/2018:01:27:33 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 6001 30788 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 28 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 12 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:08 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:28:50 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5999 13521 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:14 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5996 19437 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:15 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5997 17553 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:15 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 5998 17429 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 32 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 38 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:29:53 +0900] &quot;-&quot; 408 - 29 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:30:54 +0900] &quot;GET /aaa/ HTTP/1.1&quot; 200 6000 17881 &quot;http://www.naver.com&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 30 &quot;-&quot; &quot;-&quot;1.2.3.4 - - [26/Apr/2018:01:31:34 +0900] &quot;-&quot; 408 - 25 &quot;-&quot; &quot;-&quot; 한시간에 만건이상 응답코드는 408, referrer도 없고, useragent도 없는, ip들도 매우 다양한 이상한 녀석들이 요청되고 있었다. 이렇게 엑세스 로그를 분석할수 있는 구성을 해두고 나니 보였지 안그랬음 그냥 지나갔을 터.. 이러한 데이터를 키바나에서 보면 아래처럼 볼수있는데 한눈에 봐도 과연 의미있는 요청들일까? 하는 의구심이 들정도이다. (1시간 아파치 엑세스 로그)주황색이 408응답주황색이 408응답 그럼 이런 호출들은 도대체 뭘까? 천천히 생각좀 해보자. 정상적이지 않는 호출로 우리 서버의 취약점을 파악하려 하는것들일까? 응답코드 408은 요청시간초과 응답코드인데… 오히려 클라이언트 입장에서 문제가 있는건 아닐까? 어플리케이션 로직이 잘못되어 무한루프에 빠졌나; 위키백과에서는 아파치 응답코드 중 408에 대한 응답을 다음과 같이 알려주고 있다. The server timed out waiting for the request. According to HTTP specifications: “The client did not produce a request within the time that the server was prepared to wait. The client MAY repeat the request without modifications at any later time 즉, 아파치 단에서 타임아웃을 내버리는 상황. 여러 다양한 키워드들로 구글링을 해봐도 이렇다할 검색결과를 찾지 못하고 네트워크 관련상황인지 싶어 크롬 개발자도구를 열어 네트워크 지연 테스트를 해보았으나 별 효과가 없었다. 그렇게 범인찾는 형사의 심정으로 이것저것 알아보다 우연히 집에서 원격으로 회사 VPN 붙어서 테스트 하던도중 관련 증상을 재현 할수 있게 되었다. # 재현상황우선 아파치버전은 2.2이고 KeepAlive Off가 되어있는 상황. 아래그림처럼 집PC - 공유기 - VPN - Apache - tomcat jenkins 상황이였는데 젠킨스에 한번 접속후에는 항상 408 응답이 주루룩(?) 발생하는것을 알수 있었다. (사실 맨위에 엑세스 로그가 재현한 엑세스 로그이다.)요청의 흐름요청의 흐름 # 결론정확한 근거를 추론할수는 없었지만 재현한 바에 의해 결론을 내리자면, 특정 네트워크 환경에서 408응답이 발생하는것을 알수 있었다. (그중에 하나가 공유기 환경이라는 점)해당 요청을 막기에는 너무 다양한 ip들이고 서비스 특성상 keepAlive off로 설정한점을 미루어 볼때 해당 요청을 deny 시키기에는 다소 무리가 있다고 보여진다. 또한 시스템에 영향을 줄 정도가 아니므로 무시하는것으로 결론을 내렸다. 결과적으로는 그냥무시로 끝났지만 그래도 알고 무시해야지~ 하는 마음으로 재현까지 해볼수 있었던 좋은 경험이였다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"408","slug":"408","permalink":"https://taetaetae.github.io/tags/408/"}]},{"title":"내 서버에는 누가 들어오는걸까? (실시간 user-agent 분석기)","slug":"apache-access-log-user-agent","date":"2018-04-10T14:20:19.000Z","updated":"2018-07-29T08:31:26.056Z","comments":true,"path":"2018/04/10/apache-access-log-user-agent/","link":"","permalink":"https://taetaetae.github.io/2018/04/10/apache-access-log-user-agent/","excerpt":"Desktop 및 스마트폰의 대중화로 다양한 OS와 브라우저들을 사용하게 되었다. 이때, 내가 운영하는 웹서버에 들어오는 사람들은 무슨 기기로 접속을 하는 것일까? 혹여 특정 OS의 특정 브라우저에서만 안되는 버그를 잡기 위해 몇일밤을 고생하며 겨우 수정했는데… 과연 그 OS의 브라우저에서는 접속은 하기나 하는걸까? (ㅠㅠ)","text":"Desktop 및 스마트폰의 대중화로 다양한 OS와 브라우저들을 사용하게 되었다. 이때, 내가 운영하는 웹서버에 들어오는 사람들은 무슨 기기로 접속을 하는 것일까? 혹여 특정 OS의 특정 브라우저에서만 안되는 버그를 잡기 위해 몇일밤을 고생하며 겨우 수정했는데… 과연 그 OS의 브라우저에서는 접속은 하기나 하는걸까? (ㅠㅠ) 만약, 접속 사용자의 Device 정보를 알고있다면 고생하며 버그를 잡기 전에 먼저 해당 Device 사용율을 체크해 볼수도 있고(수정이 아닌 간단한 얼럿으로 해결한다거나?) 비지니스 모델까지 생각해야하는 서비스라면 타겟팅을 정하는 등 다양한 활용도가 높은 것이 바로 User-Agent라고 한다(이하 UA). 일반 Apache 를 웹서버로 운영하고 있다고 가정을 하고 어떻게 분석을 할수 있었는지, 그리고 분석을 하며 좀더 우아한(?) 방법은 없는지 알아 보고자 한다. # User-Agent가 뭐야?백문이 불여일타(?)라 했던가, 우선 http://www.useragentstring.com 를 들어가보자. 그러면 자신의 OS 및 브라우저 등 정보를 파싱해서 보여주는데 위키백과에 따르면 ‘사용자를 대신하여 일을 수행하는 소프트웨어 에이전트’라고 한다. 즉, UA만 알아도 어떤 기기/브라우저를 사용하는지 알 수 있다는것.mozilla에 가보면 스팩 등 다양한 UA를 볼수가 있는데 특히 맨 아래보면 기기/브라우저별로 지원정보가 나와있다. 여기서도 보면 모든 모바일 삼성 브라우저를 제외하고는 전부 지원이 되는걸 확인할수 있다.출처 : developer.mozilla.org출처 : developer.mozilla.org # 기존의 방법그럼 어떻게 내 서버에 들어온 사용자들의 UA를 확인할 수 있을까? (앞서 Apache를 웹서버로 운영한다고 했으니) Apache access log 에는 Apache에서 제공해주는 모듈을 이용해 접속한 클라이언트의 정보가 남겨지곤 한다. 그렇다면 이 access log를 리눅스 명령어든 엑셀로 뽑아서든지 활용해서 정규식으로 포맷팅 하고 그 결과를 다시 그룹화 시키면 얼추 원하는 데이터를 추출해 낼수 있다. ( 버거형들이 만들어둔 정규식을 가져다 사용할수도 있겠다. https://regexr.com/?37l4e )하지만, 우선 자동화가 안되어있어 데이터를 구하고 싶을때마다 귀차니즘에 걸릴수 있고 슈퍼 개발자 파워를 기반으로(?) 데이터 추출을 자동화 한들 실시간으로 보고싶을땐 제한사항이 많다. # 좀더 나은 방법(?)실시간 데이터를 모니터링 하는데에는 다양한 오픈소스와 다양한 툴이 있겠지만 경험이 부족한건지 아직까진 ElasticStack 만한걸 못본것 같다. 간단하게 설명을 하면 access log 를 사용하지 않고 front단에서 javascript 로 UA를 구한다음 이러한 정보를 받을수 있는 API를 만들어 그쪽으로 보내면 서버에서 해당 UA를 분석해서 카프카로 보내고 ..!@#$%^blabla…^^; 그림으로 보자.좀더 나은 방법좀더 나은 방법front단에서는 navigator.userAgent를 활용하여 UA를 구할수 있었고, API에서는 UA를 받고 파싱을 하는데 관련 코드는 다음과 같이 작성하였다.12345678910111213141516171819202122232425262728293031323334353637private static final String VERSION_SEPARATOR = \".\";private void userAgentParsingToMap(String userAgent, Map&lt;String, Object&gt; dataMap) &#123; HashMap browser = Browser.lookup(userAgent); HashMap os = OS.lookup(userAgent); HashMap device = Device.lookup(userAgent); dataMap.put(\"browserName\", browser.get(\"family\")); dataMap.put(\"browserVersion\", getVersion(browser)); dataMap.put(\"osName\", os.get(\"family\")); dataMap.put(\"osVersion\", getVersion(os)); dataMap.put(\"deviceModel\", device.get(\"model\")); dataMap.put(\"deviceBrand\", device.get(\"brand\"));&#125;private String getVersion(HashMap dataMap) &#123; String majorVersion = (String)dataMap.get(\"major\"); if (StringUtils.isEmpty(majorVersion)) &#123; return StringUtils.EMPTY; &#125; String minorVersion = (String)dataMap.get(\"minor\"); String pathVersion = (String)dataMap.get(\"path\"); StringBuffer sb = new StringBuffer(); sb.append(majorVersion); if (!StringUtils.isEmpty(minorVersion)) &#123; sb.append(VERSION_SEPARATOR); sb.append(minorVersion); &#125; if (!StringUtils.isEmpty(pathVersion)) &#123; sb.append(VERSION_SEPARATOR); sb.append(pathVersion); &#125; return sb.toString();&#125; 참고로 Java단에서 UA를 파싱하는 parser가 여러가지가 있는데 그중 uap_clj라는 모듈이 그나마 잘 파싱이 되어서 사용하게 되었다. 모듈별 비교 모듈 Browser OS Device eu.bitwalker.useragentutils.UserAgent O 불명확함 (Android 5.x) X net.sf.uadetector.UserAgentStringParser O O 불명확함 (Smartphone) uap_clj.java.api.* O O O Parsing 비교 UA 1&quot;Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Mobile Safari/537.36&quot; 결과 123- Browser : &#123;patch=3239, family=Chrome Mobile, major=63, minor=0&#125;- OS : &#123;patch=1, patch_minor=, family=Android, major=5, minor=1&#125;- Device : &#123;model=Nexus 6, family=Nexus 6, brand=Generic_Android&#125; 위와 같이 구성을 하면 Elasticsearch에 인덱싱된 데이터를 Kibana에서 입맛에 맞게 실시간으로 볼수있게 되었다!하지만 API를 만드는 부분 + front에서 별도의 javascript가 들어간다는 부분 + 운영하는 모든 페이지에 해당 javascript를 넣어야지만 볼수있는 부분 등 약간 아쉬운 감이 있다. 뭔가 더 좋은 방법이 없을까? 깔끔하면서도 우아한 방법. 짱구를 열심히 더 굴려보자. # 좀더 우아한 방법?Logstash 에는 UA를 파싱해주는 filter plugin 이 있다고 한다. (링크) 이 방법과 예전에 포스팅한 access log 를 elasticsearch 에 인덱싱 하는 방법을 혼합하면 위에서 걱정했던 문제들을 아주 깔끔하게 해결할 수 있을것 같다. 지난 포스팅 : 링크 물론 글보다는 그림설명이 더 빠르니 전체적인 흐름을 그림으로 보자.좀더 우아한 방법좀더 우아한 방법① front 단에서 javascript로 UA를 구하고 ② 별도로 만든 API에 데이터를 전송한뒤 ③ API에서는 이를 또 파싱하는 작업을 logstash 의 user-agent filter plugin으로 깔끔하게 해결할 수 있었다.우선 apache에서 access log format 은 다음과 같고1&quot;%h %l %u %t \\&quot;%r\\&quot; %&gt;s %b %D \\&quot;%&#123;Referer&#125;i\\&quot; \\&quot;%&#123;User-Agent&#125;i\\&quot;&quot; logstash쪽 설정은 다음과 같이 해주었다.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162input &#123; file &#123; path =&gt; &quot;/~~~/access.log.*&quot; start_position =&gt; &quot;beginning&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^%&#123;IPORHOST&#125;&quot; negate =&gt; true what =&gt; previous auto_flush_interval =&gt; 1 &#125; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; [&quot;%&#123;IPORHOST:clientip&#125; (?:-|%&#123;USER:ident&#125;) (?:-|%&#123;USER:auth&#125;) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;NOTSPACE:uri&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER:bytes&#125;) (?:-|%&#123;NUMBER:bytes2&#125;)( \\&quot;%&#123;DATA:referrer&#125;\\&quot;)?( \\&quot;%&#123;DATA:user-agent&#125;\\&quot;)?&quot;] &#125; remove_field =&gt; [&quot;timestamp&quot;,&quot;@version&quot;,&quot;path&quot;,&quot;tags&quot;,&quot;httpversion&quot;,&quot;bytes2&quot;] &#125; useragent &#123; source =&gt; &quot;user-agent&quot; &#125; if [os_major] &#123; mutate &#123; add_field =&gt; &#123; os_combine =&gt; &quot;%&#123;os&#125; %&#123;os_major&#125;.%&#123;os_minor&#125;&quot; &#125; &#125; &#125; else &#123; mutate &#123; add_field =&gt; &#123; os_combine =&gt; &quot;%&#123;os&#125;&quot; &#125; &#125; &#125; if [os] =~ &quot;Windows&quot; &#123; mutate &#123; update =&gt; &#123; &quot;os_name&quot; =&gt; &quot;Windows&quot; &#125; &#125; &#125; if [os] =~ &quot;Mac&quot; &#123; mutate &#123; update =&gt; &#123; &quot;os_name&quot; =&gt; &quot;Mac&quot; &#125; &#125; &#125;&#125;output &#123; if [user-agent] != &quot;-&quot; and [user-agent] !~ &quot;Java&quot; &#123; kafka &#123; bootstrap_servers =&gt; &quot;~~~&quot; topic_id =&gt; &quot;~~~&quot; codec =&gt; json&#123;&#125; &#125; &#125;&#125; 설정하는데 가장 큰 어려웠던 점은 grok filter 패턴을 작성하는데 많은 시간을 할해야만 했다. 개인적으로 http://grokconstructor.appspot.com/do/match?example=2 에서 테스트 해보면서 패턴을 작성할수 있어 그나마 다행이였다.또한 logstash에서 파싱해주는 정보를 조금 다듬기 위해 mutate filter 를 사용해서 필드를 조합/수정 하였다.위처럼 적용을 하고 한두시간 수집을 해보니 Elasticsearch에 아래와 같은 도큐먼트가 생성이 된것을 확인할수 있었다.이 얼마나 우아한가...이 얼마나 우아한가...이 정보들을 잘 조합해서 Kibana에서 보면 다음과 같이 볼수 있다.user-agent 아 모바일 에서도 안드로이드 7.0에서 많이 들어오는구나~user-agent 아 모바일 에서도 안드로이드 7.0에서 많이 들어오는구나~UA는 아니지만 기왕 한김에 Access log의 uri를 분석해보면 아래처럼 볼수도 있겠다.외부에서 막 찔러 대는구나~ 시스템에 영향줄 정도면 막아야지!외부에서 막 찔러 대는구나~ 시스템에 영향줄 정도면 막아야지! # 마치며사실 logstash filter plugin을 작성하면서 뭔가 if-else로 분기처리한게 아쉽긴 하지만 만족할만한 결과를 얻을수 있어서 다행이라고 생각하고, 역시 데이터를 raw형태로 보는것보다 시각화해서 보는게 훨씬더 이해도를 높일수 있다는걸 다시금 실감할수 있었던 좋은 시간이였다.그런데.. 이 방법말고 더 우아한 방법은 없으려나..?;;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"user-agent","slug":"user-agent","permalink":"https://taetaetae.github.io/tags/user-agent/"},{"name":"apache access log","slug":"apache-access-log","permalink":"https://taetaetae.github.io/tags/apache-access-log/"},{"name":"Elasitcsearch","slug":"Elasitcsearch","permalink":"https://taetaetae.github.io/tags/Elasitcsearch/"},{"name":"Logstash","slug":"Logstash","permalink":"https://taetaetae.github.io/tags/Logstash/"},{"name":"Kibana","slug":"Kibana","permalink":"https://taetaetae.github.io/tags/Kibana/"}]},{"title":"gzip 설정으로 속도를 더 빠르게!","slug":"apache-gzip","date":"2018-04-01T08:58:11.000Z","updated":"2018-07-29T08:31:26.069Z","comments":true,"path":"2018/04/01/apache-gzip/","link":"","permalink":"https://taetaetae.github.io/2018/04/01/apache-gzip/","excerpt":"내가 운영중인 웹서비스의 응답속도를 보다 더 빠르게 하기 위해서는 어떤 방법이 있을까?웹 서비스를 위해 서버를 구성할 경우 일반적으로 앞단에 웹서버를 두고 그뒤에 WAS를 두는 설계를 하곤 한다.","text":"내가 운영중인 웹서비스의 응답속도를 보다 더 빠르게 하기 위해서는 어떤 방법이 있을까?웹 서비스를 위해 서버를 구성할 경우 일반적으로 앞단에 웹서버를 두고 그뒤에 WAS를 두는 설계를 하곤 한다. 여기서 웹서버는 대표적으로 Apache나 Nginx가 있고 WAS는 tomcat이나 기타 다른 모듈을 사용하는데 이렇게 두단계로 나누는 이유는 여러가지가 있겠지만 여기서는 앞단의 웹서버(Apache)의 설정으로 응답속도를 줄일수 있는 방법을 알아 보고자 한다. # 웹페이지의 응답속도를 줄일수 있는 ‘일반적인’방법들꼭 서버의 설정들을 건드리지 않고도 웹페이지의 응답속도를 줄일수 있는 방법은 다양하다. 가장 간단하게 코드 레벨에서 설정할수 있는 방법으로는 스타일시트를 위에 선언하거나 java script는 코드 아래부분에 넣는것만으로도 어느정도 응답속도를 줄일수 있다고 한다. (사족) 신입시절 회사 대표님이 필수로 읽어보라고 전 직원들에게 선물해주셨던 웹사이트 최적화기법 (스티브 사우더스 저)이 생각이 난다. 모두 사주려면 돈이 얼마야… 그만큼 웹개발자들에게 중요하면서도 한편으로는 기본이 되는 부분들이니 한번쯤 목차라도 읽어보는게 좋을듯 하다. 사실 이 포스팅을 작성하게된 가장 큰 계기는 얼마전 사내 해커톤을 하면서 경험한 부분 때문이다. ( + 들어만 봤지 실제로 해보지는 않아서… ) 서버에서 node(React)를 띄우고 그 앞단에 Apache로 단순 Port Redirect ( 80 → 3000 ) 시켜주고 있었는데 react 에서 사용하는 bundle.js의 용량이 크다보니 최초 페이지 접근시 로딩시간이 5초 이상되어버린 것이다. bundle.js를 줄여보는등 다양한 방법을 사용했다가 결국 Apache 설정을 통해 1초 이내로 줄일수 있었다. # gzip우선 gzip이란 파일 압축에 쓰이는 응용 소프트웨어로 GNU zip의 준말이라고 한다. (참고 : 위키백과 Gzip) 이를 사용하기 위해서는 브라우저가 지원을 해야하는데 https://caniuse.com/#search=gzip 을 보면 대부분의 브라우저에서 지원하는것을 볼수 있다. # 데이터 흐름그럼 gzip 을 사용했을때와 사용하지 않았을때의 차이는 어떻게 다를까? 우선 Request/Response Flow 를 잠깐 살펴보면 다음과 같다. gzip 사용 전출처 : betterexplained.com출처 : betterexplained.com 브라우저가 서버측에 /index.html을 요청한다. 서버는 Request를 해석한다. Response에 요청한 내용을 담아 보낸다. Response를 기다렸다가 브라우저에 보여준다. (100kb) gzip 사용 후출처 : betterexplained.com출처 : betterexplained.com 브라우저가 서버측에 /index.html을 요청한다. 서버는 Request를 해석한다. Response에 요청한 내용을 담아 보낸다. 여기서 해당 내용을 압축하는 과정이 추가가 된다. Response header에 압축이 되어있다는 정보를 확인후 브라우저는 해당 내용을 받고(10kb), 압축을 해제한 후 사용자에게 보여준다. 정리하면, gzip을 사용하면 서버는 Client에게 보낼 Response를 압축하기 때문에 네트워크 비용을 줄일수 있어 응답속도가 빠른 장점이 있다. # 무조건 사용해야 하는가?물론 무조건 좋은 (마치 show me the money 같은)정답은 없다. 서버에서 압축을 하여 Client에게 보내면 그대로 사용자에게 보여주는것이 아니라 압축을 해제하는 과정이 추가적으로 필요하다. 이러한 과정에서 브라우저는 cpu를 사용하게 되어 오히려 랜더링 하는 과정이 느려질수 있어 자칫 응답속도는 빨라졌다 하더라도 사용자 체감상 더 느려진것처럼 보여질수 있다. 따라서 상황에 맞춰 gzip을 사용해야 할것인지 말것인지에 대해 테스트가 필요하다. # 마치며학부시절 또는 신입시절, 아파치는 정적인 리소스를 담당하고 톰켓은 서블릿 처럼 데이터 가공이 필요한 페이지를 담당한다고 주문을 외우듯 하였지만, 웹서버에서 압축을 하면 어떤 효과가 있는지 실제로 경험해보는게 가장 중요한것 같다.적용 방법은 복붙하는 느낌이라 아파치 공식 문서를 링크 하는것으로 해당 포스팅을 마무리 하겠다. Apache Document (사용법) : https://httpd.apache.org/docs/2.2/ko/mod/mod_deflate.html 적용 테스트 사이트 https://developers.google.com/speed/pagespeed/insights http://www.whatsmyip.org/http-compression-test","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"gzip","slug":"gzip","permalink":"https://taetaetae.github.io/tags/gzip/"},{"name":"mod_deflate","slug":"mod-deflate","permalink":"https://taetaetae.github.io/tags/mod-deflate/"}]},{"title":"RestClientException 처리","slug":"rest-client-exception","date":"2018-03-17T10:19:39.000Z","updated":"2018-07-29T08:31:26.431Z","comments":true,"path":"2018/03/17/rest-client-exception/","link":"","permalink":"https://taetaetae.github.io/2018/03/17/rest-client-exception/","excerpt":"Spring 환경에서 (Spring5는 달라졌지만…) 외부 API로의 호출을 할때 자주 쓰이는 RestTemplate. 이때 request에 대해 정상적인 응답이 아닌 경우에 대한 처리는 어떻게 할까? 막연하게 생각을 해보면 Http Status Code를 받아서 판별을 하면 되지만 Http Status Code만 봐도 엄청~많다.","text":"Spring 환경에서 (Spring5는 달라졌지만…) 외부 API로의 호출을 할때 자주 쓰이는 RestTemplate. 이때 request에 대해 정상적인 응답이 아닌 경우에 대한 처리는 어떻게 할까? 막연하게 생각을 해보면 Http Status Code를 받아서 판별을 하면 되지만 Http Status Code만 봐도 엄청~많다. ( 위키피디아 참고 : https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )if~else 로 다 나눌수도 없고… 우선 성공/실패에 대한 판별은 어떻게 할까 하고 코드를 파고파고 들어가다가 알게된 부분에 대해 정리를 해보겠다. # Http Status Code 그룹정의Spring-project github에 가보면 HttpStatus 하위에 내부 Enum으로 아래처럼 정의되어 있다. ( 링크 )12345678910111213141516171819202122232425262728293031public enum HttpStatus &#123; ... public Series series() &#123; return Series.valueOf(this); &#125; ... public enum Series &#123; INFORMATIONAL(1), SUCCESSFUL(2), REDIRECTION(3), CLIENT_ERROR(4), SERVER_ERROR(5); ... public static Series valueOf(int status) &#123; int seriesCode = status / 100; for (Series series : values()) &#123; if (series.value == seriesCode) &#123; return series; &#125; &#125; throw new IllegalArgumentException(\"No matching constant for [\" + status + \"]\"); &#125; &#125;&#125; 약 80여개 응답코드들을 크게 5개 묶음으로 정의해 놓은것을 볼수있다. 즉, 201 / 201 / 202 같은 녀석들은 전부 SUCCESSFUL 성공 으로 그룹핑이 되는것을 확인할수 있다. # 그럼 RestTemplate 에서는 어떻게 처리하고 있나?코드를 쭉쭉 따라가다 보면 아래처럼 4xx, 5xx 는 에러라고 판단하고 그에 따라 RestClientException 을 반환하는것을 확인할수 있다. RestTemplate 호출 링크 123456789101112131415try &#123; ClientHttpRequest request = createRequest(url, method); if (requestCallback != null) &#123; requestCallback.doWithRequest(request); &#125; response = request.execute(); handleResponse(url, method, response); if (responseExtractor != null) &#123; return responseExtractor.extractData(response); &#125; else &#123; return null; &#125;&#125;catch (IOException ex) &#123; 에러 판별 링크 123protected boolean hasError(HttpStatus statusCode) &#123; return (statusCode.series() == HttpStatus.Series.CLIENT_ERROR || statusCode.series() == HttpStatus.Series.SERVER_ERROR);&#125; # 성공/실패 처리는 어떻게 할것인가각자 정의하기 나름일것 같다. 우선 아주 간단하게 RestTemplate 를 사용할때 예외처리를 하여 정의된 대로 4xx, 5xx가 에러라고 판단할 수 있을것 같고12345try &#123; responseBody = restTemplate.postForObject(url, httpEntity, byte[].class);&#125; catch (RestClientException e) &#123; // 에러인 경우 RestClientException 을 내뱉는다. log.error(\"##### restTemplate error, url = &#123;&#125;\", url, e);&#125; 정의된 에러(?)와는 조금 다르게 처리하고 싶다면 DefaultResponseErrorHandler을 상속받고 hasError메소드를 무조건 패스하도록 Override 하고난 다음 응답 코드를 받아서 처리하는 방법이 있을수 있겠다.(물론 이러한 방법 말고 다양한 방법이 있을것 같다.)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"RestTemplate","slug":"RestTemplate","permalink":"https://taetaetae.github.io/tags/RestTemplate/"},{"name":"RestClientException","slug":"RestClientException","permalink":"https://taetaetae.github.io/tags/RestClientException/"}]},{"title":"소나큐브 이용 코드 정적분석 자동화","slug":"jenkins-sonar-github-integration","date":"2018-02-08T11:10:54.000Z","updated":"2018-07-29T08:31:26.201Z","comments":true,"path":"2018/02/08/jenkins-sonar-github-integration/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/jenkins-sonar-github-integration/","excerpt":"코드 정적분석이라 함은 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석을 말한다. 이를테면 냄새나는 코드(?)라던지, 위험성이 있는 코드, 미리 정의된 규칙이나 코딩 표준을 준수하는지에 대한 분석을 말하는데 java 기준으로는 아래 다양한 (잘 알려진) 정적분석 도구들이 있다.","text":"코드 정적분석이라 함은 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석을 말한다. 이를테면 냄새나는 코드(?)라던지, 위험성이 있는 코드, 미리 정의된 규칙이나 코딩 표준을 준수하는지에 대한 분석을 말하는데 java 기준으로는 아래 다양한 (잘 알려진) 정적분석 도구들이 있다. PMD 미사용 변수, 비어있는 코드 블락, 불필요한 오브젝트 생성과 같은 Defect을 유발할 수 있는 코드를 검사 https://pmd.github.io FindBugs 정해진 규칙에 의해 잠재적인 에러 타입을 찾아줌 http://findbugs.sourceforge.net CheckStyle 정해진 코딩 룰을 잘 따르고 있는지에 대한 분석 http://checkstyle.sourceforge.net 이외에 SonarQube 라는 툴이 있는데 개인적으로 위 알려진 다른 툴들의 종합판(?)이라고 생각이 들었고, 그중 가장 인상깊었던 기능이 github과 연동이 되고 적절한 구성을 하게 되면 코드를 수정하는과 동시에 자동으로 분석을 하고 리포팅까지 해준다는 부분이였다. ( 더 좋은 방법이 있는지는 모르겠으나 다른 도구들은 수동으로 돌려줘야 하고 리포팅 또한 Active하지 못한(?) 아쉬운 점이 있었다. ) 지금부터 Jenkins + github web-hook + SonarQube 를 구성하여 코드를 수정하고 PullRequest를 올리게 되면 수정한 파일에 대해 자동으로 정적분석이 이뤄지고, 그에대한 리포팅이 해당 PullRequest에 댓글로 달리도록 설정을 해보겠다. (코드리뷰를 봇(?)이 자동으로 해주는게 얼마나 편한 일인가…) # 기본 컨셉전체적인 컨셉은 다음 그림과 같다.전체 컨셉전체 컨셉 IDE에서 코드수정을 하고 remote 저장소에 commit &amp; push를 한다.그 다음 github에서 master(혹은 stable한 branch)에 대해 작업 branch를 PullRequest 올린다. 미리 등록한 github의 web-hook에 의해 PullRequest 정보들을 jenkins에 전송한다. 전달받은 정보를 재 가공하여 SonarQube로 정적분석을 요청한다. SonarQube에서 분석한 정보를 다시 jenkins로 return 해준다. SonarQube으로부터 return 받은 정보를 해당 PullRequest의 댓글에 리포팅을 해준다. 간단히 보면 (뭐 간단하니 쉽네~) 라고 볼수도 있겠지만 나는 이런 전체 흐름을 설정하는데 있어 어려웠다. 사실 셋팅하는 과정에서 적지않은 삽질을 했었기에, 이 포스팅을 적는 이유일수도 있겠다.더불어 검색을 해봐도 이렇게 전체흐름이 정리된 글이 잘 안보여서 + 내가 한 삽질을 다른 누군가도 할것같아서(?) # Maven 설치기본적으로 Maven의 H2DB를 사용하므로 SonarQube를 설치하기전에 Maven부터 설치해줘야 한다.123456$ wget http://apache.mirror.cdnetworks.com/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz$ tar -zxvf apache-maven-3.5.2-bin.tar.gz(환경변수 셋팅후 )$ mvn -versionApache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T16:58:13+09:00)... # SonarQube 설치정적분석을 도와주는 SonarQube를 설치해보자.123456$ wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.7.1.zip$ unzip sonarqube-6.7.1.zip$ cd sonarqube-6.7.1/bin/linux-x86-64$ ./sonar.sh startStarting SonarQube...Started SonarQube. 기본적으로 9000포트를 사용하고 있으니 다른포트를 사용하고자 한다면 /sonarqube-6.7.1/conf/sonar.properties 내 sonar.web.port=9000 을 수정해주면 된다. (SonarQube도 Elasticsearch를 사용하구나…)설치후 실행을 한뒤 서버IP:9000을 접속해보면 아래 화면처럼 나온다. (혹시 접속이 안된다거나 서버가 실행이 안된다면 ./sonar.sh console로 로그를 보면 문제해결에 도움이 될수도 있다. )SonarQube 메인화면SonarQube 메인화면 # SonarQube Scanner 설치소스를 연동시켜 정적분석을 하기 위해서는 SonarQube Scanner 라는게 필요하다고 한다. 아래 url에서 다운받아 적절한 곳에 압축을 풀어두자.https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner12$ wget https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-3.0.3.778-linux.zip$ unzip sonar-scanner-cli-3.0.3.778-linux.zip # jenkins 설치 및 SonarQube 연동jenkins 설치는 간단하니 별도 언급은 안하고 넘어가…려고 했으나, 하나부터 열까지 정리한다는 마음으로~https://jenkins.io/download/ 에서 최신버전을 tomcat/webapps/ 아래에 다운받고 server.xml 을 적절하게 수정해준다.12345$ wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war$ vi tomcat/conf/server.xml&lt;Connector port=&quot;19001&quot; protocol=&quot;HTTP/1.1&quot; # 포트 변경&lt;Context path=&quot;/jenkins&quot; debug=&quot;0&quot; privileged=&quot;true&quot; docBase=&quot;jenkins.war&quot; /&gt; #추가# tomcat/bin/startup.sh jenkins 설치를 완료 한 후 필요한 플러그인을 추가로 설치해준다. Python Plugin GitHub Pull Request Builder GitHub plugin 접속 : 서버IP:19001 (참고로 한 서버에서 다 설치하다보니 port 충돌을 신경쓰게되었다. )처음 jenkins를 실행하면 이런저런 설정을 하는데 특별한 설정 변경없이 next버튼을 연신 눌러면 설치가 완료 되고, SonarQube를 사용하기 위해 SonarQube Scanner for Jenkins라는 플러그인을 설치해주자. (이건 각 버전마다 궁합(?)이 안맞을수도 있으니 확인이 필요할수도 있다. 내가 설치한 버전은 jenkins 2.89, SonarQube Plugin 2.6.1이다.)설치를 하면 jenkins &gt; configure 에서 SonarQube servers정보를 등록해준다.SonarQube 젠킨스 설정SonarQube 젠킨스 설정authentication token 은 SonarQube에 처음 접속했을때 아래화면을 볼수 있는데, 여기서 authentication token을 발급받을수 있다. 또한 language 와 build 방법을 선택하자. (이부분은 나중에 변경이 가능하니 개발상황에 맞춰서 설정하면 될듯하다.)authentication token 발급authentication token 발급그 다음 jenkins &gt; configureTools 에서 SonarQube scanner 정보를 다음 화면과 같이 등록해준다.SonarQube scanner 젠킨스 설정SonarQube scanner 젠킨스 설정우선 여기까지 하면 jenkins 와 SonarQube 연동은 된걸로 보면 된다. # Github과 Jenkins 연동앞서 설명한 전체 컨셉 그림과 같이 github 에 pullRequest가 발생하면 web-hook을 이용하여 jenkins로 정보를 전달하는 방식인데, 이럴려면 우선 github과 jenkins가 연동이된 상태여야 한다. 관련 방법은 아래 링크에 정리를 해놨으니 참고해도 좋을듯 하다. Github과 Jenkins 연동하기 # Jenkins job 등록Jenkins job은 두개를 만들어 줘야 한다. 1번 job pullrequest_receiver : github으로부터 web-hook을 통해 pullRequest정보를 받는 job 2번 job sonaqube-job : 1번 job으로 부터 정보를 받아 SonarQube를 이용해 정적분석후 해당 pullRequest에 댓글로 리포팅 하는 Job - 1번 job편의상 job 이름은 pullrequest_receiver으로 정하였고, 매개변수로 String Parameter에 이름은 payload라 정하였다. 그리고 Build 에서 Execute Python 으로 아래 코드를 실행하도록 하였다.12345678910111213141516import os, json, sys, requestsurl = 'http://~~~/jenkins/job/sonaqube-job'hookData = json.loads(os.environ['payload'])branch = hookData['pull_request']['head']['ref'];prId = hookData['pull_request']['number'];componentName = hookData['repository']['name'];if action == 'closed' or action == 'synchronize' or action == 'assigned' : # 이 부분은 적절하게 수정이 필요하다. sys.exit(0)repository = hookData['repository']['full_name']url = url + '/buildWithParameters?branch=' + branch + '&amp;pr=' + str(prId) + '&amp;repository=' + repository + '&amp;componentName=' + componentNameresponse = requests.post(url) WebHook을 이용해서 Jenkins Job을 실행시키는 과정은 아래 링크에 별도로 정리해놨으니 참고해도 좋을듯 하다. Github의 WebHook을 이용하여 자동 Jenkins Job 실행 위와 같이 설정을 하고 pullRequest가 발생을 하면 해당 Job이 실행이 되면서 정보들을 적절하게 조합하여 2번 job으로 보내게 된다. 여기서 job을 하나로 나누지 않고 github정보를 받는 job, SonarQube 분석 job 이렇게 두개로 나눈 이유는 여러 Repository에 대해서 셋팅을 하고 싶은경우 Repository에서는 1번 job으로 쏘게 하고 1번 job내부에서 payload의 정보를 분석하여 Repository별 SonarQube 분석 job url로 보내면 되기 때문이다. 1번 job 은 하나, 2번 job은 Repository 별로 분기 되는 구조. - 2번 jobSonarQube job을 작성할 차례다. 1번 job에서 파라미터를 보내주기 때문에 역시 2번 job에서도 파라미터를 미리 받도록 설정해주자. 파라미터 목록 pr branch repository componentName 이 job에서는 SonarQube 분석이 되는데, 해당 repo에 대한 접근권한이 있어야 하기 때문에 앞서 설정한 Github과 Jenkins 연동 방법으로 Repository 와 Credentials 을 지정해주고, 브랜치 경로 또한 파라미터로 받은 값으로 지정해 준다.소스코드 관리 설정소스코드 관리 설정 마지막으로 최종 목적이였던 SonarQube 분석을 할 차례인데, 이번에 SonarQube가 버전업이 되면서 java 기분 .java 파일만 가지고 하는게 아니라 .java파일을 컴파일 하면 나오는 .class파일 경로를 필수로 지정해줘야 한다고 한다. (예전에는 .class 파일 경로를 지정하지 않아도 되었는데 홈페이지 설명에 의하면 바이너리 파일까지 같이 분석하게 되면 분석 신뢰도가 높아진다고 한다.)https://docs.sonarqube.org/display/PLUG/Java+Plugin+and+Bytecode 딴 이야기로, 난 여태까지 정적분석이라는건 코드만을 가지고 하는줄 알았다. 자바 기준에서는 .java . 하지만 이번에 SonarQube 설정을 하면서 알게된 부분으로, 정적분석이라는건 맨 위에 적어놨듯 실제 프로그램을 실행하지 않고 코드만의 형태에 대한 분석 이기 때문에 java 기준에서는 .class 파일또한 분석의 대상으로 볼수가 있다. (실제로 구버전 - .java만 분석 / 신버전 - .java + .class 분석 을 해봤더니 불필요한 분석결과(?)가 줄어든것을 알수 있었다. ) 그래서 결국 컴파일을 한 뒤에 SonarQube 분석을 하면 되겠다.빌드 및 SonarQube 분석 설정빌드 및 SonarQube 분석 설정SonarQube 설정파일은 다음과 같다.1234567891011121314151617sonar.projectKey=$&#123;componentName&#125; # 유니크한 프로젝트 키 네이밍 값sonar.projectVersion=0.1sonar.sourceEncoding=UTF-8 sonar.analysis.mode=preview sonar.github.repository=$&#123;repository&#125;sonar.github.endpoint=https://api.github.cosonar.github.login= # github login idsonar.github.oauth= # github 개인키sonar.login=adminsonar.password=admin sonar.github.pullRequest=$&#123;pr&#125;sonar.host.url= # SonarQube urlsonar.issuesReport.console.enable=true sonar.github.disableInlineComments=truesonar.sources=.sonar.exclusions=sonar.java.binaries=target/classes # 빌드 결과물 경로 이렇게 긴~~과정을 거치고 나면 pullRequest 를 올릴때마다 아래 그림처럼 수정한 파일에 대해서 자동으로 분석을 하여 알려준다.SonarQube 분석결과SonarQube 분석결과 또한 분석결과에 따라 (이 부분은 SonarQube 설정으로 조정이 가능할것같다) merge 가 가능/불가능이 조정되어 진다.SonarQube 분석 중 critical 항목이 없어서 merge 가능한 화면SonarQube 분석 중 critical 항목이 없어서 merge 가능한 화면 # 마치며반복적인 일에 대해서 자동화 구성을 한다는것은 참 의미있는 일인것 같다. 팀내 이런 구성을 도입하구서 획기적으로 코드 품질이 나아지진 않았지만 자칫 잘못해서 merge가 되는 위험한 코드들은 어느정도 SonarQube가 잡아주는것 같다. 사족이지만, 이렇게 적고나니 간단한데 이런 구성에 대한 정보를 알기까지는 엄청난 시간이 들었기에… 오랜만의 포스팅에 대한 뿌듯함을 다시한번 느낀다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"SonarQube","slug":"SonarQube","permalink":"https://taetaetae.github.io/tags/SonarQube/"},{"name":"integration","slug":"integration","permalink":"https://taetaetae.github.io/tags/integration/"}]},{"title":"Github의 WebHook을 이용하여 자동 Jenkins Job 실행","slug":"github-web-hook-jenkins-job-excute","date":"2018-02-08T08:10:54.000Z","updated":"2018-07-29T08:31:26.161Z","comments":true,"path":"2018/02/08/github-web-hook-jenkins-job-excute/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/github-web-hook-jenkins-job-excute/","excerpt":"PullRequest가 발생하면 알림을 받고싶다거나, 내가 관리하는 레파지토리에 댓글이 달릴때마다 또는 이슈가 생성될때마다 정보를 저장하고 싶다거나. 종합해보면 Github에서 이벤트가 발생할때 어떤 동작을 해야 할 경우 Github에서 제공하는 Webhook 을 사용하여 목적을 달성할 수 있다.","text":"PullRequest가 발생하면 알림을 받고싶다거나, 내가 관리하는 레파지토리에 댓글이 달릴때마다 또는 이슈가 생성될때마다 정보를 저장하고 싶다거나. 종합해보면 Github에서 이벤트가 발생할때 어떤 동작을 해야 할 경우 Github에서 제공하는 Webhook 을 사용하여 목적을 달성할 수 있다.아 당연한 이야기이지만 언급하고 넘어갈께 있다면, Github에서 Jenkins Job을 호출하기 위해서는 Jenkins가 외부에 공개되어 있어야 한다. (내부사설망이나 private 한 설정이 되어있다면 호출이 안되어 Webhook기능을 사용할 수 없다.) # Jenkins Security 설정Jenkins Job을 외부에서 URL로 실행을 하기 위해서는 아래 설정이 꼭 필요하다. (이 설정을 몰라서 수많은 삽질을 했다.)CSRF Protection 설정 체크를 풀어줘야 한다. 이렇게 되면 외부에서 Job에 대한 트리거링이 가능해 진다.Jenkins > Configure Global SecurityJenkins > Configure Global Security # Jenkins Job 설정Github 에서 Webhook에 의해 Jenkins Job을 실행하게 될텐데, 그때 정보들이 payload라는 파라미터와 함께 POST 형식으로 호출이 되기 때문에 미리 Job에서 받는 준비(?)를 해둬야 한다.설정은 간단하게 다음과 같이 Job 파라미터 설정을 해주면 된다.Jenkins > 해당 Job > configureJenkins > 해당 Job > configure # Github Webhook 설정이제 Github Repository 의 Hook 설정만 하면 끝이난다. 해당 Repository &gt; Settings &gt; Hooks 설정에 들어가서 Add webhook을 선택하여 Webhook을 등록해준다.URL은 {jenkins URL}/jenkins/job/{job name}/buildWithParameters식으로 설정해주고 Content Type 은 application/x-www-form-urlencoded으로 선택한다. 언제 Webhook을 트리거링 시킬꺼냐는 옵션에서는 원하는 설정에 맞추면 되겠지만 나는 pullRequest가 등록 될때만 미리 만들어 놓은 Jenkins Job을 실행시킬 계획이였으니 Let me select individual events.을 설정하고 Pull Request에 체크를 해준다. 아래 그림처럼 말이다.해당 Repositroy > Settings > Hooks해당 Repositroy > Settings > Hooks이렇게 등록하고 다시 들어가서 맨 아랫 부분Recent Deliveries을 보면 ping test 가 이루어져 정상적으로 응답을 받은것을 확인할수가 있다.Webhook 등록 결과Webhook 등록 결과 이렇게 설정을 다 한 뒤 PullRequest를 발생시키면 Jenkins 해당 Job에서는 파라미터를 받으며 실행이 된것을 확인할수가 있다.Jenkins Job 실행 결과Jenkins Job 실행 결과 끝~","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"},{"name":"Webhook","slug":"Webhook","permalink":"https://taetaetae.github.io/tags/Webhook/"}]},{"title":"Github과 Jenkins 연동하기","slug":"github-with-jenkins","date":"2018-02-08T08:10:21.000Z","updated":"2018-07-29T08:31:26.178Z","comments":true,"path":"2018/02/08/github-with-jenkins/","link":"","permalink":"https://taetaetae.github.io/2018/02/08/github-with-jenkins/","excerpt":"Jenkins에서 Github의 소스를 가져와서 빌드를 하는 등 Github과 Jenkins와 연동을 시켜줘야하는 상황에서, 별도의 선행 작업이 필요하다. 다른 여러 방법이 있을수 있는데 여기서는 SSH로 연동하는 방법을 알아보고자 한다.","text":"Jenkins에서 Github의 소스를 가져와서 빌드를 하는 등 Github과 Jenkins와 연동을 시켜줘야하는 상황에서, 별도의 선행 작업이 필요하다. 다른 여러 방법이 있을수 있는데 여기서는 SSH로 연동하는 방법을 알아보고자 한다. 우선 Jenkins가 설치되어있는 서버에서 인증키를 생성하자. 1234567891011121314151617181920212223$ ssh-keygen -t rsa -f id_rsaGenerating public/private rsa key pair.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in id_rsa.Your public key has been saved in id_rsa.pub.The key fingerprint is:SHA256:~~~~~ ~~~@~~~~~The key&apos;s randomart image is:+---[RSA 2048]----+| o*+**=*=**+ || o B=o+o++o || E+.o+ + oo .|| oo. * o ...|| .+ S = o || . + o . || . . . || . || |+----[SHA256]-----+$ lsid_rsa id_rsa.pub 개인키(id_rsa)는 젠킨스에 설정해준다. (처음부터 끝까지 복사, 첫줄 마지막줄 빼면 안된다… )젠킨스에 SSH 개인키 설정젠킨스에 SSH 개인키 설정그 다음 공개키(id_rsa.pub)는 Github에 설정을 해준다.Github에 SSH 공개키 설정Github에 SSH 공개키 설정이렇게 한뒤 Jenkins 에서 임의로 job을 생성하고 job 설정 &gt; 소스코드 관리 에서 git 부분에 아래처럼 테스트를 해서 정상적으로 연동이 된것을 확인한다. Credentials 값을 위에서 설정한 개인키로 설정하고, repo 주소를 SSH용으로 적었을때 에러가 안나오면 성공한것이다.정상 연결되면 Jenkins 오류도 없고, github SSH 키에 녹색불이 들어온다.정상 연결되면 Jenkins 오류도 없고, github SSH 키에 녹색불이 들어온다. 끝~","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"},{"name":"jenkins","slug":"jenkins","permalink":"https://taetaetae.github.io/tags/jenkins/"}]},{"title":"linux(centOS)에서 selenium 설정하기 (feat. python)","slug":"linux-selenium","date":"2018-02-01T05:52:10.000Z","updated":"2018-07-29T08:31:26.226Z","comments":true,"path":"2018/02/01/linux-selenium/","link":"","permalink":"https://taetaetae.github.io/2018/02/01/linux-selenium/","excerpt":"테스트 코드로 안되는 실제 브라우저단 사용성 테스트를 하고싶은 경우가 있다. 이를테면 화면이 뜨고, 어떤 버튼을 누르면, 어떤 결과가 나와야 하는 일련의 Regression Test. 이때 활용할수 있는게 다양한 도구가 있지만 이번엔 selenium 에 대해서 알아보고자 한다.","text":"테스트 코드로 안되는 실제 브라우저단 사용성 테스트를 하고싶은 경우가 있다. 이를테면 화면이 뜨고, 어떤 버튼을 누르면, 어떤 결과가 나와야 하는 일련의 Regression Test. 이때 활용할수 있는게 다양한 도구가 있지만 이번엔 selenium 에 대해서 알아보고자 한다. 처음부터 사실 web application 테스트를 하려고 selenium 를 알아보게 된건 아니고, 내가 참여하고 있는 특정 밴드(네이버 BAND)에서 일주일에 한번씩 동일한 형태의 글을 올리고 있는데 (일종의 한주 출석체크 같은…) 이를 자동화 해볼순 없을까 하며 밴드 API를 찾아보다 selenium 라는것을 알게되었고, 매크로처럼 어떤버튼 누르고 그다음 어떤버튼 누르고 하는 일련의 과정을 코드로 구성할수 있다는 점에 감동을 받아(?) + 별도의 API를 발급받지 않아도 되어 사용하게 되었다. (물론 UI가 바뀌면 골치아프겠지만…) 여기서는 selenium 이 무엇인지에 대한 설명은 하지 않는다. (인터넷에 나보다 정리 잘된글이 많으니…) 단, linux 환경에서 셋팅하는 정보가 너무 없고 몇일동안 삽질을 한게 아쉬워서 그 과정을 포스팅 해본다. (나같은 분이 이 글을 보고 도움이 되실꺼라는 기대를 갖으며…) ※ 주의 : 본 포스팅은 밴드 서비스에 글을 올릴수 있는 비 정상적인 방법의 공유가 아닌, selenium에 대한 사용 후기(?)에 대한 글입니다. (참고로 막혔어요 -ㅁ-) 설정하기서버 환경은 CentOS 7.4 64Bit + Python 3.6.3 + jdk 8 이다. 우선 selenium 을 설치해준다.1$ sudo python3.6 -m pip install selenium 그 다음 CentOS에서 크롬브라우저를 설치하기 위하여 yum 저장소를 추가한다. (꼭 크롬이 아니더라도 파이어폭스나 지금은 지원이 끊긴 팬텀JS 같은것으로 활용할수도 있으나 다른것들도 해봤는데 자꾸 설정에서 걸려서 크롬에 대한 내용을 포스팅 한다.)1234567$ sudo vi /etc/yum.repos.d/google-chrome.repo[google-chrome]name=google-chromebaseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64enabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 그리고는 yum 으로 크롬 브라우저를 설치한다. (내가 설치했을때의 버전은 google-chrome-stable.x86_64 0:64.0.3282.119-1)1$ yum install google-chrome-stable 크롬드라이버를 설치해야한다. 다음 url에서 받을수 있는데 https://sites.google.com/a/chromium.org/chromedriver/downloads 나는 2.35 linux64 버전을 받았다. 다운받고 unzip 하면 딱하나 파일이 있는데 나중에 selenium 을 사용할때 이용되니 path를 알아두자. 그다음 파이썬 코드를 작성한다. 내가 짠 파이썬 코드는 다음과 같은 순서로 실행이 된다. 밴드 접속 ( https://band.us/home ) 로그인 특정 밴드 선택 글쓰기 버튼 누르고 양식에 맞춰 글 작성 글 등록 파이썬 코드는 아래처럼 작성하였다. (중요부분만.. 그 아래는 자유)1234567891011121314from selenium import webdriverfrom selenium.webdriver.chrome.options import Options# 초기화 --------------------------------------------chrome_options = Options()chrome_options.add_argument(\"--headless\")driver = webdriver.Chrome(executable_path='home/~~~/chromedriver', chrome_options=chrome_options)driver.implicitly_wait(3)driver.get('https://band.us/home')# 로그인 --------------------------------------------driver.find_element_by_class_name('_loginLink').click()...생략 그리고 실행을 해보면 작동이 잘~ 된다.selenium + python 으로 자동작성된 밴드 글selenium + python 으로 자동작성된 밴드 글 마치며selenium 에 대해 찾아보면 거의 윈도우 환경에서 돌아가는것들에 대한 포스팅이 많았다. 난 리눅스 환경에서 스케쥴러(젠킨스 같은)를 통해 자동으로 화면없이 작동시키고 싶었는데 아무리 찾아봐도 + 삽질해도 잘 안되었다. 결국 사내에도 나같은 삽질을 하신 분을 찾고 묻고 물어 크롬드라이버만 있어야 하는것이 아니라 크롬앱또한 있어야 동작을 한다는것을 알게 되었다.역시, 내가 한 삽질은 누군가 이미 한 삽질이라는걸 다시한번 깨닳은 좋은(?) 시간이였다. 이걸로 나중에 내가 맡고있는 서비스에 대한 웹 자동 테스트 툴도 만들어 볼 생각이다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"},{"name":"linux","slug":"linux","permalink":"https://taetaetae.github.io/tags/linux/"},{"name":"selenium","slug":"selenium","permalink":"https://taetaetae.github.io/tags/selenium/"},{"name":"chromedriver","slug":"chromedriver","permalink":"https://taetaetae.github.io/tags/chromedriver/"}]},{"title":"아파치 엑세스 로그를 엘라스틱서치에 인덱싱 해보자.","slug":"apache-access-log-to-es","date":"2018-01-25T12:18:35.000Z","updated":"2018-07-29T08:31:26.047Z","comments":true,"path":"2018/01/25/apache-access-log-to-es/","link":"","permalink":"https://taetaetae.github.io/2018/01/25/apache-access-log-to-es/","excerpt":"apache access log 를 분석하고 싶은 상황이 생겼다. 아니 그보다 apache access에 대해서 실시간으로 보고싶었고, log를 검색 &amp; 데이터를 가공하여 유의미한 분석결과를 만들어 보고 싶었다. 그에 생각한것이 (역시) ElasticStack.","text":"apache access log 를 분석하고 싶은 상황이 생겼다. 아니 그보다 apache access에 대해서 실시간으로 보고싶었고, log를 검색 &amp; 데이터를 가공하여 유의미한 분석결과를 만들어 보고 싶었다. 그에 생각한것이 (역시) ElasticStack. 처음에 생각한 방안은 아래 그림처럼 단순했다.처음 생각한 단순한 구조처음 생각한 단순한 구조하지만, 내 단순한(?) 예상은 역시 빗나갔고 logstash에서는 다음과 같은 에러를 내뱉었다. retrying individual bulk actions that failed or were rejected by the previous bulk request request가 많아짐에 따라 elasticsearch가 버벅거리더니 logstash에서 대량작업은 거부하겠다며 인덱싱을 멈췄다. 고민고민하다 elasticsearch에 인덱싱할때 부하가 많이 걸리는 상황에서 중간에 버퍼를 둔 경험이 있어서 facebook그룹에 문의를 해봤다.https://www.facebook.com/groups/elasticsearch.kr/?multi_permalinks=1566735266745641역시 나보다 한참을 앞서가시는 분들은 이미 에러가 뭔지 알고 있으셨고, 중간에 버퍼를 두고 하니 잘된다는 의견이 있어 나도 따라해봤다. 물론 답변중에 나온 redis가 아닌 기존에도 비슷한 구조에서 사용하고 있던 kafka를 적용.아, 그전에 현재구성은 Elasticsearch 노드가 총 3대로 클러스터 구조로 되어있는데 노드를 추가로 늘리며 스케일 아웃을 해보기전에 할수있는 마지막 방법이다 생각하고 중간에 kafka를 둬서 부하를 줄여보고 싶었다. (언제부턴가 마치 여러개의 톱니바퀴가 맞물려 돌아가는듯한 시스템 설계를 하는게 재밌었다.) 아래 그림처럼 말이다.그나마 좀더 생각한 구조그나마 좀더 생각한 구조그랬더니 거짓말 처럼 에러하나 없이 잘 인덱싱이 될수 있었다. logstash가 양쪽에 있는게 약간 걸리긴 하지만, 처음에 생각한 구조보다는 에러가 안나니 다행이라 생각한다. 이 구조를 적용하면서 얻은 Insight가 있기에, 각 항목별로 적어 보고자 한다. ( 이것만 적어놓기엔 너무 없어보여서.. ) # access log 를 어떻게 분석하여 인덱싱 할것인가?apache 2.x를 사용하고 별도의 로그 포맷을 정하지 않으면 아래와 같은 access log가 찍힌다.1123.1.1.1 - - [25/Jan/2018:21:55:35 +0900] &quot;GET /api/test?param=12341234 HTTP/1.1&quot; 200 48 1144 &quot;http://www.naver.com/&quot; &quot;Mozilla/5.0 (iPhone; CPU iPhone OS 11_1_2 like Mac OS X) AppleWebKit/604.3.5 (KHTML, like Gecko) Mobile/15B202 NAVER(inapp; blog; 100; 4.0.44)&quot; 그럼 이 로그를 아무 포맷팅 없이 로깅을 하면 그냥 한줄의 텍스트가 인덱싱이 된다. 하지만 이렇게 되면 elasticsearch 데이터를 다시 재가공하거나 별도의 작업이 필요할수도 있으니 중간에 있는 logstash에게 일을 시켜 좀더 nice 한 방법으로 인덱싱을 해보자. 바로 logstash 의 filter 기능이다. 그중 Grok filter 라는게 있는데 패턴을 적용하여 row data 를 필터링하는 기능이다. 조금 찾아보니 너무 고맙게도 아파치 필터 예제가 있어 수정하여 적용할수 있었다. http://grokconstructor.appspot.com/do/match?example=2그래서 적용한 필터설정은 다음과 같다.1234567filter &#123; grok &#123; match =&gt; &#123; message =&gt; &quot;%&#123;IP:clientIp&#125; (?:-|) (?:-|) \\[%&#123;HTTPDATE:timestamp&#125;\\] \\&quot;(?:%&#123;WORD:httpMethod&#125; %&#123;URIPATH:uri&#125;%&#123;GREEDYDATA&#125;(?: HTTP/%&#123;NUMBER&#125;)?|-)\\&quot; %&#123;NUMBER:responseCode&#125; (?:-|%&#123;NUMBER&#125;)&quot; &#125; &#125;&#125; 이렇게 하고 elasticsearch 에 인덱싱을 하면 키바나에서 다음과 같이 볼수 있다.키바나에 내가 원하는 구조대로 이쁘게 들어가 있는 access log키바나에 내가 원하는 구조대로 이쁘게 들어가 있는 access log # 각 필드가 아닌 한줄로 인덱싱이 되어버린다.Elasticsearch 에 인덱싱이 되긴 하는데 로그 한줄이 통째로 들어가 버린다. message라는 이름으로… 알고보니 현재 구조는 logstash가 kafka 앞 뒤에 있다보니 producer logstash 와 consumer logstash 의 codec이 맞아야 제대로 인덱싱이 될수 있었다.먼저 access log에서 kafka 로 produce 하는 logstash 에서는 output 할때 codec 을 맞춰주고1234567output &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125;&#125; kafka 에서 consume 하는 logstash 에서는 input 에서 codec 을 맞춰준다.1234567input &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125;&#125; 그렇게 되면 codec이 맞아 각 필드로 이쁘게 인덱싱을 할수 있게 되었다. # 필요없는 uri는 제외하고 인덱싱할수 있을까?/으로는 uri 라던지 /server-status같이 알고있지만 인덱싱은 하기 싫은 경우는 간단하게 아래처럼 if문으로 제외시킬수 있었다.(당연하게 보일진 모르겟지만 내겐 너무 생각보다 편하게 이슈를 해결할수 있어서 좋았다.)123456789output &#123; if [uri] =~ /.+/ and [uri] != &quot;/server-status&quot; &#123; kafka &#123; bootstrap_servers =&gt; &quot;123.1.2.3:9092,123.1.2.4:9092&quot; topic_id =&gt; &quot;apache-log&quot; codec =&gt; json&#123;&#125; &#125; &#125;&#125; # 하나의 index로는 관리가 힘든데 나눌순 없을까?사실 이 항목은 logstash에 해당하는 옵션이긴 하지만. 겸사겸사 적어본다.이미 지난 로그는 지워야 할 상황이 온다. 이를테면 1년이 지났거나. 그럴경우 마지막 elasticsearch 로 output 하는 logstash 설정에서 다음과 같이 설정할수 있다.1234567output&#123; elasticsearch &#123; hosts =&gt; [&quot;123.1.2.3:9200&quot;, &quot;123.1.2.4:9200&quot;] index =&gt; &quot;apache-log-%&#123;+YYYY.MM&#125;&quot; document_type =&gt; &quot;apache&quot; &#125;&#125; 이렇게 하고 apache 라는 템플릿을 지정해 놓으면 달이 바뀔때마다 자동으로 해당 템플릿에 맞추어 index가 만들어지게 되고, 원하는 달 전체의 데이터를 한번에 지울수 있는 장점이 있는것 같다. 이런 저런 삽질의 과정의 끝에는 달콤한 보상이 따르는것 같다.(항상 그러는건 아니지만..) 아래처럼 대시보드를 만들어 한눈에 apache 단에서의 request를 분석할수 있게 되었다. 물론 이보다 더 한것도 할수 있을것 같다.키바나 대시보드 모습키바나 대시보드 모습 매번 새로운 기술을 습득할때마다 느끼는거지만, 고전 기술로 어렵게 어렵게 시간을 소비하며 구성하는 것보다 새로운 기술을 빨리 습득하고 삽질할 시간에 더 다양한 생각을 해보는게 좋은것 같다. 특히 이 Elasticsearch 는 설정 몇번만으로 이렇게 강력한(?) 구성을 만들수 있다는거에 너무 신기하면서도 감사하다. 자, 다음엔 또 어떤걸 해볼수 있을까!? 가즈아~ 2018. 01. 26 추가각 버전은 다음과 같다. (es를 어서 업그레이드 해야하는데…)elasticsearch : 2.4.0logstash : logstash-5.4.3kafka : 0.11.0.0apache : 2.2.x","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"https://taetaetae.github.io/tags/logstash/"},{"name":"kafka","slug":"kafka","permalink":"https://taetaetae.github.io/tags/kafka/"},{"name":"access log","slug":"access-log","permalink":"https://taetaetae.github.io/tags/access-log/"}]},{"title":"파이썬 버전 업그레이드 (2.6 > 3.6)","slug":"python-2-to-3","date":"2018-01-08T04:44:50.000Z","updated":"2018-07-29T08:31:26.428Z","comments":true,"path":"2018/01/08/python-2-to-3/","link":"","permalink":"https://taetaetae.github.io/2018/01/08/python-2-to-3/","excerpt":"파이썬 2.x 에서는 depreate 된 모듈도 많고 3.x에서만 지원되는 버전들이 많아지면서 실컷 개발을 해도 파이썬 버전때문에 다시 짜야하는 상황이 생긴다. 파이썬 버전업을 하고싶어 구글링을 해보면 이렇다할 정리된 문서가 잘 안나온다. (영어로된 포스트는 많이 있긴 하나, 필자의 환경과는 맞지 않는 …)","text":"파이썬 2.x 에서는 depreate 된 모듈도 많고 3.x에서만 지원되는 버전들이 많아지면서 실컷 개발을 해도 파이썬 버전때문에 다시 짜야하는 상황이 생긴다. 파이썬 버전업을 하고싶어 구글링을 해보면 이렇다할 정리된 문서가 잘 안나온다. (영어로된 포스트는 많이 있긴 하나, 필자의 환경과는 맞지 않는 …) 그래서 이것저것 삽질을 한 결과 파이썬 버전을 올릴수 있었고, 이를 포스팅 해보고자 한다. 그러보고니 2018년 첫 포스팅이네… 올해는 정말 적어도 한달에 1~2개는 올릴수 있는 내가 되기를… # 환경 CentOS 6.9 기본으로 python 2.6 이 설치되어 있는것을 확인할수 있다. (환경마다 다를수 있음.)12$ python -VPython 2.6 # 설치순서 필요한 유틸리티를 설치한다. 123$ sudo yum update$ sudo yum install yum-utils$ sudo yum groupinstall development yum 저장소에서는 최신 파이썬 릴리즈를 제공하지 않으므로 RPM 패키를 제공하는 IUM 이라는 추가 저장소를 설치 1$ sudo yum install https://centos6.iuscommunity.org/ius-release.rpm 파이썬 3.6 버전을 설치 1$ sudo yum install python36u pip 등 패키지 관련 모듈도 함께 설치 12sudo yum install python36u-pipsudo yum install python36u-devel 여기까지 하면 기존 파이썬 2.6과 새로 설치된 파이썬 3.6 이 설치되어있다. 1234567891011$ ll /usr/bin/python*-rwxr-xr-x 1 root root 9997450 Jan 2 16:02 pythonlrwxrwxrwx 1 root root 6 Jan 1 06:02 python2 -&gt; python-rwxr-xr-x 1 root root 9032 Aug 19 2016 python2.6-rwxr-xr-x 1 root root 1418 Aug 19 2016 python2.6-config-rwxr-xr-x 2 root root 6808 Oct 12 08:19 python3.6lrwxrwxrwx 1 root root 26 Jan 2 20:48 python3.6-config -&gt; /usr/bin/python3.6m-config-rwxr-xr-x 2 root root 6808 Oct 12 08:19 python3.6m-rwxr-xr-x 1 root root 173 Oct 12 08:19 python3.6m-config-rwxr-xr-x 1 root root 3339 Oct 12 08:16 python3.6m-x86_64-configlrwxrwxrwx 1 root root 16 Apr 25 2017 python-config -&gt; python2.6-config 환경변수를 설정해준다. 12$ sudo mv python python_backup$ sudo ln -s python3.6 python 확인 12$ python -VPython 3.6.3 # pip 를 이용한 모듈 설치 pip란 Python Package Index 의 약자로 공식홈페이지는 다음과 같다. ( https://pypi.python.org/pypi/pip ) 설치할 모듈을 다음과 같이 설치해주면 된다. ex : requests 모듈인 경우1$ sudo python3.6 -m pip install requests","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"https://taetaetae.github.io/tags/python/"}]},{"title":"Elastic{ON}Tour","slug":"elastic-on-tour","date":"2017-12-14T03:02:45.000Z","updated":"2018-07-29T08:31:26.124Z","comments":true,"path":"2017/12/14/elastic-on-tour/","link":"","permalink":"https://taetaetae.github.io/2017/12/14/elastic-on-tour/","excerpt":"작년에 팀을 옮기면서 로깅에 대해서 관심을 갖기 시작 하였고 찾아보다 ElasticStack 이 적합하다고 판단, 팀 내에서 나홀로 삽질해가며 지금의 로그 모니터링 시스템을 구축하였다. 그에 ElasticStack 에 관심을 갖던 찰나 지난 화요일(12월 12일)에 있었던 Elastic On Tour에 참석을 하였고 다양한 기술적 인사이트를 얻을수 있었는데 그 감동(?)을 잃기 싫어 정리해보고자 한다.","text":"작년에 팀을 옮기면서 로깅에 대해서 관심을 갖기 시작 하였고 찾아보다 ElasticStack 이 적합하다고 판단, 팀 내에서 나홀로 삽질해가며 지금의 로그 모니터링 시스템을 구축하였다. 그에 ElasticStack 에 관심을 갖던 찰나 지난 화요일(12월 12일)에 있었던 Elastic On Tour에 참석을 하였고 다양한 기술적 인사이트를 얻을수 있었는데 그 감동(?)을 잃기 싫어 정리해보고자 한다. # Registration + Partner Showcase코엑스 인터컨티넨탈 호텔에서 진행되었다. 역시 외국계 기업이여서 그런지 행사 규모가 어마어마 했다. 이정표를 따라 지하로 가서 등록을 하고, ElasticStack 을 이용해서 서비스를 하고 있는 파트너사들의 부스를 기웃거리며 ElasticStack의 저력(?)을 다시한번 실감을 할수 있었다. 특히 Elatic 본사에서 나온듯한 외국인들이 Q&amp;A 같은걸 해줬는데 답변을 해주는 외국인도 대단해 보였는데 질문을 하는 한국사람(?)들이 더 대단하게 보였다. 과연 난 저렇게 아무렇지 않고 프로페셔널(?)하게 질문을 할수 있을까?Registration + Partner ShowcaseRegistration + Partner Showcase # Track 1 : Partner SessionsTrack 1 과 2로 나뉘였는데 2는 Elastic Stack 을 경험하지 못해봤거나 소개하는 자리같아서 Track 1를 듣기로 하였다. 내가 도입을 할때만 해도 관련 자료가 잘 없었고, 정말 특이 케이스가 아닌 이상엔 잘 사용하지 않겠구나 하는 느낌이였는데 발표하시는 분들을 보고서는 생각이 180도 바뀌었다. 너무 활용들을 잘 하며 서비스를 하고 있었고 단순하게 검색엔진이 아닌 상황에 맞는 커스터 마이징이나 다른 기술 스택을 함께 사용함으로써 시너지 효과를 내고 있었다. Microsoft OpenSource 에 안좋은 이미지가 있으나 오래전부터 투자를 많이 해왔다고 설명을 하며 Azure라는 서비스에서 Elastic Stack 을 어떤식으로 활용하는지 발표를 하였다. 상당히 심플하고 처음 접하는 사람도 클릭 몇번으로 ES Cluster를 구성할수 있다는게 장점이였으나, 유료 + 커스터마이징 제한 이 아쉬웠다. S-Core : 에스코어 경험에 기반한 Elastic 활용법 EZFarm (외모 비하는 아니지만)농부 처럼 생기신 분이 나와서 기술에 대해 말씀하시는게 신기한 발표였다. 간단히 말하면 돼지가 물 먹는 량 등 농업/축산업의 데이터를 ES에 담고 머신러닝을 통하여 효율화 하는 방안 이였던것 같다. MEGAZONE 파트너 부스에서 티셔츠를 준(?) 곳이였는데 Elastic Cloud Seoul 을 발표하였다. (드디어 한국에도 이런 서비스가!) OpenBase 키바나 플러그인을 직접 개발하고 커스텀 UI의 사례를 보여주었다. 키바나 소스중에 엑셀 다운로드가 한글로 안되어 고쳐본것 말곤 플러그인을 개발할 생각은 없었는데 정말 개발자 스러운 발표였다. DIREA 결제 관련 장애추적 및 예측 시스템을 발표하였다. 마침 내가 하고있는 서비스와 비슷하고, 내가 구현해보려고 했던 부분과 거의 일맥상통한 부분이 있어서 소름이였다. Track 1 : Partner SessionsTrack 1 : Partner Sessions # Opening Keynote앞서 어떤 발표에서 ElasticSearch가 탄생하게된 계기가 어떤 분이 요리사가 되려는 아내를 위해 조리법을 더 빨리 검색할수 있는 엔진을 만들었다고 하는데 그 어떤분이 내눈앞에 나타나 발표를 하셨다. 현 Elastic CEO 이신 Shay Banon 이였다. (어색한 동시 통역으로 이해하였지만) 그분이 강조하신 Elastic 회사 정신인 “간단한건 간단하게 만들어야 하며 쉬워야 한다.” 가 연설중에 가장 인상적이였고, 통역하신 아주머님(?)때문이였는지 전달하시는 의도를 정확히 파악하긴 어려웠으나 일단 CEO를 포함한 전체 회사 분위기가 젊어보인다는걸 느낄수 있었다.Shay Banon key noteShay Banon key note # Break (식사시간)이런 세미나? 컨퍼런스? 를 많이 다녀본건 아니지만 역대급으로 좋았던 점심식사였다. ;)사실 혼자와서 밥을 어떻게 해결하나 했는데 우르르르 호텔 직원분들이 각 자리에 도시락을 대령(?)해주셔서 맛있게 먹을수 있었다.참 사람이 간사한게, 아침에 졸린눈 비벼가며 지옥철 고생을 뚫고 와서 힘들었지만 밥을 먹으면서 아침의 그 고생은 눈녹듯 사라졌다.호텔 도시락!!호텔 도시락!! # Deep Dive (Elasticsearch, Ingest, Kibana, Machine Learning)각 스택(?)에 대해서 변화된 부분, 그리고 활용가능성과 최근 출시한 6.X 버전에 대해서 소개하는 시간이였다. 사실 아직 2.4 버전을 운영중이라서인지 6.X의 변화된 부분, 그리고 5.x 버전에서의 차이를 설명해주는데 확 와닿지는 못하였다. 아직 5.x 버전의 필요성을 못느껴서 버전업을 안하고 있었는데 발표를 듣고 꼭 유료 라이센스를 구매하지 않아도 용량 단축이나 안정성, 추가 기능등 5.X로의 버전업은 해서 나쁠것도 없을듯 하다는 생각이 들었다. (단, 무조건 신버전이 좋은것은 아닌듯 잘 알아보고 해야겠지…)특히 키바나를 활용해 머신러닝을 눈앞에서 보여준건 너무 좋았다. 성능이 좋아서인지 데모시연 하는데 전혀 막힘이 없었으니…노란색 음영이 머신러닝으로 계산한 그래프의 위치값들노란색 음영이 머신러닝으로 계산한 그래프의 위치값들 # Customer PresentationsElastic Stack 및 X-Pack 을 활용하여 어떤 문제를 어떻게 해결하고 있는지에 대한 세션이였다. 삼성, NSHC, Naver 에서 약 20분씩 발표를 해주셨는데 약간 시간에 쫒기듯(?) 발표가 진행되어 집중이 잘 안되었다. 마지막에 우리 회사분이 발표하신 Application Logging with Elasticsearch at Naver 라는 주제는 실제로 내가 사용하고 있는 모듈인지라 관심갖고 들었다. 최근들어 자주 버벅이고 에러알림지연(기능중 하나)등 문제가 있었는데 이러한 부분들을 Multi-Cluster 등 튜닝을 통하여 해결했다는 부분을 설명해주셨다. (워낙에 양이 많으니…)NELONELO # 마치며밥도 맛있고 발표 내용들도 좋은 인사이트를 얻을수 있었고, 마지막에 경품추첨등 딱딱하지 않는 세미나 였다고 생각한다. 올해는 무료로 하는 행사였지만 내년부터는 유료라고 하는데 난 내년에도 사비를 내서라도 갈 생각이다. (회사지원이 된다면 좋겠지만 ^^;) 한가지 아쉬운건 너무 준비 없이 갔다는게 아쉽다. 사실 구버전(?) 2.X를 사용중이고 5.x 6.x 를 전혀 경험해보지 못해서 공감가는 부분이 없었는데, 나름 높은 버전으로 올려야 겠다는 생각을 할수있었던 좋은 시간이였던것 같다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://taetaetae.github.io/tags/elasticsearch/"}]},{"title":"What is Kafka?","slug":"what-is-kafka","date":"2017-11-02T12:30:13.000Z","updated":"2018-07-29T14:52:11.484Z","comments":true,"path":"2017/11/02/what-is-kafka/","link":"","permalink":"https://taetaetae.github.io/2017/11/02/what-is-kafka/","excerpt":"필자가 맡고있는 서비스에 Elastic Stack 을 도입하면서 중간에 버퍼가 필요하여 Message-Queue 시스템들을 알아보던 중 Kafka 에 대해 알아보고, 정리를 해보게 된다.","text":"필자가 맡고있는 서비스에 Elastic Stack 을 도입하면서 중간에 버퍼가 필요하여 Message-Queue 시스템들을 알아보던 중 Kafka 에 대해 알아보고, 정리를 해보게 된다. # 기본설명 및 기존 메세징 시스템과 다른점 메세징 큐의 일종 말 그대로 분산형 스트리밍 플랫폼, LinkedIn에서 여러 구직 + 채용 정보들을 한곳에서 처리(발행/구독)할수 있는 플랫폼으로 개발이 시작 대용량의 실시간 로그 처리에 특화되어 설계된 메시징 시스템, 기존 범용 메시징 시스템대비 TPS가 매우 우수 메시지를 기본적으로 메모리에 저장하는 기존 메시징 시스템과는 달리 메시지를 파일 시스템에 저장 → 카프카 재시작으로 인한 메세지 유실 우려 감소 기존의 메시징 시스템에서는 broker가 consumer에게 메시지를 push해 주는 방식인데 반해, Kafka는 consumer가 broker로부터 직접 메시지를 가지고 가는 pull 방식으로 동작하기 때문에 consumer는 자신의 처리능력만큼의 메시지만 broker로부터 가져오기 때문에 최적의 성능을 낼 수 있다. # 카프카 주요 개념 producer : 메세지 생산(발행)자. consumer : 메세지 소비자 consumer group : consumer 들끼리 메세지를 나눠서 가져간다.offset 을 공유하여 중복으로 가져가지 않는다. broker : 카프카 서버를 가리킴 zookeeper : 카프카 서버 (+클러스터) 상태를 관리하고 cluster : 브로커들의 묶음 topic : 메세지 종류 partitions : topic 이 나눠지는 단위 Log : 1개의 메세지 offset : 파티션 내에서 각 메시지가 가지는 unique id # 카프카는 어떤식으로 돌아가는가 zookeeper 가 kafka 의 상태와 클러스터 관리를 해준다. 정해진 topic 에 producer 가 메세지를 발행해놓으면 consumer 가 필요할때 해당 메세지를 가져간다. (여기서 카프카로 발행된 메세지들은 consumer가 메세지를 소비한다고 해서 없어지는게 아니라 카프카 설정log.retention.hours(default : 168[7일])에 의해 삭제된다.) partition 개수와 consumer group 개념 하얀색(consumer-01) : 파티션 개수가 4개인데 비해 컨슈머가 3개, 이렇게 되면 어느 컨슈머가 두개의 파티션을 담당해야하는 상황이 생긴다. 주황색(consumer-02) : 파티션 개수가 4개인데 비해 컨슈머가 5개, 이렇게 되면 하나의 노는(?) 컨슈머가 생기는 상황이 생긴다. 가장 적절한 개수는 정해지지 않았지만 통상 컨슈머그룹의 컨슈머 개수와 파티션 개수를 동일하게 가져가곤 한다. # 참고 url http://kafka.apache.org/ http://www.popit.kr/author/peter5236/ http://jinhokwon.tistory.com/168 http://programist.tistory.com/entry/Apache-Kafka-클러스터링-구축-및-테스트 https://www.elastic.co/kr/blog/just-enough-kafka-for-the-elastic-stack-part1 https://www.slideshare.net/springloops/apache-kafka-intro20150313springloops-46067669","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://taetaetae.github.io/tags/kafka/"}]},{"title":"Deview-2017 Day1 리뷰","slug":"deview-2017-review","date":"2017-10-16T08:29:55.000Z","updated":"2018-07-29T08:31:26.093Z","comments":true,"path":"2017/10/16/deview-2017-review/","link":"","permalink":"https://taetaetae.github.io/2017/10/16/deview-2017-review/","excerpt":"벌써 10번째 Naver에서 주최하는 Deview. 올해도 어김없이 참석을 하게 되었고, 이번엔 보고 듣고 느꼈던 부분들을 조금이라도 간직하고 싶은 마음에 바로 블로깅을 하려고 한다. (오랜만에 블로깅이긴 하지만…)","text":"벌써 10번째 Naver에서 주최하는 Deview. 올해도 어김없이 참석을 하게 되었고, 이번엔 보고 듣고 느꼈던 부분들을 조금이라도 간직하고 싶은 마음에 바로 블로깅을 하려고 한다. (오랜만에 블로깅이긴 하지만…)항상 Deview에 올때마다 느끼는 부분인데 이번참석이 3번째 되는듯 하다 세상은 좁고 능력자는 많으며 내가 한번쯤 본것들은 이미 지나간 기술들이라는것, 더불어 단상위에 올라가 발표하는 사람들도 예전엔 나와 똑같이 발표를 듣는 일반 사람이였다는것. 이번에도 많은 생각을 하게 되었다. 구구절절 개인적으로 느낀점을 적는것에 앞서 강한 기억이 남았던 몇몇 세션들에 대해서 간략하게 리뷰를 먼저 하는게 맞는 순서같다. # 책 읽어주는 딥러닝 ( 김태훈 / 데브시스터즈 )슬라이드 자료네이버에서 유인나의 목소리로 책을 읽어주는것을 보고 흥미를 얻어 개발하기 시작했다고 한다.음성합성은 데이터가 많아야 머신러닝이나 딥러닝 기술을 접목시키는데 도움이 되는데 박근혜 전 대통령, 문재인 대통령, 손석희 아나운서의 영상에서 데이터를 추출하여 문장별로 텍스트-음성을 맞추고(pair) 머신러닝 + 딥러닝 기술을 이용해서 만들수 있었다고 한다. 추후 누구나 사용할 수 있도록 파이썬 모듈로 제공한다고 하니, 감사할 따름이다.사실 머신러닝에 관심만있었지 이렇다할 공부나 직접 구현은 단한번도 안해보고 해당 세션을 들어보니 그냥 우와 신기하다정도였는데. 이번기회에 작은것부터 하나씩 시작하면서 요즈음 핫한(?) 트랜드를 따라가는 것도 괜찮은 방법같아 보인다. (앗, 우선 파이썬부터…) # 그런 REST API로 괜찮은가 ( 이응준 / 비바리퍼블리카 )슬라이드 자료발표자분을 어디서 많이 봤다 했더니만 예전에 우리 회사 사람이였다. 수업도 들어봤고, 같이 알고리즘 스터디도 했고(한번 나갔지만…). 발표 첫 부분에 자신이 10년전에 데뷰 staff 를 시작했는데 10년을 다 못채우고 퇴사를 했다고 ㅎㅎ.. 아무튼 개인적으로 나름 반가운 분이라 더 관심갖고 듣게 되었다.REST 가 무엇인가?에 대한 발표다. 결론부터 말하자면 아래 3가지중 하나를 사용하면 될것이라고 한다. REST API 를 구현하고 REST API라고 부른다. REST API 구현을 포기하고 HTTP API 라고 부른다. REST API 가 아니지만 REST API 라고 부른다. (현재 대부분의 API들의 상태) REST API를 구성하는 스타일중 눈여겨 볼만한 부분은 크게 두가지가 있다고 한다. (uniform interface) self-descriptive messages : 메시지는 스스로 설명이 되어야 한다. hypermedia as the engine of application state (HATEOAS) : 전이(상태의 이동)가 될수있는 정보가 있어야 한다. 정리를 해보면 REST API로 만들려면 제대로 알고 만들어라 라는 메시지가 강한 발표내용같다. 나도 이제까지는 그냥 json 으로 내려준다는 것, GETㆍPOST 등 HTTP Method 사용하는 것으로만 알고있었는데 개인적으로는 발표자분이 말씀하신 두가지 내용은 지키는게 맞다고 생각한다. 즉, 정말 REST 하게 만들꺼면 정확한 사용법을 알고 만드는게 좋아보인다. # 동네 커피샵도 사이렌오더를 쓸 수 있을까? ( 허형, 나동진 / 삼성전자[Lunch class] )슬라이드 자료오늘 발표중에 가장 들어보고 싶었던 세션. 예전부터 사이렌오더가 어떤식으로 동작하는지 + 우리회사 커피숍도 사내 앱을 활용해서 만들어 볼순 없을지(아이디어) 이런저런 생각이 많았었는데 딱! 원하던 발표가 있어 듣게 되었다.삼성전자 소속이신 분들이 따로 그룹을 만들어 진행하면서 만난 부분들을 발표해주셨는데 신기한 기술들이 많아 듣는 내내 흥미진진 했다. PWA(Progressive Web App) : PWA 로 모바일 청첩장을 만들었다고 한다. (결혼식 전날입니다. 오늘 결혼합니다. 이벤트[추첨]를 진행합니다. 등등..) Physical Web(Beacon), NFC … Browser Fingerprint (Device 구분) Push Nofification Web Payment 결국 정리를 해보면 동네 커피샵에서 사이렌 오더를 사용하기위해 이러저러한 기술들을 시도해봤다~인데. 각 기술들에 있어 현실적인 상황에 한계점이 있고, 그래서 결국 처음에 이야기 된 동네 커피샵에서 사이렌 오더를 사용에 대한 결과물이 없어서 아쉬웠다. 엄청 기대했는데 말이다.하지만 PWA를 이용해서 모바일 청접장을 만든 부분은 정말 찬사를 보내주고 싶은 아이디어 같다.나도 나중에 해야지~예전 “날씨”라는 웹서비스를 만들면서 웹이라는 환경에서 기상속보나 갑작스러운 눈/비 알림을 단순히 화면에 뿌려주는것이 아니라 사용자 기기에 노티(푸시)해줄수는 없을까하며 잠깐 본 기술이 PWA 였는데 난 프로토타이핑만 해본 수준이지만 이분들은 실제로 본업과는 별개로 구현을 해보는 노력을 했다는것에 내 자신이 부끄러워 진다. 여전히 이번에도 뒤통수 여러대 맞은 Deivew 2017. 우물안의 개구리라는 마음을 잃지 말고 + 나는 개발자라는 것을 잊지 말아야겟다고 또 다짐해본다.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[{"name":"deview","slug":"deview","permalink":"https://taetaetae.github.io/tags/deview/"}]},{"title":"Apache keepAlive","slug":"apache-keep-alive","date":"2017-08-28T10:56:40.000Z","updated":"2018-07-29T08:31:26.075Z","comments":true,"path":"2017/08/28/apache-keep-alive/","link":"","permalink":"https://taetaetae.github.io/2017/08/28/apache-keep-alive/","excerpt":"서버를 운영하다보면 간혹 문제가 발생하곤 한다. 이를테면 메모리가 다른이유없이 올라간다거나, 사용자 입장에서 응답속도가 간헐적으로 느린다거나. 그럴때마다 선배개발자분들께서 가장먼저 입에 오르내리는 단어. keepAlive.","text":"서버를 운영하다보면 간혹 문제가 발생하곤 한다. 이를테면 메모리가 다른이유없이 올라간다거나, 사용자 입장에서 응답속도가 간헐적으로 느린다거나. 그럴때마다 선배개발자분들께서 가장먼저 입에 오르내리는 단어. keepAlive. 대충 검색을 해보면 접속을 유지하거나 그렇지 않거나 하는 설정이구나 만으로 생각했었는데 제대로 짚고 넘어가는 의미에서 정리를 해볼 필요가 있을것 같다. # 정의우선 2.4버전 기준 도큐먼트의 내용을 볼 필요가 있다.https://httpd.apache.org/docs/2.4/mod/core.html#keepalive The Keep-Alive extension to HTTP/1.0 and the persistent connection feature of HTTP/1.1 provide long-lived HTTP sessions which allow multiple requests to be sent over the same TCP connection. In some cases this has been shown to result in an almost 50% speedup in latency times for HTML documents with many images. To enable Keep-Alive connections, set KeepAlive On. 발번역(파파고의 힘!)으로 이해한 내용으로는, keepAlive를 사용하면 50%까지 대기시간을 단축할수 있다는 설정이라 나와있다. 하지만 나같은 영어울렁중이 있는 사람들(?)은 영어 문서만을 가지고 완벽히 이해할수는 없다. 그래서 이것저것 찾아보고 다시 정리를 해본다. # 정리기본적으로 외부 사용자에 의해 요청(access)이 들어오게 되면 mpm방식이 어떤거든 간에 (아파치 MPM에 대해서도 정리를 해야겠군..) 아파치가 처리를 하든 WAS에게 넘겨주든 하나의 흐름이 들어오는데 동일한 사용자에 대해서 이 흐름을 끊고 다시 요청 받을것인가 아니면 연결을 유지하고 바로 처리를 할것인가에 대한 설정값으로 이해하면 좋을듯 싶다. 필자가 표현을 잘 못해서 그런것일수도 있으니 그림으로 보는게 가장 빠를지도 모르겠다.[출처 : https://www.svennd.be/keepalive-on-or-off-apache-tuning][출처 : https://www.svennd.be/keepalive-on-or-off-apache-tuning]위 그림은 IT전공자라면(?) 한번쯤은 봤을 tcp 3-way handshake 인데, keepAlive 를 on 하면 초기 연결하는 비용을 조금이나마 줄일수 있다는걸 보여준다. 즉, 다시말하면 keepAlive는 한번 연결된 상대에 대해서 연결을 잃지 않고(이런저런 설정값에 의존) 지속적으로 요청에 응답을 해줄수 있다는 옵션이다. 간단하게 생각하면 이 설정값을 이용하면 모든 요청에 의해서 지속적으로 응답을 해줄수 있으니 무조건 on하면 되는거 아닐까? 하는 생각이 먼저든다. default 값도 on 이니. 허나 자칫 잘못하면 메모리가 모든 접속자 마다 연결 유지를 해 놓아야 하기 때문에 아파치 프로세스수가 기하 급수적으로 늘어나 MaxClient값을 초과하게 된다. 또한 On상태일때 접속유지 하는 프로세스들 때문에 메모리를 그 만큼 많이 사용하게 된다.이 양날의 검 keepAlive는 과연 어떻게 사용해야 하는걸까? # 사용 시기우선 앞서 말했던것과 같이 기본값은 On이다. 즉, 아파치 설치시 기본으로 연결유지를 하는것. 하지만 상황에 따라 On할지 Off할지를 결정해야 한다. On 해야할 경우접속자의 수 상관없이 메모리가 충분할경우 메모리가 충분하다 : 접속자가 maxclient 값에 도달했을경우, swap메모리를 사용하지 않는 상태 Off 해야할 경우 동시 접속자수가 많을경우 메모리가 충분하지 못할경우 하지만 위의 경우는 지극히 일반적인경우고 운영하는 서버의 메모리의 상태나 접속자의 수를 확인하면서 조정이 필요하다. 꼭 keepAlive에 국한되는것은 아니지만 모든 개발에는 절대적인 답은 없는것 같다. # keepAlive 를 On할 경우의 추가 셋팅부분(참고 : https://abdussamad.com/archives/169-Apache-optimization:-KeepAlive-On-or-Off.html) MaxKeepAliveRequest [회수]하나의 지속적인 연결에서 서비스를 제공할 요청의 최대 값을 설정한다. 50 과 75 사이 정도면 충분하다고 한다. KeepAliveTimeout [초]연결된 사용자로부터 새로운 요청을 받기까지 서버가 얼마나 기다릴 것인가를 설정한다. (default : 15초) 일반적으로 1~5초 정도로 설정하곤 한다. MaxClients [회수]자식프로세스들의 최대 값을 설정한다. 이는 메모리의 크기와 상관관계가 있다. MaxRequestsPerChild [회수]클라이언트들의 요청 개수를 제한. 만약 자식 프로세스가 이 값만큼의 클라이언트 요청을 받았다면 이 자식 프로세스는 자동으로 죽게 된다. 0 일 경우엔 무한대. 이 설정값으로 메모리누수를 방지할수 있다. 어쨋든, 정답은 없다. 상황에 맞춰서 설정할것! 그에따른 책임은 본인이 가져가야하는걸 명심!","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"apache","slug":"apache","permalink":"https://taetaetae.github.io/tags/apache/"},{"name":"keepAlive","slug":"keepAlive","permalink":"https://taetaetae.github.io/tags/keepAlive/"}]},{"title":"hexo 블로그에 tranquilpeak 테마 적용하기","slug":"hexo-themes-tranquilpeak","date":"2017-08-27T08:52:56.000Z","updated":"2018-07-29T08:31:26.189Z","comments":true,"path":"2017/08/27/hexo-themes-tranquilpeak/","link":"","permalink":"https://taetaetae.github.io/2017/08/27/hexo-themes-tranquilpeak/","excerpt":"여러가지 hexo 테마중에 그나마(?) 영어로 된 문서가 있어서 적용해보게 된 tranquilpeak 라는 테마. 오늘은 해당 테마를 적용하면서 겪은 문제, 그리고 적용 방법에 대해서 간략하게나마 정리해보고자 한다. (다른 테마들은 거의다 중국쪽이나 일본…)","text":"여러가지 hexo 테마중에 그나마(?) 영어로 된 문서가 있어서 적용해보게 된 tranquilpeak 라는 테마. 오늘은 해당 테마를 적용하면서 겪은 문제, 그리고 적용 방법에 대해서 간략하게나마 정리해보고자 한다. (다른 테마들은 거의다 중국쪽이나 일본…)먼저 hexo 공식사이트에서 알려주는 테마들은 다음 사이트에서 확인해 볼수 있다. https://hexo.io/themes/index.html 기존에는 hueman이라는 테마를 사용하고 있었는데 (링크), 오랜만에 블로그를 다시(?) 시작하는 느낌을 내보고 싶었고 보다 더 심플하고 유행에 안탈것 같은(순전히 필자 생각) 테마를 찾아보다 tranquilpeak이라는 테마를 선택하게 되었다. 공식홈페이지 : https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak 샘플사이트 : http://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/ 우선 간략하게 설치과정을 나열해보면 다음과 같다. themes 폴더내에 테마파일을 받은후 압축 해제 테마 폴더 이름을 변경 _config.yml 파일 내에 테마 설정 부분 변경 ( theme: tranquilpeak ) hexo clean → hexo generate → hexo server(or hexo deploy) 이렇게 하면 아주 간단하게 테마가 변경이 된다. 혹여나(필자처럼) 기존 테마를 커스터마이징 하고 싶을 경우는 별도의 과정이 추가로 필요하다. 기존에는 css나 js만 변경하면 간단히 수정되었는데 이 테마는 약간의 빌드(?)를 필요로 한다. 따라서 css나 js등 html 요소들을 수정하였다면 다음과 같은 과정이 필요하다.(테마폴더 최상위에서) npm install bower install css 나 js 변경 grunt build hexo clean → hexo generate → hexo server(or hexo deploy) 나같은 경우는 테마에 적용된 폰트를 바꾸기 위해 블로그 를 참조하였다. (해당 아티클에다 댓글폭탄을 ㅎㅎ;;)","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"},{"name":"tranquilpeak","slug":"tranquilpeak","permalink":"https://taetaetae.github.io/tags/tranquilpeak/"}]},{"title":"다시 시작하자","slug":"refresh","date":"2017-07-09T08:16:23.000Z","updated":"2018-07-29T08:31:26.430Z","comments":true,"path":"2017/07/09/refresh/","link":"","permalink":"https://taetaetae.github.io/2017/07/09/refresh/","excerpt":"마지막 포스팅을 한지 벌써 3개월이 지났다. 그렇게 바빴던것도 아니고 블로그포스팅을 할 시간이 안난것도 아닌데 어느덧 다시 정신차리고 블로그를 포스팅 하려고보니 3개월이라는 시간이 흘러버렸네","text":"마지막 포스팅을 한지 벌써 3개월이 지났다. 그렇게 바빴던것도 아니고 블로그포스팅을 할 시간이 안난것도 아닌데 어느덧 다시 정신차리고 블로그를 포스팅 하려고보니 3개월이라는 시간이 흘러버렸네챗바퀴같은 일상, 느즈막히 일어나서 회사출근하고 정신없이 일하다가 퇴근, 그리고 늦게까지 잠못이루다 또 다음날이면 느즈막히 일어나고… 뭔가 변화가 필요하다. 매일 일기쓰기 : 일기라고 해봤자 거창한건 아니고 딱 3개월만 써보자. 오늘 뭐했는지. 자기전에 딱 10분이면 좋을듯 아침에 일찍 일어나기 : 월수금 수영에 화목 배드민턴. 주말에도 일찍일어나고. 일찍일어나면 먹이도 더 먹는다고 하지 않았던가 달력활용하기 : 운동하는것도 그렇지만, 달력을 자주 보면서 빼먹지 말아야 할 중요한 날들은 반드시 메모하고 기억하자 기타 : 책좀 많이 읽고 운동도 꾸준히 해야겠다. 물론 기술블로그 포스팅도 잊지말고. 첫술에 배부르랴. 하나둘씩 퍼즐 맞춰나가듯 해보다보면 내 자신이 바뀌어 있겠지.","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"mybatis insert/update 쿼리실행후 결과 가져오기","slug":"mybatis-useGeneratedKeys","date":"2017-04-04T02:41:28.000Z","updated":"2018-07-29T08:31:26.238Z","comments":true,"path":"2017/04/04/mybatis-useGeneratedKeys/","link":"","permalink":"https://taetaetae.github.io/2017/04/04/mybatis-useGeneratedKeys/","excerpt":"Select문이 아닌 다른 SQL Query(insert, update 등) 를 실행하고서 결과를 봐야하는 상황이 생긴다. 정확히 잘 수행되었나에 대한 확인. 어떻게 쿼리가 잘 수행되었나를 확인하는 방법은 다음과 같다.※ 참고 url : http://www.mybatis.org/mybatis-3/ko/sqlmap-xml.html","text":"Select문이 아닌 다른 SQL Query(insert, update 등) 를 실행하고서 결과를 봐야하는 상황이 생긴다. 정확히 잘 수행되었나에 대한 확인. 어떻게 쿼리가 잘 수행되었나를 확인하는 방법은 다음과 같다.※ 참고 url : http://www.mybatis.org/mybatis-3/ko/sqlmap-xml.html # useGeneratedKeys, keyProperty 옵션사용하는 데이터베이스가 자동생성키를 지원한다면(mySql 같은) 해당옵션을 이용해 결과를 리턴 받을수 있다.예로들어 파라미터로 아래 모델객체를 넘긴다고 가정하고123456public Student &#123; int id; String name; String email; Date regist_date;&#125; 아래 mybatis 구문으로 insert를 시도하게되면, 파라미터로 넘긴 Student 객체의 id값에 insert 했을때의 key값(id)이 들어오게 된다.123456Student student = new Student();student.setName('bla');student.setEmail('bla@naver.com');mapper.insertStudents(student); // 쿼리실행student.getId(); // 추출 가능 1234&lt;insert id=\"insertStudents\" useGeneratedKeys=\"true\" keyProperty=\"id\" parameterType=\"Student\"&gt; insert into Students ( name, email ) values ( #&#123;name&#125;, #&#123;email&#125; )&lt;/insert&gt; # selectKey 옵션Oracle 같은 경우는 Auto Increment 가 없고 Sequence를 사용해야만 하기 때문에 위 옵션을 사용할수가 없다. 하지만 다른 우회적인(?) 방법으로 위와같은 효과를 볼수가 있다.파라미터의 모델이나 java구문은 위와 동일하고 xml 쿼리 부분만 아래와 같이 설정해주면 된다.123456789&lt;insert id=\"insertStudents\" parameterType=\"Student\"&gt; &lt;selectKey keyProperty=\"id\" resultType=\"int\" order=\"BEFORE\"&gt; select SEQ_ID.nexyval FROM DUAL &lt;/selectKey&gt; insert into Students (id, name , email) values (#&#123;id&#125;, #&#123;name&#125;, #&#123;email&#125;)&lt;/insert&gt; 위와같은 코드에서 쿼리가 실행되기 전에 id값에 Sequence에 의해 값을 셋팅하게 되고, 자동적으로 해당 값을 Student의 id에 set하게 되서 동일한 결과를 볼수가 있다. 항상 테이블의 key값에만 해당하는것이 아니다. key값과는 전혀 상관없는 값도 selectKey 구문으로 리턴할수가 있는데 order옵션을 AFTER로 주고 리턴하고자 하는 값을 명시해주면 된다.아래 코드에서는 입력할시 id값을 Sequence에서 가져오는게 아니라 수동으로 넣어주고, 입력했던 id에 맞는 regist_date 값을 리턴받아 위에서처럼 동일하게 값를 가져올수 있다.123456789&lt;insert id=\"insertStudents\" parameterType=\"Student\"&gt; &lt;selectKey keyProperty=\"regist_date\" resultType=\"java.util.Date\" order=\"AFTER\"&gt; select regist_date FROM students WHERE id = #&#123;id&#125; &lt;/selectKey&gt; insert into Students (id, name , email, regist_date) values (#&#123;id&#125;, #&#123;name&#125;, #&#123;email&#125;, syadate)&lt;/insert&gt;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://taetaetae.github.io/tags/mybatis/"},{"name":"oracle","slug":"oracle","permalink":"https://taetaetae.github.io/tags/oracle/"}]},{"title":"Oracle + Mybatis 환경에서의 Date 다루기","slug":"oracle-mybatis-date","date":"2017-03-23T02:16:05.000Z","updated":"2018-07-29T08:31:26.425Z","comments":true,"path":"2017/03/23/oracle-mybatis-date/","link":"","permalink":"https://taetaetae.github.io/2017/03/23/oracle-mybatis-date/","excerpt":"# 상황 Oracle, Java 8, mybatis3 환경 Date컬럼에 데이터가 있는데 이를 select query로 조회하여 Model에 바인딩 시키고자 함.","text":"# 상황 Oracle, Java 8, mybatis3 환경 Date컬럼에 데이터가 있는데 이를 select query로 조회하여 Model에 바인딩 시키고자 함. 쿼리에 아무 기능을 추가하지 않고 Date 형태로 Model에 바인딩을 하면 시분초가 없어진 2017-01-01 00:00:00 형태로 남게됨 그래서 아래처럼 쿼리 작성할 때마다 TO_CHAR를 사용해서 포맷에 맞추어 형변환을 시키고 Date 또는 String으로 Model에 바인딩 하곤 했음. 1234SELECTTO_CHAR(reg_ymdt, 'YYYY-MM-DD HH24:MI:SS') AS registDateFROM... 이렇게 하다보니 query 만들때마다 형변환하는 쿼리를 만들어줘야하고, 자칫 포맷형식을 다르게 적으면 엉뚱한 결과를 초래하거나, Date형을 그대로 받아 사용해야하는 상황에서는 다시 형변환하는 과정(String to Date)을 해줘야만 함. .. 귀차니즘의 시작 : 삽질 1. 삽질의 시작1-1. 오라클의 DATE형 → java.sql.Date 의 경우 mybatis에서는 자동적으로 org.apache.ibatis.type.SqlDateTypeHandler를 호출하게됨 mybatis 3 문서 참고 해당 핸들러의 내부 데이터 변환 코드는 다음과 같음 1234@Overridepublic Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; return rs.getDate(columnName);&#125; java.sql.ResultSet.getDate()메소드를 호출하면 실제 ‘yyyy-mm-dd’ 만 가져와 리턴하게됨 (여기서 디버깅 해보면 rs.getTimestamp(columnName)값은 시분초까지 다 들어가 있음) 따라서 시간값이 없는 yyyy-mm-dd 형태로 리턴이 됨1-2. 오라클의 DATE형 → java.util.Date 의 경우 mybatis에서는 자동적으로 org.apache.ibatis.type.DateOnlyTypeHandler를 호출하게됨 mybatis 3 문서 참고 해당 핸들러의 내부 데이터 변환 코드는 다음과 같음 12345678@Overridepublic Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; java.sql.Date sqlDate = rs.getDate(columnName); if (sqlDate != null) &#123; return new java.util.Date(sqlDate.getTime()); &#125; return null;&#125; 위의 org.apache.ibatis.type.SqlDateTypeHandler 변환코드에서 발생한 문제점과 같이 yyyy-mm-dd 만 가져와서 java.sql.Date 객체를 만들고, 이 정보를 토대로 java.util.Date 객체를 만들게 되는데 앞서 시간값을 뺀 정보로 만들어졌기 때문에 결국 동일하게 yyyy-mm-dd 형태로 리턴이 됨 2. 삽질완료, 해결의 시작 오라클 + mybatis 환경에서 Date타입을 다루기 위해서는 타입핸들러를 명시적으로 만들어줘야 한다는걸 알게됨.2-1. 오라클의 DATE형 → java.sql.Date 의 경우 아래처럼 코드를 작성하여 커스텀 핸들러를 만들어 등록을 시켜준다. mybatis-config.xml 123&lt;typeHandlers&gt; &lt;typeHandler handler=\"com.naver.dbill.admin.common.handler.CustomDateHandler\"/&gt;&lt;/typeHandlers&gt; CustomDateHandler.java 123456789101112131415...import java.sql.Date;...public class CustomDateHandler extends BaseTypeHandler&lt;Date&gt; &#123; ... @Override public Date getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; Timestamp sqlTimestamp = rs.getTimestamp(columnName); if (sqlTimestamp != null) &#123; return new Date(sqlTimestamp.getTime()); &#125; return null; &#125; ...&#125; 위 코드를 작성하고 실행해보면 정상적으로 시분초 값이 있는 완전한 Date 형태를 볼수 있다.2-2. 오라클의 DATE형 → java.util.Date 의 경우 아래처럼 코드를 작성하여 커스텀 핸들러를 만들어 등록을 시켜준다. 단, mybatis 3 문서를 보면 java.sql.Date 와는 다르게 기본으로 설정된 typeHandler가 JDBC에 따라 3가지가 있다. 따라서 작성한 커스텀 핸들러를 적용하기 위해서는 명시적으로 자바타입 과 JDBC타입 을 적어줘야 정상적으로 오버라이딩이 되어 해당 핸들러를 사용하게 된다. mybatis-config.xml 123&lt;typeHandlers&gt; &lt;typeHandler handler=\"com.naver.dbill.admin.common.handler.CustomDateHandler\" javaType=\"java.util.Date\" jdbcType=\"DATE\"/&gt;&lt;/typeHandlers&gt; CustomDateHandler.java 는 위와 동일하다. ( import java.util.Date; 사용으로 변경 ) # 삽질하며 알게된 보너스 지식 java.sql.Date 는 java.util.Date 을 상속받았다. 12public class Date extends java.util.Date &#123;&#125; 검색을 하다보면 알수있겠지만 java.sql.Date 는 JDBC등을 이용해서 데이터베이스의 데이터를 사용하는데 적합하고, java.util.Date 은 보다 범용적인 날짜나 시각정보를 다룰때 적합하다고 한다. toString 메소드의 리턴 Format 형태 java.sql.Date : yyyy-mm-dd java.util.Date : EEE MMM dd HH:mm:ss zzz yyyy mybatis 에서 형변환은 mybatis 3 문서에 나와있는 자바타입과 JDBC타입이 일치할 경우에 해당 타입 핸들러를 기본으로 사용하게 된다. # 정상혁 님 조언 ( http://d2.naver.com/helloworld/645609 작성하신분 ) Oracle의 JDBC 드라이버가 예상 밖으로 동작하네요. Oracle의 DATE 타입도 문서를 보니 시분초까지 저장하게 되어 있는데, Oracle JDBC 구현체가 DATE 타입의 철학을 오해한게 아닌가하는 생각도 듭니다. 참고로 java.sql.Date, java.sql.TimeStamp는 잘못된 설계라는 비판이 많습니다. 저도 Java의 날짜와 시간 API 라는 글에서 아래와 같이 적은 적이 있습니다. 123java.sql.Date 클래스는 상위 클래스인 java.util.Date 클래스와 이름이 같다. 이 클래스를 두고 Java 플랫폼 설계자는 클래스 이름을 지으면서 깜빡 존 듯하다는 조롱까지 나왔다.[24]그리고 이 클래스는 Comparable 인터페이스에 대한 정의를 클래스 선언에서 하지 않았기 때문에 Comparable과 관련된 Generics 선언을 복잡하게 만들었다.[25]java.sql.TimeStamp 클래스는 java.util.Date 클래스에 나노초(nanosecond) 필드를 더한 클래스이다. 이 클래스는 equals() 선언의 대칭성을 어겼다. Date 타입과 TimeStamp 타입을 섞어 쓰면 a.equals(b)가 true라도 b.equals(a)는 false인 경우가 생길 수 있다.[26] 이런 이유 때문에 저는 가급적 Java8에 나온 ZonedDateTime 류를 모델객체에서는 쓰고 있기는합니다. 하지만 그 클래스도 JDBC 레벨에서 제대로 매핑을 안 해주는 경우가 있어서 converter, typeHandler류를 따로 만들어야합니다. 참고로 Spring JDBC에서는 아래와 같이 Converter를 만들어서 해결했습니다. 123456public class ZonedDateTimeConverter implements Converter&lt;Timestamp, ZonedDateTime&gt; &#123; @Override public ZonedDateTime convert(Timestamp source) &#123; return ZonedDateTime.ofInstant(source.toInstant(), ZoneId.of(\"UTC\")); &#125;&#125; java.sql.Date vs java.util.Date 둘 중에 선택한다면 모델 객체에서는 java.util.Date가 더 어울린다는 생각이 듭니다. 모델에 java.sql.Date가 있는 것은 Controller에서 SqlException이 있는 것 같은 비슷한 느낌이랄까요..^^; # 마치며 삽질을 하더라도 가급적이면 영양가 있는 삽질이 되야 할것같다. (하루종일 이것 붙잡다가 업무를 못해버리는;;) API문서, 블로그문서, 검색결과에 맹신하지말고 실제 소스까지 들어가봐서 확신을 갖자.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"date","slug":"date","permalink":"https://taetaetae.github.io/tags/date/"},{"name":"mybatis","slug":"mybatis","permalink":"https://taetaetae.github.io/tags/mybatis/"},{"name":"oracle","slug":"oracle","permalink":"https://taetaetae.github.io/tags/oracle/"}]},{"title":"스프링환경에서의 파라미터 관련 정리","slug":"spring-parameter","date":"2017-03-12T09:03:01.000Z","updated":"2018-07-29T08:31:26.457Z","comments":true,"path":"2017/03/12/spring-parameter/","link":"","permalink":"https://taetaetae.github.io/2017/03/12/spring-parameter/","excerpt":"일반적인 웹 프로젝트 구성에서는 Controller레벨에서 응답을 받고 비지니스 로직을 처리 후에 다시 View레벨로 넘어가는게 통상적인 흐름이다. 이 부분에서 파라미터 관련한 여러가지 부분에 대해 정리해보고자 한다.","text":"일반적인 웹 프로젝트 구성에서는 Controller레벨에서 응답을 받고 비지니스 로직을 처리 후에 다시 View레벨로 넘어가는게 통상적인 흐름이다. 이 부분에서 파라미터 관련한 여러가지 부분에 대해 정리해보고자 한다. httpServletRequest.getParameter()아래소스처럼 HttpServletRequest의 getParameter() 메서드를 이용하여 파라미터값을 가져올 수 있다.123456@RequestMapping(\"/\")public String home(HttpServletRequest httpServletRequest) &#123; String id = httpServletRequest.getParameter(\"id\"); return \"home\";&#125; @RequestParam또다른 방법으로는 @RequestParam 어노테이션을 이용하면 간단하게 파라미터값을 가져올수 있다. 우선, 해당 어노테이션의 옵션값들에 대해 간략하게 확인하고 넘어가는게 좋을듯 싶다. API문서 4.3.6 기준 이름 타입 설명 name, value (Alias for name) String 파라미터 이름 required boolean 해당 파라미터가 반드시 필수인지 여부, 기본값은 true defaultValue String 해당 파라미터의 기본값 위 옵션값들을 조합하여 컨트롤러 메소드에 적용해보면 아래 소스와 같이 만들어지고, 이렇게 reqeust에서 파라미터값을 가져올수 있다.1234@RequestMapping(\"/\")public String home(@RequestParam(value=\"id\", defaultValue=\"false\") String id) &#123; return \"home\";&#125; 이 어노테이션을 이용하게되면 자칫 잘못하다간 에러를 만날수가 있는데 required값을 true로 해놓고 (필수 파라미터 설정) 해당 파라미터를 사용하지 않고 요청을 보내게 되면 HTTP 400 에러를 받게 되니 각 옵션들을 정확히 확인하고 사용해야 할 것 같다.물론 컨트롤러의 메소드에서 해당 어노테이션을 사용하지 않고도 아래 코드처럼 바로 받을수 있다. 이렇게 바로 받을 경우는 필수 파라미터값이 false로 설정이 되고 변수명과 동일한 파라미터만 받을수 있게 되며 기본값 설정을 할수는 없다. 방법의 차이라서 상황에 따라 맞춰 사용하면 될듯 하다.1234@RequestMapping(\"/\")public String home(String id) &#123; return \"home\";&#125; @RequestBody@RequestBody어노테이션을 사용할 경우 반드시 POST형식으로 응답을 받는 구조여야만 한다. 이를테면 JSON 이나 XML같은 데이터를 적절한 messageConverter로 읽을때 사용하거나, POJO형태의 데이터 전체로 받을경우에 사용된다. 단, 이 어노테이션을 사용하여 파라미터를 받을 경우 별도의 추가 설정(POJO 의 get/set 이나 json/xml 등의 messageConverter 등)을 해줘야 적절하게 데이터를 받을수가 있다.1234@PostMapping(\"/\")public String home(@ReqeustBody Student student) &#123; return \"home\";&#125; @ModelAttribute@RequestParam과 비슷한데 1:1로 파라미터를 받을경우는 @RequestParam를 사용하고, 도메인이나 오브젝트로 파라미터를 받을 경우는 @ModelAttribute으로 받을수 있다. 또한 이 어노테이션을 사용하면 검증(Validation)작업을 추가로 할수 있는데 예로들어 null이라던지, 각 멤버변수마다 valid옵션을 줄수가 있고 여기서 에러가 날 경우 BindException 이 발생한다. Spring command 객체컨트롤러에서 파라미터로 받은 정보에 대해서는 view 에서 바로 사용이 가능하다. 예로 들어 아래그림처럼 이렇게 컨트롤러가 구성되어있고 이렇게 모델이 구성되어있을때 view 에서 이런식으로 구성되어있다고 가정해보자. 이때 /student?name=taetaetae&amp;age=32&amp;address=green-factory로 호출을 해보면 구지 Model에 값을 셋팅해주지 않아도 다음과 같이 정보를 읽을수 있게 된다. 마치며스프링에서 파라미터를 받는 방법은 상당히 다양하다. 이게 정답이다 정의할수 없을정도로. 상황에 따라 맞는 방법으로 파라미터를 받아야 하겠고, 각 방법에 장/단점을 최대한 살려서 좀더 깔끔한 코드를 작성할수 있어야 하겠다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"},{"name":"ReqeustParam","slug":"ReqeustParam","permalink":"https://taetaetae.github.io/tags/ReqeustParam/"},{"name":"RequestBody","slug":"RequestBody","permalink":"https://taetaetae.github.io/tags/RequestBody/"},{"name":"ModelAttribute","slug":"ModelAttribute","permalink":"https://taetaetae.github.io/tags/ModelAttribute/"},{"name":"spring command 객체","slug":"spring-command-객체","permalink":"https://taetaetae.github.io/tags/spring-command-객체/"}]},{"title":"벌써 3월, 다시 일어서야할 때","slug":"a-quarter-of-this-year","date":"2017-03-08T04:44:21.000Z","updated":"2018-07-29T08:31:26.008Z","comments":true,"path":"2017/03/08/a-quarter-of-this-year/","link":"","permalink":"https://taetaetae.github.io/2017/03/08/a-quarter-of-this-year/","excerpt":"벌써 3월이다. 뭐하나 제대로 한것도 없는데 시간은 야속하게도 멈추지 않고 지나가고 있다.오랜만에 동기형을 만났다.","text":"벌써 3월이다. 뭐하나 제대로 한것도 없는데 시간은 야속하게도 멈추지 않고 지나가고 있다.오랜만에 동기형을 만났다. 신입사원이 되기 전 연습생(?)시절 동거동락하며 개발에 대해 고군분투 하던 사이인지라. 오랜만에 만나도 서로 이야기 하고자 하는 주제는 언제나 동일하다. 개발자로서의 삶이런저런 이야기를 하며 다시 나를 돌아보게 되었다. 내 노력에 의해, 아니면 운이 좋아 지금 다니고있는 회사에 들어온 이후로 예전만큼의 열정은 온데간데 없으며, 그만큼 간절하지도 않고 치열하지도 않는 내 자신이 너무 미안하고 쪽팔릴정도로 한심하기 그지 없었다. 무엇때문일까, 도대체 왜 이렇게 안일해졌고 적극적이지 못하게 되었을까.그 질문에 대한 정답은 이것이다 라고 정의를 할수는 없겠지만 확실한건, 현재 내 상황에 안주하고 타협하려하는 마음가짐이 생겼다는건 회피할수 없을정도로 나도 정말 많이 변해버린것 같다. 물론 지금 상황이 잘못되었다는건 아니지만 내 직업 특성상 끊임없이 노력하고 도전하며 배워야 하는 상황인데 지금 난 퇴근하고 집에가면 쉬고싶고 자기 바쁘고 다음날 늦잠자고… 계속된 생활패턴에 젖어 사는것 같다.일단 독서좀 많이 해야겟다. 회사에, 집에 쌓인 책만 벌써 몇권인지… 기본이 되는 전공서적 하나 정하고 끝까지 완독해보자. 그게 자바든 스프링이든, 최신 신기술보다 기본이 탄탄해져야 하는건 백번 천번 말해도 당연하기에. 다시 정신차리자. 오늘 걷지 않으면 내일은 달려야 한다고 누군가 그랬듯..","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"github api 사용방법","slug":"github-api","date":"2017-03-02T02:18:05.000Z","updated":"2018-07-29T08:31:26.151Z","comments":true,"path":"2017/03/02/github-api/","link":"","permalink":"https://taetaetae.github.io/2017/03/02/github-api/","excerpt":"github 에서는 레파지토리의 전반적인 상황에 대해 다양한 API를 제공해주고 있다. 이번에는 그 API를 사용하는 방법에 대해 알아보고자 한다.","text":"github 에서는 레파지토리의 전반적인 상황에 대해 다양한 API를 제공해주고 있다. 이번에는 그 API를 사용하는 방법에 대해 알아보고자 한다. # Personal access tokens 발급우선 정상적인 API를 사용하기 위해 Personal access tokens를 발급받아야 한다. github 초기화면 &gt; 우측상단 프로필사진 클릭 &gt; setting &gt; Personal access tokens 에 들어가 토큰을 생성을 한다.해당 토큰의 허용범위를 설정한뒤 생성을 하면 만들어 지는데 여기서 발급되는 문자열은 따로 보관하는게 좋다. (나중에 다시 확인하려면 새로 재 생성하는 방법말고는 없기 때문에 한번 만들때 메모해 두는게 좋다.)아래와 같이 생성완료. # API 사용방법권한이 없는 Repository 의 내용을 확인할수 없듯이 github에서 제공하는 API또한 권한이 있는 Repository에 대해서만 API를 제공한다. 위에서 발급한 token 을 권한 체크할때 사용하는데 다양한 방법이 있을수 있겠으나 나는 간단하게 헤더에 포함시켜서 일반 GET 호출을 하는 방식으로 하였다. 윈도우 환경에서는 헤더 셋팅하고 호출하는게 조금 어려울수 있으니 이러한 부분을 설정할수 있는 Postman이라는 프로그램으로 호출을 해본다.아래처럼 url은 https://api.github.com/으로 설정하고 Headers파라미터에 Authorization라는 key에 value를 위에서 발급받았던 token을 이용하여 token abcd~~식으로 입력해준다음 send버튼을 눌러주면 응답을 받을수가 있는데, 아래 그림은 제공하는 api의 모든 url을 확인하는 방법이다.아래애서는 위에서 확인된 api url을 활용하여 내가 권한이 있는 레파지토리 내에서 확인할수있는 정보에 대한 API를 호출해보았다. API 호출시 가장 보편화되어있는(?) 스팩인 JSON으로 응답이 내려오기때문에 어떠한 환경에서도 충분히 활용할수 있을것이라 생각한다.나는 개인적으로 팀 내에서 하나의 Organizations내에 여러 Repository가 있는데 각각의 PullRequest에 대해 코드리뷰를 해야하는 상황에서 일일히 다 찾아보기 귀찮아 github-api를 활용해 open된 PullRequest가 있으면 알림을 주는 걸 만들어 보았다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"github","slug":"github","permalink":"https://taetaetae.github.io/tags/github/"}]},{"title":"리눅스상에서 json 파싱","slug":"shell-script-json","date":"2017-02-28T08:50:44.000Z","updated":"2018-07-29T08:31:26.432Z","comments":true,"path":"2017/02/28/shell-script-json/","link":"","permalink":"https://taetaetae.github.io/2017/02/28/shell-script-json/","excerpt":"리눅스 상에서 json형태의 String 을 파싱해야하는 상황이라면 아래 라이브러리를 사용해보는것을 추천해본다.","text":"리눅스 상에서 json형태의 String 을 파싱해야하는 상황이라면 아래 라이브러리를 사용해보는것을 추천해본다. # jq사용방법은 너무너무 간단하다. 자신의 시스템에 맞는 라이브러리를 다운받고 1234(32-bit system)$ wget http://stedolan.github.io/jq/download/linux32/jq(64-bit system)$ wget http://stedolan.github.io/jq/download/linux64/jq 실행 권한을 설정해 준 뒤 1chmod +x ./jq root 권한으로 해당 파일을 이동시킨다. 1sudo cp jq /usr/bin 실행은 다음과 같이 한다.Json String 이 아래와 같이 있다고 가정했을때 12345678910111213141516171819202122232425&#123;\"name\": \"Google\",\"location\": &#123; \"street\": \"1600 Amphitheatre Parkway\", \"city\": \"Mountain View\", \"state\": \"California\", \"country\": \"US\" &#125;,\"employees\": [ &#123; \"name\": \"Michael\", \"division\": \"Engineering\" &#125;, &#123; \"name\": \"Laura\", \"division\": \"HR\" &#125;, &#123; \"name\": \"Elise\", \"division\": \"Marketing\" &#125; ]&#125; 실제 사용과 결과는 다음과 같이 이루어 진다.1234567891011121314$ cat json.txt | jq &apos;.name&apos;&quot;Google&quot;$ cat json.txt | jq &apos;.location.city&apos;&quot;Mountain View&quot;$ cat json.txt | jq &apos;.employees[0].name&apos;&quot;Michael&quot;$ cat json.txt | jq &apos;.location | &#123;street, city&#125;&apos;&#123; &quot;city&quot;: &quot;Mountain View&quot;, &quot;street&quot;: &quot;1600 Amphitheatre Parkway&quot;&#125; 보다 자세한 사용방법은 공식홈페이지( https://stedolan.github.io/jq/ )를 참조하면 좋을듯 하다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://taetaetae.github.io/tags/linux/"},{"name":"json","slug":"json","permalink":"https://taetaetae.github.io/tags/json/"},{"name":"jq","slug":"jq","permalink":"https://taetaetae.github.io/tags/jq/"}]},{"title":"eclipse에서 spring-boot로 web 만들기","slug":"spring-boot-eclipse","date":"2017-02-27T05:37:27.000Z","updated":"2018-07-29T08:31:26.454Z","comments":true,"path":"2017/02/27/spring-boot-eclipse/","link":"","permalink":"https://taetaetae.github.io/2017/02/27/spring-boot-eclipse/","excerpt":"Spring 환경에서 웹 어플리케이션을 만들어야 한다면 pom.xml 에 이런저런 설정들을 적어줘야 했다. 하지만 이런 수고(?)를 덜어줄수 있는 방법중에 한가지가 바로 Spring Boot로 만드는 방법인데, 이클립스 환경에서 만드는 법을 정리하고자 한다.","text":"Spring 환경에서 웹 어플리케이션을 만들어야 한다면 pom.xml 에 이런저런 설정들을 적어줘야 했다. 하지만 이런 수고(?)를 덜어줄수 있는 방법중에 한가지가 바로 Spring Boot로 만드는 방법인데, 이클립스 환경에서 만드는 법을 정리하고자 한다. # new &gt; Maven Project빈 Maven Project 를 만드는 방법은 아주 간단하니 생략하고… 만들게 되면 pom.xml 은 아래처럼 아주 깔끔한(?)상태로 만들어지게 된다.12345678&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;boot&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 그러면 이 비어있는 pom.xml 에 Spring-Boot 에 필요한 설정들을 추가해주기로 한다.12345678910111213141516171819202122&lt;parent&gt; &lt;!--boot의 스타터를 사용하겠다고 명시적으로 설정--&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;relativePath /&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;!--boot에서 스타터패키지로 제공해주는 것들중에 web 설정 부분 --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 그다음 임의의 java 클래스를 하나 만들고 거기에 아래처럼 설정하면 끝123456@SpringBootApplication // @Configuration + @EnableAutoConfiguration + @ComponentScan 들의 종합 어노테이션public class TestApplication&#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(TestApplication.class, args); &#125;&#125; Spring Boot 에서는 내장WAS를 가지고 있기 때문에 main 메소드에서 우클릭후 run AS → Spring Boot App 을 선택해주면 8080포트로 띄워지게 된다. # new &gt; Spring Stater project(STS가 설치되어있다는 가정하에)이 메뉴를 사용하면 위에서 했던 일련의 설정들을 자동으로 해주게 된다. 간단한 내용이니 next를 해주다 마지막에 Dependencies 설정하는 부분에서 Web 을 체크해주고 Finish 버튼을 누르면 끝 # Spring Initializr (start.spring.io)http://start.spring.io/ 에 들어가보면 구지 설명하지 않아도 친절하게 Generate 해주는 페이지가 보인다. 여기서 web 을 Dependencies에 추가하고 Generate를 하면 해당 프로젝트가 압축된 상태로 다운이 받아지게 되고 이를 IDE 에서 열어보면 위에서 했던 일련의 과정들이 설정되어 있는것을 확인해볼수가 있다. # 내장톰켓을 사용안하고 별도 톰켓을 사용해야 하는 경우Spring boot는 자체적으로 내장 WAS를 가지고 있다. 하지만 관리포인트나 이런저런 이유로 내장톰켓을 사용하지 못하는 환경이라면 다음과 같은 설정을 해주면 된다. 일반적으로 빌드가 되면 jar로 만들어 질텐데 war로 빌드 되도록 수정을 해야한다. (was가 WAR를 물고 떠야하기 때문..) 1&lt;packaging&gt;war&lt;/packaging&gt; dependency 에 tomcat을 추가해준다. 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 아래처럼 main 메소드가 있는 클래스에 SpringBootServletInitializer를 상속받게 한 후 configure메소드를 오버라이딩 해준다. 12345678910@SpringBootApplicationpublic class TestApplication extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(TestApplication.class); &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(TestApplication.class, args); &#125;&#125; 톰켓에 띄우기 위하여 프로젝트 설정(Project Facets)에서 Dynamic Web Module을 체크해준다. 참고 URL http://www.donnert.net/86 http://opennote46.tistory.com/124","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"spring-boot","slug":"spring-boot","permalink":"https://taetaetae.github.io/tags/spring-boot/"},{"name":"eclipse","slug":"eclipse","permalink":"https://taetaetae.github.io/tags/eclipse/"}]},{"title":"lombok(롬복)소개 및 설치","slug":"lombok","date":"2017-02-22T08:48:42.000Z","updated":"2018-07-29T08:31:26.232Z","comments":true,"path":"2017/02/22/lombok/","link":"","permalink":"https://taetaetae.github.io/2017/02/22/lombok/","excerpt":"일반적으로 자바개발을 하다보면 Model 을 만들고 각 멤버변수를 접근할수 있는 (각 요소들이 private 접근권한을 가지고 있을때) method 를 만들게 된다. IDE에서 제공하는 아래처럼… (윈도우/이클립스 기준)","text":"일반적으로 자바개발을 하다보면 Model 을 만들고 각 멤버변수를 접근할수 있는 (각 요소들이 private 접근권한을 가지고 있을때) method 를 만들게 된다. IDE에서 제공하는 아래처럼… (윈도우/이클립스 기준) get/set 메소드 : Alt + Shift + S + R toString 메소드 : Alt + Shift + S + S 기타 등등…12345678910111213141516171819202122232425262728293031323334353637383940414243public class Student &#123; private int id; private String name; private int grade; private String department; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getGrade() &#123; return grade; &#125; public void setGrade(int grade) &#123; this.grade = grade; &#125; public String getDepartment() &#123; return department; &#125; public void setDepartment(String department) &#123; this.department = department; &#125; @Override public String toString() &#123; return \"Student [id=\" + id + \", name=\" + name + \", grade=\" + grade + \", department=\" + department + \"]\"; &#125; &#125; 이렇게 하는 방법도 있지만 어노테이션 설정으로 적용할수 있는 간단한 라이브러리를 소개하고자 한다.바로 lombok, 공식 홈페이지 : https://projectlombok.org설치 및 사용방법은 아주 간단하다. 공식 홈페이지에서 jar를 다운받고 실행, 아래처럼 이클립스 실행파일 경로를 설정해준다음에 인스톨을 누르면 된다.maven 환경에서 dependency를 가져오기 위해서는 당연히 추가설정을 해줘야 한다.12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.10&lt;/version&gt; &lt;!--버전은 그때 맞춰서--&gt;&lt;/dependency&gt; 실제로 코드상에서 사용방법은 다음과 같다. 정말 간단히, 어노테이션만 적용해주면 끝!123456789import lombok.Data;@Datapublic class Student &#123; private int id; private String name; private int grade; private String department;&#125; 그럼 이렇게 기본적인 method들이 생성된다.일반적으로 @Data를 사용하고 상황에 따라 필요한 어노테이션만 지정도 가능하다고 한다. @Getter and @Setter @NonNull @ToString @EqualsAndHashCode @Data @Cleanup @Synchronized @SneakyThrows참고 URL : http://jnb.ociweb.com/jnb/jnbJan2010.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"lombok","slug":"lombok","permalink":"https://taetaetae.github.io/tags/lombok/"}]},{"title":"logback 설정하기","slug":"logback","date":"2017-02-19T06:10:45.000Z","updated":"2018-07-29T08:31:26.230Z","comments":true,"path":"2017/02/19/logback/","link":"","permalink":"https://taetaetae.github.io/2017/02/19/logback/","excerpt":"자바 개발자라면 한번쯤은 들어봤고, 한번쯤은 사용했을법한 logger 로 log4j가 있을것이다. 하지만 최근들어 logback이라는것을 알게되었고, 왜 logback을 사용해야 하는 이유라는 글이 있을정도로 여러 측면에서 개선이 된듯 하다.","text":"자바 개발자라면 한번쯤은 들어봤고, 한번쯤은 사용했을법한 logger 로 log4j가 있을것이다. 하지만 최근들어 logback이라는것을 알게되었고, 왜 logback을 사용해야 하는 이유라는 글이 있을정도로 여러 측면에서 개선이 된듯 하다. (링크)이번에 작성할 글의 목적은 logback을 설정하고 어떻게 사용하는지에 대해 작성해 보고자 한다.※ 공식사이트 : https://logback.qos.ch/ # pom.xmlmaven구조라고 가정했을때 logback Dependency를 가져오기 위해서는 아래와 같이 pom.xml 에 설정해 주면 된다.12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.7&lt;/version&gt; &lt;!--버전은 상황에 따라 --&gt;&lt;/dependency&gt; # 로그레벨ERROR, WARN, INFO, DEBUG or TRACE # logback 설정파일일반적으로 logback.xml 이라는 이름으로 만들어 src/main/resources/아래에 위치하게 된다. Spring-Boot 환경에서는 logback-spring.xml 이라는 이름으로 설정해야 하는데 logback.xml로 설정하면 스프링부트가 설정하기 전에 로그백 관련한 설정을 하기 때문에 제어할 수가 없게 된다.( 공식사이트 메뉴얼 : https://logback.qos.ch/documentation.html )12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;include resource=\"org/springframework/boot/logging/logback/defaults.xml\" /&gt; &lt;include resource=\"org/springframework/boot/logging/logback/console-appender.xml\" /&gt; &lt;!-- 변수 지정 --&gt; &lt;property name=\"LOG_DIR\" value=\"/logs\" /&gt; &lt;property name=\"LOG_PATH_NAME\" value=\"$&#123;LOG_DIR&#125;/data.log\" /&gt; &lt;!-- FILE Appender --&gt; &lt;appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;LOG_PATH_NAME&#125;&lt;/file&gt; &lt;!-- 일자별로 로그파일 적용하기 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH_NAME&#125;.%d&#123;yyyyMMdd&#125;&lt;/fileNamePattern&gt; &lt;maxHistory&gt;60&lt;/maxHistory&gt; &lt;!-- 일자별 백업파일의 보관기간 --&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%F]%M\\(%L\\) : %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;layout class=\"ch.qos.logback.classic.PatternLayout\"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%F]%M\\(%L\\) : %m%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- TRACE &gt; DEBUG &gt; INFO &gt; WARN &gt; ERROR, 대소문자 구분 안함 --&gt; &lt;!-- profile 을 읽어서 appender 을 설정할수 있다.(phase별 파일을 안만들어도 되는 좋은 기능) --&gt; &lt;springProfile name=\"local\"&gt; &lt;root level=\"DEBUG\"&gt; &lt;appender-ref ref=\"FILE\" /&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;springProfile name=\"real\"&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"FILE\" /&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt; &lt;/springProfile&gt;&lt;/configuration&gt; # java 코딩에서의 로깅실제 사용은 다음과 같이 LoggerFactory를 이용해서 사용하거나 Lombok어노테이션을 활용하면 심플하게 사용이 가능하다. LoggerFactory 사용 12345678910import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class Foo &#123; static final Logger logger = LoggerFactory.getLogger(Foo.class); public void test() &#123; logger.debug(\"ID : &#123;&#125;\", \"foo\"); &#125;&#125; Lombok 어노테이션 사용 123456789import lombok.extern.slf4j.Slf4j;@Slf4jpublic class Foo &#123; public void test() &#123; log.debug(\"ID : &#123;&#125;\", \"foo\"); &#125;&#125; # 마치며일반적인 웹 어플리케이션에서는 WAS에서 로깅을 따로 관리하고 있기 때문에 file 로 로깅을 할 필요는 없을것 같다.(일반 jar 형태에서는 파일 로깅이 필요 할수도…) # 참고사이트 http://yookeun.github.io/java/2015/11/10/log4jtologback/ http://java.ihoney.pe.kr/397 https://logback.qos.ch/","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"logback","slug":"logback","permalink":"https://taetaetae.github.io/tags/logback/"}]},{"title":"자바 8 Date","slug":"java8-date","date":"2017-01-10T11:55:33.000Z","updated":"2018-07-29T08:31:26.198Z","comments":true,"path":"2017/01/10/java8-date/","link":"","permalink":"https://taetaetae.github.io/2017/01/10/java8-date/","excerpt":"이제까지 내 기억으로는 Date 관련 클래스를 아래처럼 점차 바꿔써온걸로 기억이 난다.java.util.Date &gt; java.util.Calendar &gt; org.joda.time그런데 java 8 버전에서 기존에 있었던 문제들을 개선해서 나왔다고 한다. (네이버 HellowWorld 포스팅 참고) JSR-310 이라는 표준명세로.","text":"이제까지 내 기억으로는 Date 관련 클래스를 아래처럼 점차 바꿔써온걸로 기억이 난다.java.util.Date &gt; java.util.Calendar &gt; org.joda.time그런데 java 8 버전에서 기존에 있었던 문제들을 개선해서 나왔다고 한다. (네이버 HellowWorld 포스팅 참고) JSR-310 이라는 표준명세로.지금부터는 JAVA 8 에서 제공하는 API로 날짜 연산을 어떻게 하는지에 대해 알아보고자 한다. (물론 수많은 날짜 연산 방법을이 있지만 자주 쓰이는 부분들 위주로 정리해보자.) Date &gt; String (format) 1LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); String &gt; Date (format) 1LocalDateTime.parse(\"2017-01-01 12:30:00\", DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); 날짜/시간 증감 1234567LocalDateTime localDateTime = LocalDateTime.of(2017, 1, 1, 10, 0, 0);localDateTime.plusDays(1); // 일localDateTime.plusMonths(1); // 월localDateTime.plusHours(1); // 시간localDateTime.plusWeeks(1); // 주localDateTime.minusYears(1); // 년localDateTime.minusMinutes(1); // 분 더 다양한 내용들은 아래 URL 에서 확인이 가능하다.https://docs.oracle.com/javase/tutorial/datetime/iso/overview.html","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"java","slug":"java","permalink":"https://taetaetae.github.io/tags/java/"},{"name":"date","slug":"date","permalink":"https://taetaetae.github.io/tags/date/"}]},{"title":"Spring Transactional 설정 및 주요속성","slug":"transactional-setting-and-property","date":"2017-01-08T08:19:30.000Z","updated":"2018-07-29T08:31:26.468Z","comments":true,"path":"2017/01/08/transactional-setting-and-property/","link":"","permalink":"https://taetaetae.github.io/2017/01/08/transactional-setting-and-property/","excerpt":"지난번에는 트랜잭션의 설정값에 대해 알아본 바 있다. [ Spring Transaction 옵션 ]이번 포스팅에서는 실제로 스프링 환경에서 어떤식으로 설정해야 @Transactional 어노테이션을 사용할수 있는지, 그리고 어떤 속성들이 있는지에 대해 알아보고자 한다.","text":"지난번에는 트랜잭션의 설정값에 대해 알아본 바 있다. [ Spring Transaction 옵션 ]이번 포스팅에서는 실제로 스프링 환경에서 어떤식으로 설정해야 @Transactional 어노테이션을 사용할수 있는지, 그리고 어떤 속성들이 있는지에 대해 알아보고자 한다. # 설정기존 xml방식에서는 다음과 같이 설정을 한다.1234&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\" proxy-target-class=\"true\"/&gt; 혹, JavaConfig 방식으로 설정하기 위해서는 다음과 같이 설정한다.123456789@EnableTransactionManagementpublic class AppConfig &#123; ... @Bean public PlatformTransactionManager transactionManager() throws URISyntaxException, GeneralSecurityException, ParseException, IOException &#123; return new DataSourceTransactionManager(dataSource()); &#125;&#125; 위와같이 설정을 해주면 트랜잭션을 설정하고자 하는 곳 어디서든 @Transactional 어노테이션을 지정해서 적용이 가능하다.1234567public class UserService&#123; @Transactional public boolean insertUser(User user)&#123; ... &#125;&#125; # 주요속성@Transactional 어노테이션의 주요속성은 다음과 같다. 속성 설 명 사용 예 isolation Transaction의 isolation Level. 별도로 정의하지 않으면 DB의 Isolation Level을 따름. @Transactional(isolation=Isolation.DEFAULT) propagation 트랜잭션 전파규칙을 정의 , Default=REQURIED @Transactional(propagation=Propagation.REQUIRED) readOnly 해당 Transaction을 읽기 전용 모드로 처리 (Default = false) @Transactional(readOnly = true) rollbackFor 정의된 Exception에 대해서는 rollback을 수행 @Transactional(rollbackFor=Exception.class) noRollbackFor 정의된 Exception에 대해서는 rollback을 수행하지 않음. @Transactional(noRollbackFor=Exception.class) timeout 지정한 시간 내에 해당 메소드 수행이 완료되지 않은 경우 rollback 수행. -1일 경우 no timeout (Default = -1) @Transactional(timeout=10) # 마치며자칫 잘못했다가는 원치않는 트랜잭션으로 잘못된 결과를 초래할수 있기때문에 기본값은 숙지하는게 좋을것 같다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"Transaction","slug":"Transaction","permalink":"https://taetaetae.github.io/tags/Transaction/"},{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"}]},{"title":"spring4에서 json view 활용하기(with @ResponseBody)","slug":"spring4-json","date":"2017-01-07T06:47:59.000Z","updated":"2018-07-29T08:31:26.465Z","comments":true,"path":"2017/01/07/spring4-json/","link":"","permalink":"https://taetaetae.github.io/2017/01/07/spring4-json/","excerpt":"수많은 블로거분들의 도움을 받고자 구글링을 해서 적용을 해봤지만 너무많은 삽질을 했다.(해봤던 방식은 jsonViewResolver 를 따로 설정해보거나, @RequestMapping 옵션을 바꿔보는 수준..) 특히나 Spring설정방식이 예전 방식이였던 xml이 아닌 javaconfig였기 때문에 더욱더 자료가 없었고..","text":"수많은 블로거분들의 도움을 받고자 구글링을 해서 적용을 해봤지만 너무많은 삽질을 했다.(해봤던 방식은 jsonViewResolver 를 따로 설정해보거나, @RequestMapping 옵션을 바꿔보는 수준..) 특히나 Spring설정방식이 예전 방식이였던 xml이 아닌 javaconfig였기 때문에 더욱더 자료가 없었고.. 한참을 삽질하다 해결을 하여 포스팅하게 된다. 우선 환경은 spring 4.3.4.RELEASE, Maven, jdk8임을 밝힌다. # pom.xmljackson-mapper-asl을 이용해서 하라는 블로거들도 있었지만, 아무리해도(뭔가 Spring버전과 맞지 않는듯 했다.) 잘 안되어 아래와 같은 dependency를 주었다.12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;/dependency&gt; # Controller아래와같이 @ResponseBody 어노테이션을 설정해주고 리턴은 해당 모델을 넘기면 된다.12345678@RequestMapping(value=\"/test\")@ResponseBodypublic Map&lt;String, Object&gt; test()&#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(\"1\", \"111\"); map.put(\"2\", 222); return map; &#125; 그리고 호출을 해보면 기대했던것처럼 이쁘게 json형태로 나온다.12345&#123;\"1\": \"111\",\"2\": 222&#125;물론, list 나 array, 일반 객체도 가능하다. # 정리삽질을 끝에 알게된 사실(?)을 정리해보자.다른측면에서 분석을 해보면. @ResponseBody을 이용하여 view 에 json 형태로 나타내고자 할 경우 가능한 상황은 toString으로 했을때 json형태로 나올수 있으면 가능하다. 예로들어 아래처럼 클래스에 Lombok 어노테이션인 @Data가 붙으면 자동으로 toString을 오버라이딩 해주기 때문에 해당 클래스를 리턴하게되면 자동으로 json 처리가 된다. 123456@Datapublic Student&#123; private String id; private String name; ...&#125; @ResponseBody을 붙이고 List&lt;Student&gt;를 리턴하게 되면 에러가 나는데, 이럴경우 별도 라이브러리를 추가해줘야 자동으로 변환되어 json 형태로 나올수 있게 된다. (list.toString을 하면 json형태가 아닌 이상한 문자형태로 나오기 때문… Map같은것도 마찬가지 이유로 별도 라이브러리를 추가해줘야 정상적으로 나온다.) # 마치며단순히 @ResponseBody를 사용해서 json으로 리턴하려면 어떤 라이브러리를 추가해야한다 로 생각했던것에서, 이것저것 테스트 한 결과 toString을 할수 있어야 하고 그 값이 json형태이면 가능하다 로 결론이 지어졌다. 확실히 장님 코끼리 만지듯이 ‘그런가보다’하고 넘어가면 삽질이 진짜 불필요한 삽질이 되는것 같다. 구글링을 해보고, 테스트를 해봐서, 결론적으로 내것으로 만드는 습관을 가져야 겠다.","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"json","slug":"json","permalink":"https://taetaetae.github.io/tags/json/"},{"name":"spring","slug":"spring","permalink":"https://taetaetae.github.io/tags/spring/"}]},{"title":"jsp include","slug":"20170104","date":"2017-01-04T09:36:17.000Z","updated":"2018-07-29T08:31:26.005Z","comments":true,"path":"2017/01/04/20170104/","link":"","permalink":"https://taetaetae.github.io/2017/01/04/20170104/","excerpt":"1. 디렉티브방식1&lt;%@ include file=\"~~\"%&gt; 정적 include 방식, 인클루드 되는 내용이 단순하게 텍스트로 포함되어 컴파일이 된다. (복사된다는 느낌)","text":"1. 디렉티브방식1&lt;%@ include file=\"~~\"%&gt; 정적 include 방식, 인클루드 되는 내용이 단순하게 텍스트로 포함되어 컴파일이 된다. (복사된다는 느낌) 주의할점은 비록 포함되는 페이지라 하더라도 한글을 제대로 처리하기 위해서는 포함되어지는 jsp파일 상단에 인코딩 명시를 해줘야 한다. 포함되어지는 jsp 내용이 변경이 될 경우 해당 jsp를 사용하는 jsp를 강제로 변경(touch) 해줘서 다시 컴파일이 되게 해야하는 불편함이 있다. 정적 방식이기 때문에 예로들어 전역변수를 인클루드 되는 jsp에서 지정하게 되면 상위jsp에서 사용이 가능하게 된다. 2. 액션태그 방식1&lt;jsp:include page=\"~~\"/&gt; 동적 include 방식, 포함하는 문서와 상관없이 동적으로 컴파일 된다. (완전히 별도로 동작하기 때문에 변수를 동시에 사용하려면 따로 파라미터로 넘겨줘야 한다.) flush 옵션은 요청흐름이 넘어가면서 현재까지 페이지의 결과를 출력할 것인지 말것인지를 결정하는것이다. 일반적으로 false로 설정한다. &lt;jsp:param&gt;를 이용하여 파라미터를 전송할수 있다.1234&lt;jsp:include page=\"...\" flush=\"false\"&gt; &lt;jsp:param name=\"name\" value=\"이름\" /&gt; &lt;jsp:param name=\"pageName\" value=\"페이지이름\"/&gt;&lt;/jsp:include&gt; 3. JSTL 방식1&lt;c:import url=\"~~\" /&gt; JSTL(JSP Standard Tag Library) 태그중의 하나 컴파일 되고 동작하는 방식은 액션태그&lt;jsp:include page=&quot;~~&quot;/&gt;와 같음 현재 컨테이너 안에 있는 자원외에 다른 외부 자원도 포함이 가능하다. 1&lt;c:import url=\"http://www.google.com/\"/&gt; 아래와 같이 보다 더 다양한 옵션이 제공된다12345&lt;c:import! url=&quot;읽어올 URL&quot; var=&quot;읽어올 데이터를 저장할 변수명&quot; scope=&quot;변수의 공유 범위&quot; varReader=&quot;리소스의 내용을 Reader 객체로 읽어올 때 사용&quot; charEncoding=&quot;읽어온 데이터의 캐릭터셋 지정&quot; /&gt;","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"jsp","slug":"jsp","permalink":"https://taetaetae.github.io/tags/jsp/"}]},{"title":"2017 버킷리스트","slug":"hello-2017","date":"2017-01-01T06:49:31.000Z","updated":"2018-07-29T08:31:26.186Z","comments":true,"path":"2017/01/01/hello-2017/","link":"","permalink":"https://taetaetae.github.io/2017/01/01/hello-2017/","excerpt":"올해도 어김없이(?) 1월 1일이 되어 해맞이(해돋인지 해맞인지 햇갈리지만, 새해 첫날부터 복잡해지기 싫당 =ㅁ=)를 다녀오고 까페에서 새해 계획을 세워본다. 정말 지킬수 있는 계획들, 현실적인 부분들만 고려해서 써내려 가보자. 절반 이상만이라도 지킬수만 있다면 그나마 다행이라고 생각!","text":"올해도 어김없이(?) 1월 1일이 되어 해맞이(해돋인지 해맞인지 햇갈리지만, 새해 첫날부터 복잡해지기 싫당 =ㅁ=)를 다녀오고 까페에서 새해 계획을 세워본다. 정말 지킬수 있는 계획들, 현실적인 부분들만 고려해서 써내려 가보자. 절반 이상만이라도 지킬수만 있다면 그나마 다행이라고 생각! 기술블로그 운영하기 : 월 2회 posting 내가 아는지식이 얼만큼인지, 보여주기식이 아닌 내 머릿속에 자리잡고 있는 부분들을 정리해서 기록화 하는 이름하야 기술블로그를 작성하는거다. 2주에 최소 하나씩, 이렇게 되면 한달에 최소 2post, 1년이면 약 20post. 작다고 해도 마냥 작게만 느껴지지 않을 분량이다. 사소한거 하나라도. 이를테면 서버 설치나 스프링의 기본 설정 관련된 것들도. 글쓰는 연습도 하고 좋은 기회가 될것 같다. 4대강 종주 : 영산강, 1박2일코스 2014년에 한강(북한강, 남한강), 2015년에 금강, 2016년에는 못갔다. 4대강 종주의 목표가 갑자기 시들어진 작년이라 생각이 든다. 우여곡절 산전수전 다겪은 내 자전거 붕붕이에게 미안하지 않기위해서라도 올해 여름에는 꼭 영산강이나 낙동강 하나를 계획잡아 1박2일 코스로 다녀와야겠다. 음, 대략 5월? 아마 영산강을 가게될것같다. 이번에는 무리하지 않고 1박2일코스로.. 독서 : 월 전공1권, 전공외1권 작년에 내 입에서 나왔던 이야기들중에 한심스럽게(?)나온 멘트중 가장 많이나왔던 책좀읽자 올해에는 정말 많이는 아닐지라도 자주읽는 습관을 길러야겠다. 한달에 전공책 한권, 기타서적 한권. 얇은책+읽고싶은책 부터 읽기 시작해서 내년 1월1일때는 내 책상 한켠에 자리잡고있는 책장을 가득 메워보고싶다. 아, 물론 다 읽은 책들로만. 여행+사진 : 한달에 한번이상 여행가기 해외든 국내든, 올해는 정말 많이 다녀와야겠다. 가볍게 당일치기부터 시작해서 갈수만 있다면 해외여행도. 물론 올해도 야근과의 싸움은 계속될테지만 주말 잠깐이라도 시간을 내서 두달에?아니 한달에 한번이라도 휴가를 써서라도 가까운곳에 힐링하러 다녀오고 싶다. 가서 작년에 산 카메라로 사진도 이것저것 많이 찍고 좋은추억 많이 만들고오고 싶다. 저축+a : 근검절약의 생활화, 경제공부 나름 월급의 60%이상을 저축하는 중이다. 그치만 상황이 상황인지라 지금도 만족하지 못한다. 천장에 굴비 달아놓고 간장찍어 먹는다는게 아니라 아낄수 있는 부분들은 최대한 아끼면서 살자는거다. 아침에 택시 타지말고 조금 일찍 일어나서 버스를 탄다던지, 버스를 타지말고 조금일찍 일어나서 자전거를 탄다던지(사실 자전거를 타면 퇴근할때 더 빠르고 편하게 올수 있으니) 생활속에서 절약할수 있는 부분들을 찾고, 몸에 베도록 습관화 시켜야겠다. 그리고 주식이나 펀드 등 투자에 대해서도 이제 공부를 해봐야겠다. (독서하자는거랑 비슷한 이야기) 저금리시대 마냥 저축만 하다보면 힘든건 누구나 다 아는이야기. 일에 치여 생활에 치여 핑계대지말고 배워가면서 챙겨보자. 운동 : 자유형마스터, 몸짱 항상 하는 이야기지만 건강보다 중요한건 없다고 생각한다. 올해에도 병원가지 않는 나를 만들기 위해 헬스 + 배드민턴 + 라이딩 은 필수고 가능하면 수영도 배워서 자유형 정도는 할수있는 나를 만들고 싶다. 그래서 다들 말하는 몸짱도 되보고 싶고 자신있게 해변가에서 상의를 탈의할수있는(?!) 건강한 내가 되도록 노력해야겠다. 봉사활동하기 : 연탄배달, 자원봉사 작년에 하려다가 못한 봉사활동 올해는 꼭 해야겠다. 가깝게 할수있을법한게 연탄배달, 이건 1월달 내로 꼭! 해서 봉사라는것과 나눔이라는 행복을 느껴보고 싶다. 지금 생각나는건 자원봉사 같은것도 해보고 싶고 무보수 알바(?) 같은것도 해보고싶다. 나이들면 못할, 언제 해보겠나. 가까운, 먼 사람들 만나기 마지막으로, 잊고있었던 중요한 행동. 바로 사람들 만나기다. 바쁘다는 핑계 하나만으로 등한시한 내 소중한 사람들. 아무리 연봉을 많이 받고 일을 잘한다고 회사에서 잘나간다 할지라도, 내 곁엔 나를 생각해주는 소중한 사람들이 있기에 내가 있을수 있는것 같다. 가까운 사람들부터 시작해서 오랬동안 못봤던 사람들도 하나둘씩 연락하면서 지내는 여유를 가져야 겠다. 할수 있을까? 라는 생각보다 하나둘씩 잊지말고, 놓치지 말고 하루를, 이번주를, 이번달을 점검하고 실천해 나가보자.올 한해도 열심히 최선을 다해 살것!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"2016 회고","slug":"adieu-2016","date":"2016-12-31T07:59:43.000Z","updated":"2018-07-29T08:31:26.014Z","comments":true,"path":"2016/12/31/adieu-2016/","link":"","permalink":"https://taetaetae.github.io/2016/12/31/adieu-2016/","excerpt":"2016년, 내겐 정말 수많은 일들이 있었고 그 어느때보다 (전역 후로) 미친듯이 회사에 집중했던 시간들로 기억난다. 무작정 다가오는 새해를 맞이하는것도 좋지만 올 한해를 되돌아보는 시간을 갖고, 나를 다시 점검해보는 차원에서 일명 ‘회고’를 해볼까 한다.","text":"2016년, 내겐 정말 수많은 일들이 있었고 그 어느때보다 (전역 후로) 미친듯이 회사에 집중했던 시간들로 기억난다. 무작정 다가오는 새해를 맞이하는것도 좋지만 올 한해를 되돌아보는 시간을 갖고, 나를 다시 점검해보는 차원에서 일명 ‘회고’를 해볼까 한다. # 회사정말 열심히 했다. 잘했는지는… 잘 모르겠다. 난 잘한것 같다. 물론 내 하루중에 가장많은 시간을 쏟은것도 있지만 작년에 많이 하지 못하던것을 ‘날씨’라는 서비스를 홀로 맡으면서 정말 많은것을 배우고 결과물도 후회하지 않을만큼 나온것 같다. 지나고보면 구지 하지 않아도 월급은 똑같이 나올테고, 시키지도 않았는데 그시간에 잠을 더 잤으면 하는 생각도 들지만 후회하지 않는다. 아무도 없는 사무실 나혼자 12시넘어서 퇴근을 해도 즐거웠으니까, 그거면 됬다.모바일 개편이라는 큰 업무를 무사히(?) 해쳐내고는 사내에서 조직(서비스)을 변경할수 있는 기회가 되어 나홀로 지원, 다행스럽게도 합격을 해서 지금은 네이버페이 와 관련된 일을 하고있는 중이다. 기존 서비스운영을 하면서 느끼지 못했던, 초기 설계부터 시작하여 어떤 기술스택을 쓸것인가에 대한 선택부터 다양한 시행착오를 통해 이제 한 두달 되었는데 정말 많이 배우고 있다. 너무 힘들지만 너무 행복하다.돌이켜보면 작심삼일로 개발 관련된 공부를 등한시 한게 너무 후회가 된다. 바쁘다는 이유하나만으로 (솔직히 바쁘다는건 핑계다) 기능구현에만 신경을 써왔는데, 내년부터는 할수만 있다면 업무 외적으로 나만의 개발트리를 세워보고 싶다. # 건강일주일에 한번 이상 오전엔 배드민턴, 저녁엔 헬스장엘 가려고 노력했다. 그 결과였을까, 사랑니 뺀거 말고는 병원을 단한번도 안갔다. 감기조차 걸리지 않는게 다행이라고 생각하지만, 요즘들어(야근이 많아져서인지) 책상에 앉아있는 자세가 불량해서 거북목이 되가고 있다. 폼롤러도 구비했고 어깨 펴지라고 밴드도 구입해서 사용은 하는데 잘 실천이 안되는 중이다.작년에 자전거를 잃어버리다 되찾고는 자전거를 등한시 하게 되는것 같다. 이또한 핑계겠지. 내년엔 꼭 4대강중 하나 잡고 종주한번 해야겟다. 기필코.. 아맞다 수영도. ㅠㅠ 물에 뜨질 않으니 큰일이다… # 사람관계학교선후배동기 및 동아리 사람들, 군대 동기들 및 소대원들 과 선임 장교분들, 기타 등등… 올해 들어서인지. 연락에 너무 무색할만큼 잊고 살았던것 같다. 지나고보면 다른곳에 신경쓴다고 연락을 못했다고 핑계를 대고 있는 나이지만, 또 한편으로는 그 연락 10분 시간이 없다는건 … 역시나 핑계다. 나를 도와주고 나를 믿어주고 나를 생각해주는 사람들을 조금이라도 더 신경써서 연락하고 찾아 뵙는 시간을 내년부터서라도 가져야겠다. # 마치며일단 첫번째로 내년부터 할일은, 기술 블로그를 운영하는것이다. 솔직히 두달전 이 gitHub 를 이용해서 블로그를 만들긴 했지만 그닥 포스팅도 못했고 방치 수준이였으니… 적어도 한달에 한두개 정도는 포스팅 해보려고 노력해야겟다. 글쓰는게 힘들고 시간이 많이 들어가는 작업이지만, 돌이켜 생각하면 다 내 자산이고 나를 다시 바라볼수 있는 기회니까. 꼭 기술블로그만이 아닌, 하루를 기록하는 무언가를 해야겠다. 막상 한해를 돌이켜보니 그때는 뭐했는지 기억도 안나네..두번째로는 지킬수 있는 계획을 잡는것이다. 올 한해 목표중에 이룬건 10개중에 단 두개… (그중에 노래대회나가기, 스쿠버다이빙 하기, 자유형 마스터하기도 있다;;) 부끄럽다.. 2016년, 나라도 뒤숭숭 하고 정신없던 한해였지만 나름 의미있던 시간들을 보낸것 같아 다행이라 생각한다.음,. 10점만점에 8점??2017년! 다시한번 일어서자! 화이팅!!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]},{"title":"Spring Transaction 옵션","slug":"20161008","date":"2016-10-08T09:04:19.000Z","updated":"2018-07-29T08:31:26.003Z","comments":true,"path":"2016/10/08/20161008/","link":"","permalink":"https://taetaetae.github.io/2016/10/08/20161008/","excerpt":"상황스프링 환경에서 일반적으로 DAO 나 BO 레벨에서 다음과 같이 코딩을 하게 된다.","text":"상황스프링 환경에서 일반적으로 DAO 나 BO 레벨에서 다음과 같이 코딩을 하게 된다.1234@Transactional(isolation = Isolation.READ_COMMITTED, propagation = Propagation.REQUIRED, rollbackFor = Exception.class)public int method(int i) throws Exception &#123; return sqlMapClient.delete(\"~~~~\");&#125; 무분별하게 Ctrl+C,V 신공으로 트랜잭션 어노테이션을 가져다가 사용할수도 있겠으나, 각 값들이 어떤 역활을 아는지에 대해 알고 넘어갈 필요성이 있다. @Transactional우선 해당 어노테이션을 적용하면 적용된 클래스 또는 메소드에 트랜잭션이 적용된다. 따라서 로직 흐름에 맞추어 전체적으로 트랜잭션을 적용할것인지, 아니면 특정 메소드에 적용할것인지 전략을 잘 세워야 한다. isolation격리수준이라는 옵션이다. 트랜잭션에서 일관성이 없는 데이터를 허용하도록 하는 수준을 말하는데 옵션은 다음과 같다. READ_UNCOMMITTED (level 0) 트랜잭션에 처리중인 혹은 아직 커밋되지 않은 데이터를 다른 트랜잭션이 읽는 것을 허용 어떤 사용자가 A라는 데이터를 B라는 데이터로 변경하는 동안 다른 사용자는 B라는 아직 완료되지 않은(Uncommitted 혹은 Dirty) 데이터 B를 읽을 수 있다. Dirty read위와 같이 다른 트랜잭션에서 처리하는 작업이 완료되지 않았는데도 다른 트랜잭션에서 볼 수 있는 현상을 dirty read 라고 하며, READ UNCOMMITTED 격리수준에서만 일어나는 현상 READ_COMMITTED (level 1) dirty read 방지 : 트랜잭션이 커밋되어 확정된 데이터만을 읽는 것을 허용 어떠한 사용자가 A라는 데이터를 B라는 데이터로 변경하는 동안 다른 사용자는 해당 데이터에 접근할 수 없다. REPEATABLE_READ (level 2) 트랜잭션이 완료될 때까지 SELECT 문장이 사용하는 모든 데이터에 shared lock이 걸리므로 다른 사용자는 그 영역에 해당되는 데이터에 대한 수정이 불가능하다. 선행 트랜잭션이 읽은 데이터는 트랜잭션이 종료될 때까지 후행 트랜잭션이 갱신하거나 삭제하는 것을 불허함으로써 같은 데이터를 두 번 쿼리했을 때 일관성 있는 결과를 리턴함 SERIALIZABLE (level 3) 완벽한 읽기 일관성 모드를 제공 데이터의 일관성 및 동시성을 위해 MVCC(Multi Version Concurrency Control)을 사용하지 않음(MVCC는 다중 사용자 데이터베이스 성능을 위한 기술로 데이터 조회 시 LOCK을 사용하지 않고 데이터의 버전을 관리해 데이터의 일관성 및 동시성을 높이는 기술) 트랜잭션이 완료될 때까지 SELECT 문장이 사용하는 모든 데이터에 shared lock이 걸리므로 다른 사용자는 그 영역에 해당되는 데이터에 대한 수정 및 입력이 불가능하다. propagation ( 전파옵션) REQUIRED : 부모 트랜잭션 내에서 실행하며 부모 트랜잭션이 없을 경우 새로운 트랜잭션을 생성 REQUIRES_NEW : 부모 트랜잭션을 무시하고 무조건 새로운 트랜잭션이 생성 SUPPORT : 부모 트랜잭션 내에서 실행하며 부모 트랜잭션이 없을 경우 nontransactionally로 실행 MANDATORY : 부모 트랜잭션 내에서 실행되며 부모 트랜잭션이 없을 경우 예외가 발생 NOT_SUPPORT : nontransactionally로 실행하며 부모 트랜잭션 내에서 실행될 경우 일시 정지 NEVER : nontransactionally로 실행되며 부모 트랜잭션이 존재한다면 예외가 발생 NESTED : 해당 메서드가 부모 트랜잭션에서 진행될 경우 별개로 커밋되거나 롤백될 수 있음. 둘러싼 트랜잭션이 없을 경우 REQUIRED와 동일하게 작동 no-rollback-for - 예외처리 (기본값 : 없음)특정 예외가 발생하더라도 롤백되지 않도록 설정 스프링 배치에서의 트랜잭션 (내가 당했던(?) 문제)스프링 배치에서는 Tasklet 에서 기본적으로 step 단위 트랜잭션을 지원하고 있다고 한다.기본적으로 job이 하나의 tasklet 의 step 으로 실행하다보니 명시적이진 않지만 내부적으로 전체 트랜잭션으로 걸려있게 된다. 나같은 job 내 DAO delete 메소드에서 @Transactional 설정을 하고 그 DAO 메소드를 반복문에 의해 delete 하는 로직을 수행하는 부분이 있었는데 부모의 트랜잭션(tasklet에서 설정된 트랜잭션)으로 인해 dao 를 몇번 호출하던 job단위로 트렌젝션이 걸리게 되었다. (결국 트랜잭션은 반복문이 다 끝나야 적용이 된다는점.)그러다보니 가끔 DB Query Lock이 걸렸는데 DB레벨에서 undolog를 남기는게 너무 무거워져 lock이 발생 따라서 전파옵션을 수정해서 해당 문제를 해결하였다.1234567891011# 기존begin delete &lt; for 반복문commit# 전파옵션 수정 (기존 REQUIRES 에서 REQUIRES_NEW 으로 수정)for begin delete commitfor end","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"Transaction","slug":"Transaction","permalink":"https://taetaetae.github.io/tags/Transaction/"}]},{"title":"디자인패턴-싱글톤","slug":"20161006","date":"2016-10-06T08:03:48.000Z","updated":"2018-07-29T08:31:26.000Z","comments":true,"path":"2016/10/06/20161006/","link":"","permalink":"https://taetaetae.github.io/2016/10/06/20161006/","excerpt":"디자인 패턴중에 가장 잘 알려진 싱글톤 에 대해서 알아보고자 한다. 멀티 스레드 환경에서 자주 이용되는 패턴이라고만 들었는데 이번 기회를 통해 제대로 정리해보자","text":"디자인 패턴중에 가장 잘 알려진 싱글톤 에 대해서 알아보고자 한다. 멀티 스레드 환경에서 자주 이용되는 패턴이라고만 들었는데 이번 기회를 통해 제대로 정리해보자 싱글톤이 무엇인가 싱글톤(Singleton)은 정확히 하나의 인스턴스만 생성되는 클래스이다. 라고 이펙티브 자바에서 정의되어있다. 즉, 딱 하나만 생성하고 이를 여기저기서 사용하는 패턴이라 생각하면 될듯 하다. 싱글 스레드 환경에서는 당연히 인스턴스를 공유할 상황이 없겠지만 대부분 멀티 스레드 환경이기 때문에 싱글톤 패턴은 아주 중요한 부분이다. 0. 아주 고전적인 방법 (위험한 방법)123456789101112public class Singleton &#123; private static Singleton uniqueInstance; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if (uniqueInstance == null)&#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125; &#125; 위와 같은 상황에서 if절을 도달하는 시점이 각 스레드마다 다를경우 문제가 발생할 수 있다.(교묘한 시점에 객체가 1개 이상 반환될 여지가 있음) 이를 해결하기 위해서는 다음과 같이 getInstance()를 동기화 해주면 된다. 하지만 불필요하게 동기화 하는 오버헤드만 증가하게 된다.123456789101112public class Singleton &#123; private static Singleton uniqueInstance; private Singleton()&#123;&#125; public static synchronized Singleton getInstance()&#123; if (uniqueInstance == null)&#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 1. public static 인스턴스로 생성12345public static final LocalCache sharedObject = new LocalCache();private LocalCache()&#123;&#125; 이코드는 간단하다는 장점이 있는 반면에 유연하지 못한 부분이 있다. (아래 이어서 설명) 2. private static final 인스턴스로 생성12345678910private static final LocalCache sharedObject = new LocalCache(); private LocalCache() &#123; &#125; public static LocalCache getInstance() &#123; return sharedObject ; &#125; 이렇게 하면 factory 메소드를 통해 객체를 반환받고, 반환 받는 시점에 다양한 작업들을 할수 있다. 3. enum 으로 생성123456public enum LocalCacheEnum&#123; LocalCache; //etc another functions&#125; 잘 사용하지는 않지만 가장 좋은 세번째 방법인 enum으로 클래스를 만드는 방법이라고 한다. 복잡한 직렬화나 리플렉션(reflection) 상황에서도 직렬화가 자동으로 지원되고, 인스턴스가 여러개 생기지 않도록 확실하게 보장해준단다. (by effective java) 그럼 어디서 사용될까 static 으로 선언해서 공통적으로 사용되는 부분이나 환경설정 내용이 변경되면 다른 클래스에서도 그 부분이 똑같이 적용되어 실행되어야 할때 자주 사용되는 부분을 싱글톤으로 만들어 생성되는 시간을 줄일때 스프링에서의 DB커넥션 로직","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"디자인패턴","slug":"디자인패턴","permalink":"https://taetaetae.github.io/tags/디자인패턴/"},{"name":"싱글톤","slug":"싱글톤","permalink":"https://taetaetae.github.io/tags/싱글톤/"}]},{"title":"hexo 환경 구축하기","slug":"20160923","date":"2016-09-23T01:26:53.000Z","updated":"2018-07-29T08:31:25.996Z","comments":true,"path":"2016/09/23/20160923/","link":"","permalink":"https://taetaetae.github.io/2016/09/23/20160923/","excerpt":"개요이전포스팅 에서 이야기 한것과 같이 어느곳에서든지(집 또는 회사 등) 블로그 포스팅을 할수 있는 환경을 만들고 싶었다. (git을 이용해서.)","text":"개요이전포스팅 에서 이야기 한것과 같이 어느곳에서든지(집 또는 회사 등) 블로그 포스팅을 할수 있는 환경을 만들고 싶었다. (git을 이용해서.)그래서 git repository 를 두개를 만들었고, 하나는 실제 블로그서버로 이용하고 하나는 블로그를 포스팅하는 hexo 환경을 저장하게 된다. 지금부터 이야기 할 내용은 hexo환경을 git repository 에서 pull 받아서 환경구성하는 부분을 이야기 하려고 한다. 환경구성hexo설치와 git설치는 되어있다고 가정.먼저 구성할 폴더를 생성하고 이 폴더에 hexo 환경을 구성하겠다고 초기 셋팅을 한다12mkdir bloghexo init blog 그리고 hexo환경을 저장해둔 repository를 가져와야 하므로 git설정을 한다1234cd blog/git initgit remote add origin https://github.com/taetaetae/hexo.gitgit fetch 필요없는초기셋팅이 되는 파일은 지우고12rm source/_posts/hello-world.mdrm -r themes/landscape/ #해당 테마를 사용하고 있다면 지울필요가 없다. hexo환경 repository 를 pull받는다12git reset --hard origin/mastergit pull origin master hueman테마의 검색 기능을 사용한다는 가정하에 필요한 플러그인과, 나중에 deploy 할때 필요한 플러그인을 설치해준다12npm install hexo-deployer-git --savenpm install -S hexo-generator-json-content 이렇게 되면 기존처럼 환경설정이 마무리 되고, 포스팅을 할수 있게 된다. # 추가 canonical 속성npm install –save hexo-auto-canonical 사이트맵 속성npm install hexo-generator-seo-friendly-sitemap –save feed 속성npm install hexo-generator-feed –save","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"}]},{"title":"hexo + github + blog 연동하기","slug":"hexo_github_blog","date":"2016-09-18T06:38:34.000Z","updated":"2018-07-29T08:31:26.194Z","comments":true,"path":"2016/09/18/hexo_github_blog/","link":"","permalink":"https://taetaetae.github.io/2016/09/18/hexo_github_blog/","excerpt":"들어가기에 앞서예전부터 블로그를 운영해야지 하구서 tistory, naver blog 등 다양한 플랫폼으로 시작을 했었지만 이렇다할 운영이 안되었고 사실 열정이 부족했었다. 직접 홈페이지를 만들기에는 너무 많은 허들이 있다보니 (서버구축, 호스팅, 도메인 등 …) 계속 차일피일 미루고 있었다.","text":"들어가기에 앞서예전부터 블로그를 운영해야지 하구서 tistory, naver blog 등 다양한 플랫폼으로 시작을 했었지만 이렇다할 운영이 안되었고 사실 열정이 부족했었다. 직접 홈페이지를 만들기에는 너무 많은 허들이 있다보니 (서버구축, 호스팅, 도메인 등 …) 계속 차일피일 미루고 있었다.그러다 github에서 제공하는 pages라는 걸 이용해서 무료로 도메인과 웹호스팅을 할수 있다는 부분을 알게되었고, 거기에다 jekyll을 이용하면 설치형 블로그를 운영할수 있다는것에 놀라웠다. 하지만 jekyll을 적용해보려고 이것저것 하다보니 ruby라는 언어로 만들어져있고 커스터마이징이 어렵다는 부분을 확인, 좀더 알아보다 hexo 라는 걸로 해당 블로그를 만들게 되었다.필자처럼 남들과는 다른 블로그를 만들고 싶거나, git command 공부도 하면서 블로그를 운영해볼 사람들은 해당 글을 천천히 따라오면 좋을것 같다. - hexo 시작하기hexo 라는걸 시작하기 위해 몇가지 준비물이 있다. node 설치 git 설치 github에 블로그로 사용할 빈 repository 생성 github에 hexo 설정을 저장할 빈 repository 생성 위 4가지(?!)가 전부 설치 되었다고 가정을 하고 시작을 해보겠다. - hexo 설치간단하다. hexo 페이지에도 나와있는것처럼 아래 명령어를 실행해주면 된다.1$ npm install -g hexo-cli - 블로그로 운영할 폴더 hexo 초기화폴더 구조로 구성이 되기 때문에 임의의 폴더를 하나 만들고 해당 폴더를 hexo 명령어로 초기화 시켜준다.12$ mkdir &lt;디렉토리명&gt;$ hexo init &lt;디렉토리명&gt; - 로컬서버 띄워보기이제 로컬에서 서버를 띄워서 블로그가 어떻게 나오는지 확인을 해보면 된다.1$ hexo s (or server) 간혹 서버가 실행이 안되거나 오류가 발생, 수정한 부분이 반영이 안된다면 clean 명령어를 한번 해준 다음에 다시 서버를 실행해주면 되는 경우도 있다.12$ hexo clean$ hexo s (or server) http://localhost:4000 을 접속해서 정상적으로 페이지가 나오는지 확인을 해보자.페이지가 정상적으로 나온다면 성공! - 글 작성아래 명령어를 실행하면 /source/_post/ 아래에 .md 파일이 생성이 된다.1$ hexo new &lt;글 제목&gt; 해당 파일을 사용하기 편한 에디터로 열어서 마크다운 문법에 맞추어 수정을 하면 끝! - 왜 두개의 repository가 필요한가아래에서 이야기 하겠지만, 하나는 실제 블로그 내용이 올라갈 저장소이고 다른 하나는 블로그를 운영하고 있는 hexo 자체를 저장할 저장소이다. hexo 정보를 가지고 있지 않을꺼라면(필자처럼 다양한 PC에서 업로드 환경을 구축하지 않을꺼라면) 하나의 레파지토리만 필요할수도 있다. - github 셋팅 지금부터가 알짜배기다. 즉 이글을 포스팅 하는 의미. 다른 글들에서도 hexo 사용법을 친절하게 알려 주셨으나 github와의 연동, 그리고 어떤식으로 운영해야할지는 찾기 힘들었다. 필자는 감으로 그런가보다(?)하고서 터득한 바를 공유하려한다. (이게 정답은 아니지만, 나는 이렇게 사용하는게 맞겠다 싶어..) 일반적으로 github에서 블로그로 사용할 repository를 만들게 되면 http://(github아이디).github.io/(repository이름) 으로 블로그가 만들어 지는데 뭔가 조금 이상해서 간지가 안나서 찾고 찾아서 아래와 같은 방식으로 하게 되었다.필자의 github 아이디는 taetaetae 이고 도메인은 아이디 그대로를 사용하여 http://taetaetae.github.io 으로 사용하고 싶었다. 따라서 github에 repository이름을 taetaetae.github.io로 만들어야 한다. 여기까지만 하면 일단 github에 배포할수 있는 준비가 되어있는 상태 - hexo로 배포하기포스팅한 글이 정상적으로 등록이 된 것을 로컬서버에서 확인이 되었으면 이 상태를 조금전 만든 github repository으로 배포(정확히 말하면 git push)해줘야 한다. 그전에 최상위 폴더에 있는 _config.yml 파일을 열어서 github 정보를 입력해 줘야 한다.하단 영역 Deployment 부분에 다음과 같이 작성하고 저장한다.12345# Deploymentdeploy: type: git repo: https://github.com/taetaetae/taetaetae.github.io branch: master 그 다음 hexo 에서 github로 배포할수있는 플러그인을 설치해준다.1$ npm install hexo-deployer-git --save 이제 설정한 github에 배포를 하면 끝!1$ hexo deploy 1분~3분 뒤에 도메인을 접속하여 정상적으로 페이지가 나오는지 확인하고, github에 파일들이 push가 잘 되었는지를 확인한다. - 향후 관리 hexo 정보 저장나중에 다른 PC에서도 블로그를 포스팅 할 경우가 있으니 hexo를 이용하여 포스팅 한 환경 자체를 저장 해야할 필요가 생겼다. 따라서 만들었던 폴더 또한 github에 업로드를 해놓는게 좋을것 같다 (지극히 개인적인 생각)git command 설명은 따로 정리하지 않겠다.1234567891011121314git 초기화$ git init변경사항 추가(전체))$ git add .커밋$ git commit -m &quot;커밋메시지&quot;remote repository 등록$ git remote add origin https://github.com/taetaetae/hexo.gitremote repository로 push$ git push origin master 마치며아직 git command 나 markdown, hexo 등 잘 모르는 부분들이 많다. 하나씩 시행착오를 겪어가면서 정리될수 있는 부분들은 이어 정리를 할 계획이다. 그리고 hexo 기본개념이나 설정파일 수정하는 부분들은 다른 분들이 많이 올려놓으셨기 때문데 중요하다고 생각되는 지극히 개인적으로 정리해야겠다고 느끼는 부분들 중심으로 정리를 해야겠다","categories":[{"name":"tech","slug":"tech","permalink":"https://taetaetae.github.io/categories/tech/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://taetaetae.github.io/tags/hexo/"}]},{"title":"첫번째 포스팅","slug":"first","date":"2016-09-17T15:34:23.000Z","updated":"2018-07-29T17:01:22.077Z","comments":true,"path":"2016/09/18/first/","link":"","permalink":"https://taetaetae.github.io/2016/09/18/first/","excerpt":"","text":"시작은 언제든지 새롭고 떨리고 가슴벅차는 순간이다.과연 이 블로그를 잘 운영할수 있을런지..제대로 한번 관리 해보고, 나만의 공간으로 꾸며보자!","categories":[{"name":"blog","slug":"blog","permalink":"https://taetaetae.github.io/categories/blog/"}],"tags":[]}]}